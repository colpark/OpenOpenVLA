{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 15: Evaluate Fine-tuned OpenVLA with Action Chunking\n",
    "\n",
    "**Purpose**: Comprehensive evaluation of fine-tuned OpenVLA model trained with temporal subsampling.\n",
    "\n",
    "**Key Comparisons**:\n",
    "- Base OpenVLA vs Fine-tuned (with LoRA adapters)\n",
    "- Action quality metrics: L1 error, direction accuracy, gripper accuracy\n",
    "- Visualization of predictions vs ground truth\n",
    "\n",
    "**Action Chunking Context**:\n",
    "- LIBERO runs at 20 Hz\n",
    "- OpenVLA trained on Bridge V2 (5 Hz) and Fractal (3 Hz)\n",
    "- With 4x chunking: 20 Hz → 5 Hz (matches Bridge V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration for SciServer or local\n",
    "if 'SCRATCH' in os.environ:\n",
    "    BASE_DIR = os.environ['SCRATCH']\n",
    "else:\n",
    "    BASE_DIR = \"/home/idies/workspace/Temporary/dpark1/scratch\"\n",
    "\n",
    "CACHE_DIR = f\"{BASE_DIR}/.cache\"\n",
    "LIBERO_DATA_DIR = f\"{BASE_DIR}/libero_data\"\n",
    "\n",
    "# Set cache directories\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version check\n",
    "import transformers\n",
    "import tokenizers\n",
    "import timm\n",
    "\n",
    "print(f\"transformers: {transformers.__version__} (need 4.40.1)\")\n",
    "print(f\"tokenizers: {tokenizers.__version__} (need 0.19.1)\")\n",
    "print(f\"timm: {timm.__version__} (need 0.9.x)\")\n",
    "\n",
    "assert transformers.__version__ == \"4.40.1\", \"transformers version mismatch!\"\n",
    "assert tokenizers.__version__ == \"0.19.1\", \"tokenizers version mismatch!\"\n",
    "assert timm.__version__.startswith(\"0.9.\"), \"timm version mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda:0\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONFIGURE YOUR CHECKPOINT PATH HERE\n",
    "# =====================================================\n",
    "\n",
    "# Path to the fine-tuned model checkpoint\n",
    "# Options: 'best', 'final', or specific checkpoint like 'checkpoints/checkpoint-1000'\n",
    "RESULTS_DIR = Path(\"../../results\")\n",
    "\n",
    "# List available runs\n",
    "print(\"Available runs:\")\n",
    "if RESULTS_DIR.exists():\n",
    "    for run_dir in sorted(RESULTS_DIR.iterdir()):\n",
    "        if run_dir.is_dir() and not run_dir.name.startswith('.'):\n",
    "            config_path = run_dir / \"config.json\"\n",
    "            if config_path.exists():\n",
    "                with open(config_path) as f:\n",
    "                    config = json.load(f)\n",
    "                print(f\"  - {run_dir.name}\")\n",
    "                print(f\"      chunk_size: {config.get('chunk_size', 'N/A')}, epochs: {config.get('epochs', 'N/A')}\")\n",
    "else:\n",
    "    print(\"  No results directory found. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your run (modify this)\n",
    "RUN_NAME = \"libero_spatial_chunk4_XXXXXXXX_XXXXXX\"  # Replace with your run name\n",
    "CHECKPOINT_TYPE = \"best\"  # 'best', 'final', or checkpoint path\n",
    "\n",
    "# Construct paths\n",
    "RUN_DIR = RESULTS_DIR / RUN_NAME\n",
    "CHECKPOINT_PATH = RUN_DIR / CHECKPOINT_TYPE\n",
    "\n",
    "# Action chunking settings (must match training)\n",
    "CHUNK_SIZE = 4  # 20 Hz → 5 Hz\n",
    "VAL_DEMOS_PER_TASK = 5\n",
    "\n",
    "print(f\"Run directory: {RUN_DIR}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training logs\n",
    "training_log = None\n",
    "validation_log = None\n",
    "\n",
    "training_log_path = RUN_DIR / \"training_log.csv\"\n",
    "validation_log_path = RUN_DIR / \"validation_log.csv\"\n",
    "\n",
    "if training_log_path.exists():\n",
    "    training_log = pd.read_csv(training_log_path)\n",
    "    print(f\"Training log: {len(training_log)} entries\")\n",
    "    print(training_log.head())\n",
    "else:\n",
    "    print(\"No training log found\")\n",
    "\n",
    "if validation_log_path.exists():\n",
    "    validation_log = pd.read_csv(validation_log_path)\n",
    "    print(f\"\\nValidation log: {len(validation_log)} entries\")\n",
    "    print(validation_log.head())\n",
    "else:\n",
    "    print(\"No validation log found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if training_log is not None and validation_log is not None:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Training loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(training_log['step'], training_log['loss'], alpha=0.7)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(validation_log['step'], validation_log['val_loss'], 'b-o', markersize=4)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Validation Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # L1 Error\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(validation_log['step'], validation_log['l1_error'], 'g-o', markersize=4)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('L1 Error')\n",
    "    ax.set_title('Action L1 Error')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Direction Accuracy\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(validation_log['step'], validation_log['direction_accuracy'], 'r-o', markersize=4)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', label='Random')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Direction Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gripper Accuracy\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(validation_log['step'], validation_log['gripper_accuracy'], 'purple', marker='o', markersize=4)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Gripper Accuracy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax = axes[1, 2]\n",
    "    ax.plot(training_log['step'], training_log['lr'])\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Summary\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total steps: {training_log['step'].max()}\")\n",
    "    print(f\"Best validation loss: {validation_log['val_loss'].min():.4f}\")\n",
    "    print(f\"Best L1 error: {validation_log['l1_error'].min():.4f}\")\n",
    "    print(f\"Best direction accuracy: {validation_log['direction_accuracy'].max():.4f}\")\n",
    "    print(f\"Best gripper accuracy: {validation_log['gripper_accuracy'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    ")\n",
    "\n",
    "print(\"Base model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model with LoRA adapters\n",
    "print(f\"Loading LoRA adapters from {CHECKPOINT_PATH}...\")\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        str(CHECKPOINT_PATH),\n",
    "        is_trainable=False,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Fine-tuned model loaded.\")\n",
    "    print(model.print_trainable_parameters())\n",
    "else:\n",
    "    print(f\"ERROR: Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    print(\"Please update RUN_NAME and CHECKPOINT_TYPE in Section 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Action Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTokenizer:\n",
    "    \"\"\"OpenVLA-compatible action tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000, n_bins=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = np.linspace(-1, 1, n_bins)\n",
    "        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2\n",
    "        self.action_token_start = vocab_size - n_bins\n",
    "        self.action_token_end = vocab_size - 1\n",
    "    \n",
    "    def encode(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        discretized = np.digitize(action, self.bins)\n",
    "        return self.vocab_size - discretized\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().numpy()\n",
    "        discretized = self.vocab_size - token_ids\n",
    "        indices = np.clip(discretized - 1, 0, len(self.bin_centers) - 1)\n",
    "        return self.bin_centers[indices]\n",
    "\n",
    "vocab_size = len(processor.tokenizer)\n",
    "action_tokenizer = ActionTokenizer(vocab_size=vocab_size)\n",
    "print(f\"Action token range: [{action_tokenizer.action_token_start}, {action_tokenizer.action_token_end}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_action(action):\n",
    "    \"\"\"Transform LIBERO action to OpenVLA format.\"\"\"\n",
    "    action = action.astype(np.float32)\n",
    "    action[:6] = np.clip(action[:6], -1.0, 1.0)\n",
    "    gripper = np.clip(action[6], 0.0, 1.0)\n",
    "    action[6] = 1.0 - gripper\n",
    "    return action\n",
    "\n",
    "def load_validation_samples(data_dir, suite_name, chunk_size=4, val_demos=5, max_samples=500):\n",
    "    \"\"\"Load validation samples with chunking.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    samples = []\n",
    "    \n",
    "    # Find HDF5 files\n",
    "    hdf5_files = list(data_dir.rglob(\"*.hdf5\"))\n",
    "    print(f\"Found {len(hdf5_files)} HDF5 files\")\n",
    "    \n",
    "    for filepath in tqdm(hdf5_files, desc=\"Loading validation data\"):\n",
    "        try:\n",
    "            with h5py.File(filepath, 'r') as f:\n",
    "                # Get language instruction\n",
    "                language = \"complete the task\"\n",
    "                for key in ['language_instruction', 'problem_info', 'language']:\n",
    "                    if key in f.attrs:\n",
    "                        lang = f.attrs[key]\n",
    "                        if isinstance(lang, bytes):\n",
    "                            lang = lang.decode('utf-8')\n",
    "                        language = lang\n",
    "                        break\n",
    "                \n",
    "                if 'data' not in f:\n",
    "                    continue\n",
    "                \n",
    "                demo_keys = sorted([k for k in f['data'].keys() if k.startswith('demo_')])\n",
    "                \n",
    "                # Use last N demos for validation\n",
    "                val_demo_keys = demo_keys[-val_demos:]\n",
    "                \n",
    "                for demo_key in val_demo_keys:\n",
    "                    demo = f['data'][demo_key]\n",
    "                    \n",
    "                    if 'actions' not in demo or 'obs' not in demo:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find image key\n",
    "                    img_key = None\n",
    "                    for key in ['agentview_rgb', 'agentview_image', 'rgb', 'image']:\n",
    "                        if key in demo['obs']:\n",
    "                            img_key = key\n",
    "                            break\n",
    "                    if img_key is None:\n",
    "                        continue\n",
    "                    \n",
    "                    n_steps = len(demo['actions'])\n",
    "                    \n",
    "                    # Apply chunking\n",
    "                    for t in range(0, n_steps, chunk_size):\n",
    "                        image = demo['obs'][img_key][t]\n",
    "                        image = np.rot90(image, k=2)  # 180° rotation\n",
    "                        \n",
    "                        action = demo['actions'][t]\n",
    "                        if len(action) < 7:\n",
    "                            action = np.pad(action, (0, 7 - len(action)))\n",
    "                        else:\n",
    "                            action = action[:7]\n",
    "                        \n",
    "                        samples.append({\n",
    "                            'image': image,\n",
    "                            'action': transform_action(action),\n",
    "                            'language': language,\n",
    "                        })\n",
    "                        \n",
    "                        if len(samples) >= max_samples:\n",
    "                            return samples\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load validation samples\n",
    "val_samples = load_validation_samples(\n",
    "    LIBERO_DATA_DIR, \n",
    "    \"libero_spatial\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    val_demos=VAL_DEMOS_PER_TASK,\n",
    "    max_samples=500\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded {len(val_samples)} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Base vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(model, processor, image, instruction, device):\n",
    "    \"\"\"Predict action from image and instruction.\"\"\"\n",
    "    # Preprocess image\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    pil_image = pil_image.resize((224, 224), Image.LANCZOS)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(prompt, pil_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    if 'pixel_values' in inputs:\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=7,\n",
    "            do_sample=False,\n",
    "            pad_token_id=model.config.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode action tokens\n",
    "    pred_tokens = outputs[0, -7:].cpu().numpy()\n",
    "    pred_action = action_tokenizer.decode(pred_tokens)\n",
    "    \n",
    "    return pred_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, samples, processor, device, model_name=\"Model\", max_samples=200):\n",
    "    \"\"\"Evaluate model on validation samples.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for sample in tqdm(samples[:max_samples], desc=f\"Evaluating {model_name}\"):\n",
    "        try:\n",
    "            pred = predict_action(model, processor, sample['image'], sample['language'], device)\n",
    "            predictions.append(pred)\n",
    "            ground_truths.append(sample['action'])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "    \n",
    "    # Compute metrics\n",
    "    l1_error = np.abs(predictions - ground_truths).mean()\n",
    "    position_l1 = np.abs(predictions[:, :3] - ground_truths[:, :3]).mean()\n",
    "    rotation_l1 = np.abs(predictions[:, 3:6] - ground_truths[:, 3:6]).mean()\n",
    "    \n",
    "    # Direction accuracy\n",
    "    threshold = 0.02\n",
    "    dir_correct = 0\n",
    "    dir_total = 0\n",
    "    for dim in range(3):\n",
    "        significant = np.abs(ground_truths[:, dim]) > threshold\n",
    "        if significant.sum() > 0:\n",
    "            same_sign = np.sign(ground_truths[:, dim][significant]) == np.sign(predictions[:, dim][significant])\n",
    "            dir_correct += same_sign.sum()\n",
    "            dir_total += significant.sum()\n",
    "    direction_accuracy = dir_correct / dir_total if dir_total > 0 else 0.5\n",
    "    \n",
    "    # Gripper accuracy\n",
    "    gripper_threshold = 0.5\n",
    "    gt_gripper = (ground_truths[:, 6] > gripper_threshold).astype(int)\n",
    "    pred_gripper = (predictions[:, 6] > gripper_threshold).astype(int)\n",
    "    gripper_accuracy = (gt_gripper == pred_gripper).mean()\n",
    "    \n",
    "    return {\n",
    "        'l1_error': l1_error,\n",
    "        'position_l1': position_l1,\n",
    "        'rotation_l1': rotation_l1,\n",
    "        'direction_accuracy': direction_accuracy,\n",
    "        'gripper_accuracy': gripper_accuracy,\n",
    "        'predictions': predictions,\n",
    "        'ground_truths': ground_truths,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model (disable LoRA adapters)\n",
    "print(\"Evaluating BASE model (LoRA disabled)...\")\n",
    "model.disable_adapter_layers()\n",
    "base_results = evaluate_model(model, val_samples, processor, device, \"Base Model\", max_samples=200)\n",
    "\n",
    "print(\"\\nBase Model Results:\")\n",
    "print(f\"  L1 Error: {base_results['l1_error']:.4f}\")\n",
    "print(f\"  Position L1: {base_results['position_l1']:.4f}\")\n",
    "print(f\"  Direction Accuracy: {base_results['direction_accuracy']:.4f}\")\n",
    "print(f\"  Gripper Accuracy: {base_results['gripper_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model (enable LoRA adapters)\n",
    "print(\"Evaluating FINE-TUNED model (LoRA enabled)...\")\n",
    "model.enable_adapter_layers()\n",
    "finetuned_results = evaluate_model(model, val_samples, processor, device, \"Fine-tuned Model\", max_samples=200)\n",
    "\n",
    "print(\"\\nFine-tuned Model Results:\")\n",
    "print(f\"  L1 Error: {finetuned_results['l1_error']:.4f}\")\n",
    "print(f\"  Position L1: {finetuned_results['position_l1']:.4f}\")\n",
    "print(f\"  Direction Accuracy: {finetuned_results['direction_accuracy']:.4f}\")\n",
    "print(f\"  Gripper Accuracy: {finetuned_results['gripper_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" COMPARISON: Base vs Fine-tuned (with Action Chunking)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nChunk size: {CHUNK_SIZE} (20 Hz → {20/CHUNK_SIZE:.1f} Hz)\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"{'Metric':<25} {'Base':>12} {'Fine-tuned':>12} {'Change':>12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "metrics = ['l1_error', 'position_l1', 'direction_accuracy', 'gripper_accuracy']\n",
    "labels = ['L1 Error', 'Position L1', 'Direction Accuracy', 'Gripper Accuracy']\n",
    "\n",
    "for metric, label in zip(metrics, labels):\n",
    "    base_val = base_results[metric]\n",
    "    ft_val = finetuned_results[metric]\n",
    "    \n",
    "    if 'accuracy' in metric:\n",
    "        change = ft_val - base_val\n",
    "        sign = '+' if change > 0 else ''\n",
    "        status = '✅' if change > 0 else ('⚠️' if change < -0.05 else '→')\n",
    "    else:\n",
    "        change = (ft_val - base_val) / base_val * 100\n",
    "        sign = '+' if change > 0 else ''\n",
    "        status = '✅' if change < 0 else ('⚠️' if change > 10 else '→')\n",
    "    \n",
    "    if 'accuracy' in metric:\n",
    "        print(f\"{label:<25} {base_val:>11.1%} {ft_val:>11.1%} {sign}{change:>+.1%} {status}\")\n",
    "    else:\n",
    "        print(f\"{label:<25} {base_val:>12.4f} {ft_val:>12.4f} {sign}{change:>+.1f}% {status}\")\n",
    "\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "\n",
    "for i, (name, ax_idx) in enumerate(zip(dim_names, [(0,0), (0,1), (0,2), (0,3), (1,0), (1,1), (1,2)])):\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    gt = base_results['ground_truths'][:, i]\n",
    "    base_pred = base_results['predictions'][:, i]\n",
    "    ft_pred = finetuned_results['predictions'][:, i]\n",
    "    \n",
    "    ax.hist(gt, bins=30, alpha=0.5, label='Ground Truth', density=True)\n",
    "    ax.hist(base_pred, bins=30, alpha=0.5, label='Base', density=True)\n",
    "    ax.hist(ft_pred, bins=30, alpha=0.5, label='Fine-tuned', density=True)\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Action Distribution: Ground Truth vs Base vs Fine-tuned', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: Predicted vs Ground Truth\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "for i, (name, ax) in enumerate(zip(['dx', 'dy', 'dz'], axes[0])):\n",
    "    gt = base_results['ground_truths'][:, i]\n",
    "    base_pred = base_results['predictions'][:, i]\n",
    "    ft_pred = finetuned_results['predictions'][:, i]\n",
    "    \n",
    "    ax.scatter(gt, base_pred, alpha=0.3, s=20, label='Base', color='blue')\n",
    "    ax.scatter(gt, ft_pred, alpha=0.3, s=20, label='Fine-tuned', color='orange')\n",
    "    ax.plot([-1, 1], [-1, 1], 'k--', label='Perfect')\n",
    "    ax.set_xlabel(f'Ground Truth {name}')\n",
    "    ax.set_ylabel(f'Predicted {name}')\n",
    "    ax.set_title(f'{name}: Predicted vs Ground Truth')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (name, ax) in enumerate(zip(['rx', 'ry', 'rz'], axes[1])):\n",
    "    gt = base_results['ground_truths'][:, i+3]\n",
    "    base_pred = base_results['predictions'][:, i+3]\n",
    "    ft_pred = finetuned_results['predictions'][:, i+3]\n",
    "    \n",
    "    ax.scatter(gt, base_pred, alpha=0.3, s=20, label='Base', color='blue')\n",
    "    ax.scatter(gt, ft_pred, alpha=0.3, s=20, label='Fine-tuned', color='orange')\n",
    "    ax.plot([-1, 1], [-1, 1], 'k--', label='Perfect')\n",
    "    ax.set_xlabel(f'Ground Truth {name}')\n",
    "    ax.set_ylabel(f'Predicted {name}')\n",
    "    ax.set_title(f'{name}: Predicted vs Ground Truth')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis: Near-zero action percentage\n",
    "threshold = 0.05\n",
    "\n",
    "print(\"Near-Zero Action Analysis (|action| < 0.05)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Dimension':<10} {'GT %':>10} {'Base %':>10} {'FT %':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, name in enumerate(dim_names[:6]):\n",
    "    gt = base_results['ground_truths'][:, i]\n",
    "    base_pred = base_results['predictions'][:, i]\n",
    "    ft_pred = finetuned_results['predictions'][:, i]\n",
    "    \n",
    "    gt_near_zero = (np.abs(gt) < threshold).mean() * 100\n",
    "    base_near_zero = (np.abs(base_pred) < threshold).mean() * 100\n",
    "    ft_near_zero = (np.abs(ft_pred) < threshold).mean() * 100\n",
    "    \n",
    "    print(f\"{name:<10} {gt_near_zero:>9.1f}% {base_near_zero:>9.1f}% {ft_near_zero:>9.1f}%\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"\\nNote: Mode collapse is indicated when model predicts more near-zero\")\n",
    "print(\"actions than ground truth. Action chunking should reduce this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(val_samples), min(6, len(val_samples)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (sample_idx, ax) in enumerate(zip(sample_indices, axes.flat)):\n",
    "    sample = val_samples[sample_idx]\n",
    "    \n",
    "    # Show image\n",
    "    ax.imshow(sample['image'])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Get predictions\n",
    "    gt = sample['action']\n",
    "    base_pred = base_results['predictions'][sample_idx]\n",
    "    ft_pred = finetuned_results['predictions'][sample_idx]\n",
    "    \n",
    "    # Create title with key metrics\n",
    "    title = f\"Task: {sample['language'][:40]}...\\n\"\n",
    "    title += f\"GT dx/dy/dz: [{gt[0]:.2f}, {gt[1]:.2f}, {gt[2]:.2f}]\\n\"\n",
    "    title += f\"Base: [{base_pred[0]:.2f}, {base_pred[1]:.2f}, {base_pred[2]:.2f}]\\n\"\n",
    "    title += f\"FT: [{ft_pred[0]:.2f}, {ft_pred[1]:.2f}, {ft_pred[2]:.2f}]\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=9)\n",
    "\n",
    "plt.suptitle('Sample Predictions: Base vs Fine-tuned', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'effective_hz': 20 / CHUNK_SIZE,\n",
    "    'num_samples': len(val_samples),\n",
    "    'base_model': {\n",
    "        'l1_error': float(base_results['l1_error']),\n",
    "        'position_l1': float(base_results['position_l1']),\n",
    "        'rotation_l1': float(base_results['rotation_l1']),\n",
    "        'direction_accuracy': float(base_results['direction_accuracy']),\n",
    "        'gripper_accuracy': float(base_results['gripper_accuracy']),\n",
    "    },\n",
    "    'finetuned_model': {\n",
    "        'l1_error': float(finetuned_results['l1_error']),\n",
    "        'position_l1': float(finetuned_results['position_l1']),\n",
    "        'rotation_l1': float(finetuned_results['rotation_l1']),\n",
    "        'direction_accuracy': float(finetuned_results['direction_accuracy']),\n",
    "        'gripper_accuracy': float(finetuned_results['gripper_accuracy']),\n",
    "    },\n",
    "    'improvement': {\n",
    "        'l1_error_reduction': float((base_results['l1_error'] - finetuned_results['l1_error']) / base_results['l1_error'] * 100),\n",
    "        'direction_accuracy_change': float(finetuned_results['direction_accuracy'] - base_results['direction_accuracy']),\n",
    "        'gripper_accuracy_change': float(finetuned_results['gripper_accuracy'] - base_results['gripper_accuracy']),\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_output_path = RUN_DIR / \"evaluation_results.json\"\n",
    "with open(eval_output_path, 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation results saved to: {eval_output_path}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(eval_results['improvement'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "This notebook evaluated the fine-tuned OpenVLA model trained with action chunking.\n",
    "\n",
    "**Key Questions to Answer**:\n",
    "1. Did L1 error improve? (Lower is better)\n",
    "2. Did direction accuracy improve? (Higher is better, should be >50%)\n",
    "3. Did gripper accuracy improve? (Higher is better)\n",
    "4. Is there less mode collapse? (Near-zero % closer to GT)\n",
    "\n",
    "**If direction accuracy improved**:\n",
    "- Action chunking successfully addressed the control frequency mismatch\n",
    "- The model learned meaningful LIBERO-specific behaviors\n",
    "\n",
    "**If direction accuracy is still poor**:\n",
    "- Try different chunk sizes (e.g., 5 or 7 for ~4 Hz or ~3 Hz)\n",
    "- Consider action scaling instead of temporal subsampling\n",
    "- Try longer training or different learning rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
