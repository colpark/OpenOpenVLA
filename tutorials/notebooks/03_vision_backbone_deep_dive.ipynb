{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Vision Backbone Deep Dive\n",
    "\n",
    "**Goal**: Understand how OpenVLA processes visual information using DINOv2 and SigLIP.\n",
    "\n",
    "## What We'll Learn\n",
    "1. Vision Transformer (ViT) fundamentals\n",
    "2. DINOv2: Self-supervised visual features\n",
    "3. SigLIP: Text-aligned visual features\n",
    "4. Feature fusion strategy\n",
    "5. Practical visualization of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Vision Transformer Fundamentals\n",
    "\n",
    "Both DINOv2 and SigLIP are based on the **Vision Transformer (ViT)** architecture.\n",
    "\n",
    "### How ViT Works\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                  Vision Transformer Pipeline                    │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  Input Image (224 × 224 × 3)                                   │\n",
    "│           │                                                     │\n",
    "│           ▼                                                     │\n",
    "│  ┌──────────────────────────────────────────┐                  │\n",
    "│  │ 1. Patch Extraction                       │                  │\n",
    "│  │    Split into 14×14 patches (16×16 each) │                  │\n",
    "│  │    = 196 patches                          │                  │\n",
    "│  └──────────────────────────────────────────┘                  │\n",
    "│           │                                                     │\n",
    "│           ▼                                                     │\n",
    "│  ┌──────────────────────────────────────────┐                  │\n",
    "│  │ 2. Linear Embedding                       │                  │\n",
    "│  │    Each patch → 1024-dim vector          │                  │\n",
    "│  │    + [CLS] token + Position embeddings   │                  │\n",
    "│  └──────────────────────────────────────────┘                  │\n",
    "│           │                                                     │\n",
    "│           ▼                                                     │\n",
    "│  ┌──────────────────────────────────────────┐                  │\n",
    "│  │ 3. Transformer Encoder (24 layers)        │                  │\n",
    "│  │    Multi-Head Self-Attention              │                  │\n",
    "│  │    + Feed-Forward Networks                │                  │\n",
    "│  └──────────────────────────────────────────┘                  │\n",
    "│           │                                                     │\n",
    "│           ▼                                                     │\n",
    "│  Output: 197 tokens × 1024 dims                                │\n",
    "│  ([CLS] token + 196 patch tokens)                              │\n",
    "│                                                                 │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize patch extraction\n",
    "def visualize_patches(image_size=224, patch_size=16):\n",
    "    \"\"\"Visualize how an image is divided into patches.\"\"\"\n",
    "    n_patches = image_size // patch_size\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Create sample image\n",
    "    sample_img = np.random.randint(0, 255, (image_size, image_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(sample_img)\n",
    "    axes[0].set_title(f\"Original Image ({image_size}×{image_size})\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Image with patch grid\n",
    "    axes[1].imshow(sample_img)\n",
    "    for i in range(n_patches + 1):\n",
    "        axes[1].axhline(y=i * patch_size, color='red', linewidth=1)\n",
    "        axes[1].axvline(x=i * patch_size, color='red', linewidth=1)\n",
    "    axes[1].set_title(f\"Patches ({n_patches}×{n_patches} = {n_patches**2} patches)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nViT-L/14 Configuration:\")\n",
    "    print(f\"  Image size: {image_size}×{image_size}\")\n",
    "    print(f\"  Patch size: {patch_size}×{patch_size}\")\n",
    "    print(f\"  Number of patches: {n_patches**2}\")\n",
    "    print(f\"  + 1 [CLS] token = {n_patches**2 + 1} tokens total\")\n",
    "\n",
    "visualize_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DINOv2: Self-Supervised Visual Features\n",
    "\n",
    "**DINO** (Distillation with NO labels) learns visual features without any labeled data.\n",
    "\n",
    "### Training Approach\n",
    "- Teacher-student self-distillation\n",
    "- Student learns to match teacher's output on augmented views\n",
    "- Discovers semantic structure naturally\n",
    "\n",
    "### What DINOv2 Captures\n",
    "- Object boundaries and parts\n",
    "- Semantic segmentation (emergent property)\n",
    "- Spatial relationships\n",
    "- Rich local features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2 directly for exploration\n",
    "import timm\n",
    "\n",
    "print(\"Loading DINOv2-Large...\")\n",
    "dinov2 = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True)\n",
    "dinov2.eval()\n",
    "print(f\"DINOv2 loaded: {sum(p.numel() for p in dinov2.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect DINOv2 architecture\n",
    "print(\"DINOv2-L Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Patch embedding: {dinov2.patch_embed}\")\n",
    "print(f\"Num transformer blocks: {len(dinov2.blocks)}\")\n",
    "print(f\"Hidden dimension: {dinov2.embed_dim}\")\n",
    "print(f\"Num attention heads: {dinov2.blocks[0].attn.num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process an image through DINOv2\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "# Get DINOv2's preprocessing config\n",
    "config = resolve_data_config({}, model=dinov2)\n",
    "transform = create_transform(**config)\n",
    "\n",
    "print(\"DINOv2 Image Transform:\")\n",
    "print(f\"  Input size: {config['input_size']}\")\n",
    "print(f\"  Mean: {config['mean']}\")\n",
    "print(f\"  Std: {config['std']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and process a sample robot image\n",
    "sample_image = Image.fromarray(\n",
    "    np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    ")\n",
    "\n",
    "# Transform image\n",
    "input_tensor = transform(sample_image).unsqueeze(0)\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    # Get patch features (not pooled)\n",
    "    features = dinov2.forward_features(input_tensor)\n",
    "    print(f\"DINOv2 output shape: {features.shape}\")\n",
    "    print(f\"  - Batch: {features.shape[0]}\")\n",
    "    print(f\"  - Tokens: {features.shape[1]} (1 CLS + 256 patches)\")\n",
    "    print(f\"  - Feature dim: {features.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns in DINOv2\n",
    "def get_attention_maps(model, image_tensor):\n",
    "    \"\"\"Extract attention maps from ViT.\"\"\"\n",
    "    attention_maps = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # Output is (attn_output, attn_weights) for some implementations\n",
    "        if isinstance(output, tuple):\n",
    "            attention_maps.append(output[1].detach())\n",
    "    \n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hook = block.attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attention_maps\n",
    "\n",
    "print(\"DINOv2 attention patterns capture semantic structure:\")\n",
    "print(\"  - Different heads attend to different semantic parts\")\n",
    "print(\"  - Early layers: low-level features (edges, textures)\")\n",
    "print(\"  - Middle layers: object parts\")\n",
    "print(\"  - Late layers: high-level semantics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. SigLIP: Text-Aligned Visual Features\n",
    "\n",
    "**SigLIP** (Sigmoid Loss for Language-Image Pre-training) learns to align images with text.\n",
    "\n",
    "### Training Approach\n",
    "- Contrastive learning on image-text pairs\n",
    "- Sigmoid loss (more efficient than softmax)\n",
    "- Large-scale web data (billions of pairs)\n",
    "\n",
    "### What SigLIP Captures\n",
    "- Text-image correspondence\n",
    "- Compositional understanding (\"red ball on table\")\n",
    "- Action-relevant features\n",
    "- Language-grounded concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SigLIP for exploration\n",
    "print(\"Loading SigLIP-Large...\")\n",
    "siglip = timm.create_model('vit_large_patch16_siglip_256', pretrained=True)\n",
    "siglip.eval()\n",
    "print(f\"SigLIP loaded: {sum(p.numel() for p in siglip.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DINOv2 vs SigLIP architectures\n",
    "print(\"Architecture Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Feature':<25} {'DINOv2-L':<15} {'SigLIP-L':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Patch size':<25} {'14×14':<15} {'16×16':<15}\")\n",
    "print(f\"{'Hidden dim':<25} {dinov2.embed_dim:<15} {siglip.embed_dim:<15}\")\n",
    "print(f\"{'Num layers':<25} {len(dinov2.blocks):<15} {len(siglip.blocks):<15}\")\n",
    "print(f\"{'Training':<25} {'Self-supervised':<15} {'Contrastive':<15}\")\n",
    "print(f\"{'Strength':<25} {'Semantics':<15} {'Text align':<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process same image through SigLIP\n",
    "siglip_config = resolve_data_config({}, model=siglip)\n",
    "siglip_transform = create_transform(**siglip_config)\n",
    "\n",
    "siglip_input = siglip_transform(sample_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    siglip_features = siglip.forward_features(siglip_input)\n",
    "    print(f\"SigLIP output shape: {siglip_features.shape}\")\n",
    "    print(f\"  - Batch: {siglip_features.shape[0]}\")\n",
    "    print(f\"  - Tokens: {siglip_features.shape[1]}\")\n",
    "    print(f\"  - Feature dim: {siglip_features.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Fusion in OpenVLA\n",
    "\n",
    "OpenVLA combines both encoders to get the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_diagram = \"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    OpenVLA Feature Fusion                           │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│                      [Input Image 224×224]                          │\n",
    "│                              │                                       │\n",
    "│                 ┌────────────┴────────────┐                         │\n",
    "│                 │                         │                          │\n",
    "│                 ▼                         ▼                          │\n",
    "│        ┌──────────────┐          ┌──────────────┐                   │\n",
    "│        │   DINOv2     │          │   SigLIP     │                   │\n",
    "│        │   ViT-L/14   │          │   ViT-L/16   │                   │\n",
    "│        └──────┬───────┘          └──────┬───────┘                   │\n",
    "│               │                         │                            │\n",
    "│               ▼                         ▼                            │\n",
    "│     [B, 257, 1024]              [B, 257, 1024]                       │\n",
    "│     (Rich semantics)            (Text-aligned)                       │\n",
    "│               │                         │                            │\n",
    "│               └──────────┬──────────────┘                           │\n",
    "│                          │                                           │\n",
    "│                          ▼                                           │\n",
    "│               ┌──────────────────┐                                   │\n",
    "│               │   Concatenate    │                                   │\n",
    "│               │   along patches  │                                   │\n",
    "│               └────────┬─────────┘                                   │\n",
    "│                        │                                             │\n",
    "│                        ▼                                             │\n",
    "│               [B, 514, 1024]                                         │\n",
    "│               (Fused features)                                       │\n",
    "│                        │                                             │\n",
    "│                        ▼                                             │\n",
    "│               ┌──────────────────┐                                   │\n",
    "│               │    Projector     │                                   │\n",
    "│               │  1024 → 4096     │                                   │\n",
    "│               └────────┬─────────┘                                   │\n",
    "│                        │                                             │\n",
    "│                        ▼                                             │\n",
    "│               [B, 514, 4096]                                         │\n",
    "│               (LLM-ready tokens)                                     │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "print(fusion_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actual OpenVLA vision backbone\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "print(\"Loading OpenVLA to inspect vision backbone...\")\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore OpenVLA's vision backbone structure\n",
    "vision_backbone = vla.vision_backbone\n",
    "\n",
    "print(\"OpenVLA Vision Backbone:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Type: {type(vision_backbone).__name__}\")\n",
    "\n",
    "# List sub-components\n",
    "for name, child in vision_backbone.named_children():\n",
    "    params = sum(p.numel() for p in child.parameters())\n",
    "    print(f\"  {name}: {params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace feature extraction\n",
    "sample_image = Image.fromarray(\n",
    "    np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    ")\n",
    "\n",
    "# Process through OpenVLA's processor\n",
    "inputs = processor(\"Pick up the red block\", sample_image)\n",
    "pixel_values = inputs['pixel_values'].to(vla.dtype)\n",
    "\n",
    "print(f\"Pixel values shape: {pixel_values.shape}\")\n",
    "\n",
    "# Extract vision features\n",
    "with torch.no_grad():\n",
    "    vision_features = vla.vision_backbone(pixel_values)\n",
    "    print(f\"\\nVision backbone output: {vision_features.shape}\")\n",
    "    \n",
    "    # Project to LLM space\n",
    "    projected = vla.projector(vision_features)\n",
    "    print(f\"Projected features: {projected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Why This Combination Works for Robotics\n",
    "\n",
    "### DINOv2 Contribution\n",
    "- **Object discovery**: Naturally segments objects without labels\n",
    "- **Spatial understanding**: Captures where things are\n",
    "- **Part-whole relationships**: Understands object structure\n",
    "\n",
    "### SigLIP Contribution  \n",
    "- **Instruction grounding**: Maps language to visual concepts\n",
    "- **Compositional understanding**: \"red block\" vs \"blue block\"\n",
    "- **Action relevance**: Learned from web data describing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual comparison of what each encoder captures\n",
    "comparison_table = \"\"\"\n",
    "┌─────────────────┬──────────────────────┬──────────────────────┐\n",
    "│  Robot Task     │  DINOv2 Captures     │  SigLIP Captures     │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│                 │                      │                      │\n",
    "│ \"Pick up the    │ - Object boundaries  │ - \"Red\" vs \"blue\"    │\n",
    "│  red block\"     │ - Block shape/size   │ - \"Block\" concept    │\n",
    "│                 │ - Spatial position   │ - \"Pick up\" action   │\n",
    "│                 │                      │                      │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│                 │                      │                      │\n",
    "│ \"Place cup in   │ - Cup outline        │ - \"Cup\" vs \"mug\"     │\n",
    "│  the drawer\"    │ - Drawer structure   │ - \"In\" relationship  │\n",
    "│                 │ - Opening detection  │ - \"Drawer\" concept   │\n",
    "│                 │                      │                      │\n",
    "├─────────────────┼──────────────────────┼──────────────────────┤\n",
    "│                 │                      │                      │\n",
    "│ \"Stack blocks   │ - Each block's       │ - \"Stack\" action     │\n",
    "│  by size\"       │   position           │ - Size comparison    │\n",
    "│                 │ - Relative sizes     │ - Order concept      │\n",
    "│                 │                      │                      │\n",
    "└─────────────────┴──────────────────────┴──────────────────────┘\n",
    "\"\"\"\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Practical Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of feature activations\n",
    "def visualize_feature_statistics(features, name):\n",
    "    \"\"\"Visualize basic statistics of extracted features.\"\"\"\n",
    "    features_np = features.detach().float().numpy()\n",
    "    \n",
    "    print(f\"\\n{name} Feature Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Shape: {features_np.shape}\")\n",
    "    print(f\"Mean: {features_np.mean():.4f}\")\n",
    "    print(f\"Std: {features_np.std():.4f}\")\n",
    "    print(f\"Min: {features_np.min():.4f}\")\n",
    "    print(f\"Max: {features_np.max():.4f}\")\n",
    "    \n",
    "    # Feature magnitude per token\n",
    "    token_magnitudes = np.linalg.norm(features_np[0], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(features_np.flatten(), bins=50, alpha=0.7)\n",
    "    plt.title(f\"{name}: Feature Value Distribution\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(token_magnitudes)\n",
    "    plt.title(f\"{name}: Token Magnitude\")\n",
    "    plt.xlabel(\"Token Index\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the fused features\n",
    "visualize_feature_statistics(vision_features, \"Fused Vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the projected features\n",
    "visualize_feature_statistics(projected, \"Projected (LLM-ready)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with standalone encoders\n",
    "with torch.no_grad():\n",
    "    # DINOv2 alone\n",
    "    dino_input = transform(sample_image).unsqueeze(0)\n",
    "    dino_features = dinov2.forward_features(dino_input)\n",
    "    visualize_feature_statistics(dino_features, \"DINOv2 Only\")\n",
    "    \n",
    "    # SigLIP alone\n",
    "    siglip_input = siglip_transform(sample_image).unsqueeze(0)\n",
    "    siglip_feats = siglip.forward_features(siglip_input)\n",
    "    visualize_feature_statistics(siglip_feats, \"SigLIP Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. The Projector: Bridging Vision and Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the projector architecture\n",
    "projector = vla.projector\n",
    "\n",
    "print(\"Projector Architecture:\")\n",
    "print(\"=\"*60)\n",
    "for name, module in projector.named_modules():\n",
    "    if name:  # Skip root\n",
    "        if hasattr(module, 'in_features'):\n",
    "            print(f\"{name}: Linear({module.in_features} → {module.out_features})\")\n",
    "        elif hasattr(module, '__class__'):\n",
    "            print(f\"{name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding projector dimensionality\n",
    "print(\"\\nDimensionality Flow:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vision features: {vision_features.shape[-1]} dims\")\n",
    "print(f\"LLM embedding: {vla.llm_backbone.llm.config.hidden_size} dims\")\n",
    "print(f\"Projected: {projected.shape[-1]} dims\")\n",
    "print(f\"\\nThe projector maps vision features to match LLM embedding dimension.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Dual Vision Encoder**: OpenVLA uses both DINOv2 and SigLIP\n",
    "   - DINOv2: Rich semantic features from self-supervised learning\n",
    "   - SigLIP: Text-aligned features from contrastive learning\n",
    "\n",
    "2. **ViT Architecture**: Both use Vision Transformers\n",
    "   - Image → patches → tokens → self-attention → features\n",
    "   - ~257 tokens per encoder (1 CLS + 256 patches)\n",
    "\n",
    "3. **Feature Fusion**: Simple concatenation\n",
    "   - Combines complementary information\n",
    "   - Preserves both semantic richness and text alignment\n",
    "\n",
    "4. **Projector**: Maps to LLM space\n",
    "   - Vision dim (1024) → LLM dim (4096)\n",
    "   - Enables cross-modal attention in the LLM\n",
    "\n",
    "### Why This Matters for Robot Actions\n",
    "- DINOv2 tells the model **where objects are** and their **structure**\n",
    "- SigLIP tells the model **what the instruction means** visually\n",
    "- Together, they enable precise action prediction\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **04_action_tokenization.ipynb** to understand how continuous actions become tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del vla, dinov2, siglip\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
