{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Vision Backbone Deep Dive\n",
    "\n",
    "**Goal**: Understand how OpenVLA processes visual information using DINOv2 and SigLIP.\n",
    "\n",
    "## What We'll Learn\n",
    "1. Vision Transformer (ViT) fundamentals\n",
    "2. DINOv2: Self-supervised visual features\n",
    "3. SigLIP: Text-aligned visual features\n",
    "4. Feature fusion strategy\n",
    "5. Practical visualization of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Vision Transformer Fundamentals\n",
    "\n",
    "Both DINOv2 and SigLIP are based on the **Vision Transformer (ViT)** architecture.\n",
    "\n",
    "### How ViT Works\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  Vision Transformer Pipeline                    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Input Image (224 \u00d7 224 \u00d7 3)                                   \u2502\n",
    "\u2502           \u2502                                                     \u2502\n",
    "\u2502           \u25bc                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502 1. Patch Extraction                       \u2502                  \u2502\n",
    "\u2502  \u2502    Split into 14\u00d714 patches (16\u00d716 each) \u2502                  \u2502\n",
    "\u2502  \u2502    = 196 patches                          \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502           \u2502                                                     \u2502\n",
    "\u2502           \u25bc                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502 2. Linear Embedding                       \u2502                  \u2502\n",
    "\u2502  \u2502    Each patch \u2192 1024-dim vector          \u2502                  \u2502\n",
    "\u2502  \u2502    + [CLS] token + Position embeddings   \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502           \u2502                                                     \u2502\n",
    "\u2502           \u25bc                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502 3. Transformer Encoder (24 layers)        \u2502                  \u2502\n",
    "\u2502  \u2502    Multi-Head Self-Attention              \u2502                  \u2502\n",
    "\u2502  \u2502    + Feed-Forward Networks                \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502           \u2502                                                     \u2502\n",
    "\u2502           \u25bc                                                     \u2502\n",
    "\u2502  Output: 197 tokens \u00d7 1024 dims                                \u2502\n",
    "\u2502  ([CLS] token + 196 patch tokens)                              \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL: Set environment variables BEFORE importing packages!\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# Auto-detect environment (NERSC vs SciServer)\n",
    "import os\n",
    "if os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']  # NERSC Perlmutter\n",
    "elif os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']  # Generic scratch\n",
    "else:\n",
    "    SCRATCH = \"/home/idies/workspace/Temporary/dpark1/scratch\"  # SciServer default  # CHANGE THIS TO YOUR PATH\n",
    "CACHE_DIR = f\"{SCRATCH}/.cache\"\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = CACHE_DIR\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\n",
    "os.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n",
    "\n",
    "for path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"\u2705 All caches \u2192 {CACHE_DIR}\")\n",
    "\n",
    "# Now import packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize patch extraction\n",
    "def visualize_patches(image_size=224, patch_size=16):\n",
    "    \"\"\"Visualize how an image is divided into patches.\"\"\"\n",
    "    n_patches = image_size // patch_size\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Create sample image\n",
    "    sample_img = np.random.randint(0, 255, (image_size, image_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(sample_img)\n",
    "    axes[0].set_title(f\"Original Image ({image_size}\u00d7{image_size})\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Image with patch grid\n",
    "    axes[1].imshow(sample_img)\n",
    "    for i in range(n_patches + 1):\n",
    "        axes[1].axhline(y=i * patch_size, color='red', linewidth=1)\n",
    "        axes[1].axvline(x=i * patch_size, color='red', linewidth=1)\n",
    "    axes[1].set_title(f\"Patches ({n_patches}\u00d7{n_patches} = {n_patches**2} patches)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nViT-L/14 Configuration:\")\n",
    "    print(f\"  Image size: {image_size}\u00d7{image_size}\")\n",
    "    print(f\"  Patch size: {patch_size}\u00d7{patch_size}\")\n",
    "    print(f\"  Number of patches: {n_patches**2}\")\n",
    "    print(f\"  + 1 [CLS] token = {n_patches**2 + 1} tokens total\")\n",
    "\n",
    "visualize_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DINOv2: Self-Supervised Visual Features\n",
    "\n",
    "**DINO** (Distillation with NO labels) learns visual features without any labeled data.\n",
    "\n",
    "### Training Approach\n",
    "- Teacher-student self-distillation\n",
    "- Student learns to match teacher's output on augmented views\n",
    "- Discovers semantic structure naturally\n",
    "\n",
    "### What DINOv2 Captures\n",
    "- Object boundaries and parts\n",
    "- Semantic segmentation (emergent property)\n",
    "- Spatial relationships\n",
    "- Rich local features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2 directly for exploration\n",
    "import timm\n",
    "\n",
    "print(\"Loading DINOv2-Large...\")\n",
    "dinov2 = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True)\n",
    "dinov2.eval()\n",
    "print(f\"DINOv2 loaded: {sum(p.numel() for p in dinov2.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect DINOv2 architecture\n",
    "print(\"DINOv2-L Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Patch embedding: {dinov2.patch_embed}\")\n",
    "print(f\"Num transformer blocks: {len(dinov2.blocks)}\")\n",
    "print(f\"Hidden dimension: {dinov2.embed_dim}\")\n",
    "print(f\"Num attention heads: {dinov2.blocks[0].attn.num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process an image through DINOv2\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "# Get DINOv2's preprocessing config\n",
    "config = resolve_data_config({}, model=dinov2)\n",
    "transform = create_transform(**config)\n",
    "\n",
    "print(\"DINOv2 Image Transform:\")\n",
    "print(f\"  Input size: {config['input_size']}\")\n",
    "print(f\"  Mean: {config['mean']}\")\n",
    "print(f\"  Std: {config['std']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and process a sample robot image\n",
    "sample_image = Image.fromarray(\n",
    "    np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    ")\n",
    "\n",
    "# Transform image\n",
    "input_tensor = transform(sample_image).unsqueeze(0)\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    # Get patch features (not pooled)\n",
    "    features = dinov2.forward_features(input_tensor)\n",
    "    print(f\"DINOv2 output shape: {features.shape}\")\n",
    "    print(f\"  - Batch: {features.shape[0]}\")\n",
    "    print(f\"  - Tokens: {features.shape[1]} (1 CLS + 256 patches)\")\n",
    "    print(f\"  - Feature dim: {features.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns in DINOv2\n",
    "def get_attention_maps(model, image_tensor):\n",
    "    \"\"\"Extract attention maps from ViT.\"\"\"\n",
    "    attention_maps = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # Output is (attn_output, attn_weights) for some implementations\n",
    "        if isinstance(output, tuple):\n",
    "            attention_maps.append(output[1].detach())\n",
    "    \n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hook = block.attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attention_maps\n",
    "\n",
    "print(\"DINOv2 attention patterns capture semantic structure:\")\n",
    "print(\"  - Different heads attend to different semantic parts\")\n",
    "print(\"  - Early layers: low-level features (edges, textures)\")\n",
    "print(\"  - Middle layers: object parts\")\n",
    "print(\"  - Late layers: high-level semantics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. SigLIP: Text-Aligned Visual Features\n",
    "\n",
    "**SigLIP** (Sigmoid Loss for Language-Image Pre-training) learns to align images with text.\n",
    "\n",
    "### Training Approach\n",
    "- Contrastive learning on image-text pairs\n",
    "- Sigmoid loss (more efficient than softmax)\n",
    "- Large-scale web data (billions of pairs)\n",
    "\n",
    "### What SigLIP Captures\n",
    "- Text-image correspondence\n",
    "- Compositional understanding (\"red ball on table\")\n",
    "- Action-relevant features\n",
    "- Language-grounded concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SigLIP for exploration\n",
    "print(\"Loading SigLIP-Large...\")\n",
    "siglip = timm.create_model('vit_large_patch16_siglip_256', pretrained=True)\n",
    "siglip.eval()\n",
    "print(f\"SigLIP loaded: {sum(p.numel() for p in siglip.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DINOv2 vs SigLIP architectures\n",
    "print(\"Architecture Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Feature':<25} {'DINOv2-L':<15} {'SigLIP-L':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Patch size':<25} {'14\u00d714':<15} {'16\u00d716':<15}\")\n",
    "print(f\"{'Hidden dim':<25} {dinov2.embed_dim:<15} {siglip.embed_dim:<15}\")\n",
    "print(f\"{'Num layers':<25} {len(dinov2.blocks):<15} {len(siglip.blocks):<15}\")\n",
    "print(f\"{'Training':<25} {'Self-supervised':<15} {'Contrastive':<15}\")\n",
    "print(f\"{'Strength':<25} {'Semantics':<15} {'Text align':<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process same image through SigLIP\n",
    "siglip_config = resolve_data_config({}, model=siglip)\n",
    "siglip_transform = create_transform(**siglip_config)\n",
    "\n",
    "siglip_input = siglip_transform(sample_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    siglip_features = siglip.forward_features(siglip_input)\n",
    "    print(f\"SigLIP output shape: {siglip_features.shape}\")\n",
    "    print(f\"  - Batch: {siglip_features.shape[0]}\")\n",
    "    print(f\"  - Tokens: {siglip_features.shape[1]}\")\n",
    "    print(f\"  - Feature dim: {siglip_features.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Fusion in OpenVLA\n",
    "\n",
    "OpenVLA combines both encoders to get the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_diagram = \"\"\"\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    OpenVLA Feature Fusion                           \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502                      [Input Image 224\u00d7224]                          \u2502\n",
    "\u2502                              \u2502                                       \u2502\n",
    "\u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n",
    "\u2502                 \u2502                         \u2502                          \u2502\n",
    "\u2502                 \u25bc                         \u25bc                          \u2502\n",
    "\u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n",
    "\u2502        \u2502   DINOv2     \u2502          \u2502   SigLIP     \u2502                   \u2502\n",
    "\u2502        \u2502   ViT-L/14   \u2502          \u2502   ViT-L/16   \u2502                   \u2502\n",
    "\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n",
    "\u2502               \u2502                         \u2502                            \u2502\n",
    "\u2502               \u25bc                         \u25bc                            \u2502\n",
    "\u2502     [B, 257, 1024]              [B, 257, 1024]                       \u2502\n",
    "\u2502     (Rich semantics)            (Text-aligned)                       \u2502\n",
    "\u2502               \u2502                         \u2502                            \u2502\n",
    "\u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n",
    "\u2502                          \u2502                                           \u2502\n",
    "\u2502                          \u25bc                                           \u2502\n",
    "\u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n",
    "\u2502               \u2502   Concatenate    \u2502                                   \u2502\n",
    "\u2502               \u2502   along patches  \u2502                                   \u2502\n",
    "\u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n",
    "\u2502                        \u2502                                             \u2502\n",
    "\u2502                        \u25bc                                             \u2502\n",
    "\u2502               [B, 514, 1024]                                         \u2502\n",
    "\u2502               (Fused features)                                       \u2502\n",
    "\u2502                        \u2502                                             \u2502\n",
    "\u2502                        \u25bc                                             \u2502\n",
    "\u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n",
    "\u2502               \u2502    Projector     \u2502                                   \u2502\n",
    "\u2502               \u2502  1024 \u2192 4096     \u2502                                   \u2502\n",
    "\u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n",
    "\u2502                        \u2502                                             \u2502\n",
    "\u2502                        \u25bc                                             \u2502\n",
    "\u2502               [B, 514, 4096]                                         \u2502\n",
    "\u2502               (LLM-ready tokens)                                     \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\"\"\"\n",
    "print(fusion_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load actual OpenVLA to inspect vision backbone\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\n\nprint(\"Loading OpenVLA to inspect vision backbone...\")\nprint(\"Note: This uses transformers==4.40.1 for compatibility\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nvla = AutoModelForVision2Seq.from_pretrained(\n    \"openvla/openvla-7b\",\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True,\n).to(device)\n\nprocessor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n\n# Dynamically discover model components\nprint(\"\\nOpenVLA Model Components:\")\nprint(\"=\"*60)\nfor name, child in vla.named_children():\n    params = sum(p.numel() for p in child.parameters()) / 1e6\n    print(f\"  {name}: {params:.1f}M parameters\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore OpenVLA's vision backbone structure\n# Dynamically find vision-related component\nvision_backbone = None\nfor name, child in vla.named_children():\n    if 'vision' in name.lower():\n        vision_backbone = child\n        vision_name = name\n        break\n\nif vision_backbone is not None:\n    print(f\"OpenVLA Vision Backbone ({vision_name}):\")\n    print(\"=\"*60)\n    print(f\"Type: {type(vision_backbone).__name__}\")\n    \n    # List sub-components\n    for name, sub_child in vision_backbone.named_children():\n        params = sum(p.numel() for p in sub_child.parameters())\n        print(f\"  {name}: {params/1e6:.1f}M parameters\")\nelse:\n    print(\"Vision backbone not found. Available components:\")\n    for name, child in vla.named_children():\n        print(f\"  - {name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trace feature extraction\nsample_image = Image.fromarray(\n    np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n)\n\n# Process through OpenVLA's processor\ninputs = processor(\"Pick up the red block\", sample_image)\n\n# Convert to bfloat16 and move to GPU\npixel_values = inputs['pixel_values'].to(device, dtype=torch.bfloat16)\n\nprint(f\"Pixel values shape: {pixel_values.shape}\")\nprint(f\"Pixel values dtype: {pixel_values.dtype}\")\nprint(f\"Pixel values device: {pixel_values.device}\")\n\n# Extract vision features using dynamically discovered components\nif vision_backbone is not None:\n    with torch.no_grad():\n        vision_features = vision_backbone(pixel_values)\n        print(f\"\\nVision backbone output: {vision_features.shape}\")\n        \n        # Find projector dynamically\n        projector = None\n        for name, child in vla.named_children():\n            if 'project' in name.lower():\n                projector = child\n                break\n        \n        if projector is not None:\n            projected = projector(vision_features)\n            print(f\"Projected features: {projected.shape}\")\n        else:\n            print(\"Projector not found in model components\")\nelse:\n    print(\"Cannot extract features: vision backbone not found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Why This Combination Works for Robotics\n",
    "\n",
    "### DINOv2 Contribution\n",
    "- **Object discovery**: Naturally segments objects without labels\n",
    "- **Spatial understanding**: Captures where things are\n",
    "- **Part-whole relationships**: Understands object structure\n",
    "\n",
    "### SigLIP Contribution  \n",
    "- **Instruction grounding**: Maps language to visual concepts\n",
    "- **Compositional understanding**: \"red block\" vs \"blue block\"\n",
    "- **Action relevance**: Learned from web data describing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual comparison of what each encoder captures\n",
    "comparison_table = \"\"\"\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Robot Task     \u2502  DINOv2 Captures     \u2502  SigLIP Captures     \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                 \u2502                      \u2502                      \u2502\n",
    "\u2502 \"Pick up the    \u2502 - Object boundaries  \u2502 - \"Red\" vs \"blue\"    \u2502\n",
    "\u2502  red block\"     \u2502 - Block shape/size   \u2502 - \"Block\" concept    \u2502\n",
    "\u2502                 \u2502 - Spatial position   \u2502 - \"Pick up\" action   \u2502\n",
    "\u2502                 \u2502                      \u2502                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                 \u2502                      \u2502                      \u2502\n",
    "\u2502 \"Place cup in   \u2502 - Cup outline        \u2502 - \"Cup\" vs \"mug\"     \u2502\n",
    "\u2502  the drawer\"    \u2502 - Drawer structure   \u2502 - \"In\" relationship  \u2502\n",
    "\u2502                 \u2502 - Opening detection  \u2502 - \"Drawer\" concept   \u2502\n",
    "\u2502                 \u2502                      \u2502                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                 \u2502                      \u2502                      \u2502\n",
    "\u2502 \"Stack blocks   \u2502 - Each block's       \u2502 - \"Stack\" action     \u2502\n",
    "\u2502  by size\"       \u2502   position           \u2502 - Size comparison    \u2502\n",
    "\u2502                 \u2502 - Relative sizes     \u2502 - Order concept      \u2502\n",
    "\u2502                 \u2502                      \u2502                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\"\"\"\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Practical Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of feature activations\n",
    "def visualize_feature_statistics(features, name):\n",
    "    \"\"\"Visualize basic statistics of extracted features.\"\"\"\n",
    "    features_np = features.detach().float().numpy()\n",
    "    \n",
    "    print(f\"\\n{name} Feature Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Shape: {features_np.shape}\")\n",
    "    print(f\"Mean: {features_np.mean():.4f}\")\n",
    "    print(f\"Std: {features_np.std():.4f}\")\n",
    "    print(f\"Min: {features_np.min():.4f}\")\n",
    "    print(f\"Max: {features_np.max():.4f}\")\n",
    "    \n",
    "    # Feature magnitude per token\n",
    "    token_magnitudes = np.linalg.norm(features_np[0], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(features_np.flatten(), bins=50, alpha=0.7)\n",
    "    plt.title(f\"{name}: Feature Value Distribution\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(token_magnitudes)\n",
    "    plt.title(f\"{name}: Token Magnitude\")\n",
    "    plt.xlabel(\"Token Index\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the fused features\n",
    "visualize_feature_statistics(vision_features, \"Fused Vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the projected features\n",
    "visualize_feature_statistics(projected, \"Projected (LLM-ready)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with standalone encoders\n",
    "with torch.no_grad():\n",
    "    # DINOv2 alone\n",
    "    dino_input = transform(sample_image).unsqueeze(0)\n",
    "    dino_features = dinov2.forward_features(dino_input)\n",
    "    visualize_feature_statistics(dino_features, \"DINOv2 Only\")\n",
    "    \n",
    "    # SigLIP alone\n",
    "    siglip_input = siglip_transform(sample_image).unsqueeze(0)\n",
    "    siglip_feats = siglip.forward_features(siglip_input)\n",
    "    visualize_feature_statistics(siglip_feats, \"SigLIP Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. The Projector: Bridging Vision and Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect the projector architecture\n# Find projector dynamically\nprojector = None\nfor name, child in vla.named_children():\n    if 'project' in name.lower():\n        projector = child\n        projector_name = name\n        break\n\nif projector is not None:\n    print(f\"Projector Architecture ({projector_name}):\")\n    print(\"=\"*60)\n    for name, module in projector.named_modules():\n        if name:  # Skip root\n            if hasattr(module, 'in_features'):\n                print(f\"{name}: Linear({module.in_features} \u2192 {module.out_features})\")\n            elif hasattr(module, '__class__'):\n                print(f\"{name}: {module.__class__.__name__}\")\nelse:\n    print(\"Projector not found in model components\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Understanding projector dimensionality\nprint(\"\\nDimensionality Flow:\")\nprint(\"=\"*60)\n\nif 'vision_features' in dir():\n    print(f\"Vision features: {vision_features.shape[-1]} dims\")\nelse:\n    print(\"Vision features: (run previous cells first)\")\n\n# Find LLM backbone dynamically\nllm_backbone = None\nfor name, child in vla.named_children():\n    if 'llm' in name.lower() or 'language' in name.lower():\n        llm_backbone = child\n        break\n\nif llm_backbone is not None:\n    # Try to get hidden size from config or nested model\n    hidden_size = None\n    if hasattr(llm_backbone, 'config') and hasattr(llm_backbone.config, 'hidden_size'):\n        hidden_size = llm_backbone.config.hidden_size\n    elif hasattr(llm_backbone, 'llm') and hasattr(llm_backbone.llm, 'config'):\n        hidden_size = llm_backbone.llm.config.hidden_size\n    \n    if hidden_size:\n        print(f\"LLM embedding: {hidden_size} dims\")\n    else:\n        print(\"LLM embedding: could not determine hidden size\")\nelse:\n    print(\"LLM backbone not found\")\n\nif 'projected' in dir():\n    print(f\"Projected: {projected.shape[-1]} dims\")\nelse:\n    print(\"Projected: (run previous cells first)\")\n\nprint(f\"\\nThe projector maps vision features to match LLM embedding dimension.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Dual Vision Encoder**: OpenVLA uses both DINOv2 and SigLIP\n",
    "   - DINOv2: Rich semantic features from self-supervised learning\n",
    "   - SigLIP: Text-aligned features from contrastive learning\n",
    "\n",
    "2. **ViT Architecture**: Both use Vision Transformers\n",
    "   - Image \u2192 patches \u2192 tokens \u2192 self-attention \u2192 features\n",
    "   - ~257 tokens per encoder (1 CLS + 256 patches)\n",
    "\n",
    "3. **Feature Fusion**: Simple concatenation\n",
    "   - Combines complementary information\n",
    "   - Preserves both semantic richness and text alignment\n",
    "\n",
    "4. **Projector**: Maps to LLM space\n",
    "   - Vision dim (1024) \u2192 LLM dim (4096)\n",
    "   - Enables cross-modal attention in the LLM\n",
    "\n",
    "### Why This Matters for Robot Actions\n",
    "- DINOv2 tells the model **where objects are** and their **structure**\n",
    "- SigLIP tells the model **what the instruction means** visually\n",
    "- Together, they enable precise action prediction\n",
    "\n",
    "### Next Steps\n",
    "\u2192 Continue to **04_action_tokenization.ipynb** to understand how continuous actions become tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del vla, dinov2, siglip\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}