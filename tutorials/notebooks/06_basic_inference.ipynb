{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Basic Inference with OpenVLA\n",
    "\n",
    "**Goal**: Run OpenVLA-7B inference on sample images to predict robot actions.\n",
    "\n",
    "## What We'll Learn\n",
    "1. Loading OpenVLA from HuggingFace\n",
    "2. Processing inputs (image + instruction)\n",
    "3. Generating action predictions\n",
    "4. Multi-GPU inference strategies\n",
    "5. Batch inference for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load OpenVLA Model\n",
    "\n",
    "We'll use the HuggingFace transformers library for easy model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CRITICAL: Set these BEFORE importing any packages!\n# ============================================================\nimport os\n\n# For NERSC Perlmutter, use your $PSCRATCH directory\nPSCRATCH = \"/pscratch/sd/d/dpark1\"  # CHANGE THIS TO YOUR PATH\nCACHE_DIR = f\"{PSCRATCH}/.cache\"\n\n# Set all cache directories to $PSCRATCH/.cache\nos.environ['XDG_CACHE_HOME'] = CACHE_DIR\nos.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\nos.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\nos.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n\n# Create directories\nfor path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n    os.makedirs(path, exist_ok=True)\n\nprint(f\"✅ All caches → {CACHE_DIR}\")\n\n# Now import other packages\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nimport time\n\n# Check GPU availability\nprint(\"\\nGPU Configuration:\")\nprint(f\"  CUDA available: {torch.cuda.is_available()}\")\nprint(f\"  GPU count: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    props = torch.cuda.get_device_properties(i)\n    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1e9:.1f} GB)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_ID = \"openvla/openvla-7b\"\n",
    "DEVICE = \"cuda:0\"  # Use first GPU\n",
    "DTYPE = torch.bfloat16  # BF16 for memory efficiency\n",
    "\n",
    "print(f\"Loading OpenVLA from {MODEL_ID}...\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor (handles image and text preprocessing)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Processor loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model with optimizations\n# Check if Flash Attention 2 is available\ntry:\n    import flash_attn\n    ATTN_IMPL = \"flash_attention_2\"\n    print(\"✅ Flash Attention 2 available - using for 2-3x faster attention\")\nexcept ImportError:\n    ATTN_IMPL = None  # Use default attention\n    print(\"⚠️ Flash Attention 2 not installed - using default attention\")\n    print(\"   To install: pip install flash-attn --no-build-isolation\")\n\n# Build model kwargs\nmodel_kwargs = {\n    \"torch_dtype\": DTYPE,\n    \"low_cpu_mem_usage\": True,\n    \"trust_remote_code\": True,\n}\nif ATTN_IMPL:\n    model_kwargs[\"attn_implementation\"] = ATTN_IMPL\n\nvla = AutoModelForVision2Seq.from_pretrained(\n    MODEL_ID,\n    **model_kwargs\n).to(DEVICE)\n\n# Set to evaluation mode\nvla.eval()\n\nprint(f\"\\nModel loaded!\")\nprint(f\"  Parameters: {sum(p.numel() for p in vla.parameters()) / 1e9:.2f}B\")\nprint(f\"  Memory: ~{torch.cuda.memory_allocated() / 1e9:.1f} GB allocated\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Prepare Input Data\n",
    "\n",
    "OpenVLA needs:\n",
    "- RGB image from robot camera\n",
    "- Natural language instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample robot observation image\n",
    "# In practice, this would come from your robot's camera\n",
    "\n",
    "def create_sample_observation(size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Create a sample robot observation.\n",
    "    \n",
    "    In real usage:\n",
    "    - image = camera.get_rgb_image()\n",
    "    - instruction = task_specification\n",
    "    \"\"\"\n",
    "    # Simulated robot workspace image\n",
    "    # Add some structure to make it more realistic\n",
    "    img = np.zeros((size[0], size[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Background (table)\n",
    "    img[:, :] = [200, 180, 160]  # Brownish table\n",
    "    \n",
    "    # Add some \"objects\"\n",
    "    # Red block\n",
    "    img[100:150, 80:130] = [200, 50, 50]  # Red\n",
    "    # Blue cube\n",
    "    img[80:120, 160:200] = [50, 50, 200]  # Blue\n",
    "    # Green cylinder (approximated)\n",
    "    img[140:180, 140:170] = [50, 200, 50]  # Green\n",
    "    \n",
    "    return Image.fromarray(img)\n",
    "\n",
    "# Create sample image\n",
    "sample_image = create_sample_observation()\n",
    "\n",
    "# Display the image (in Jupyter)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.title(\"Sample Robot Observation\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {sample_image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the task instruction\n",
    "instruction = \"pick up the red block\"\n",
    "\n",
    "# Format as OpenVLA prompt\n",
    "# The model expects: \"In: What action should the robot take to {task}?\\nOut:\"\n",
    "prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n",
    "print(\"Input prompt:\")\n",
    "print(f\"  '{prompt}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Process Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image and text with the processor\n",
    "inputs = processor(prompt, sample_image)\n",
    "\n",
    "print(\"Processed inputs:\")\n",
    "for key, value in inputs.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inputs to device\n",
    "inputs_device = {\n",
    "    k: v.to(DEVICE, dtype=DTYPE) if isinstance(v, torch.Tensor) else v \n",
    "    for k, v in inputs.items()\n",
    "}\n",
    "\n",
    "print(\"Inputs moved to device:\")\n",
    "for key, value in inputs_device.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: device={value.device}, dtype={value.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Generate Action Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# First, see available unnorm_key options\nprint(\"Available unnorm_key options (dataset statistics):\")\nprint(\"=\"*60)\nfor key in sorted(vla.norm_stats.keys()):\n    print(f\"  - {key}\")\n\n# Common choices:\n# - bridge_orig: Good for tabletop manipulation (WidowX, similar to Franka)\n# - fractal20220817_data: Google RT-1 dataset\n# - kuka: KUKA robot grasping"
  },
  {
   "cell_type": "code",
   "source": "# Generate action prediction with unnorm_key\n# IMPORTANT: Must specify unnorm_key since model was trained on multiple datasets\n\nUNNORM_KEY = \"bridge_orig\"  # Use Bridge dataset statistics (good for tabletop manipulation)\n\nprint(f\"Generating action prediction using '{UNNORM_KEY}' statistics...\")\n\nwith torch.no_grad():\n    start_time = time.time()\n    \n    action = vla.predict_action(\n        **inputs_device,\n        unnorm_key=UNNORM_KEY,  # REQUIRED for multi-dataset models\n        do_sample=False,        # Greedy decoding for determinism\n    )\n    \n    inference_time = time.time() - start_time\n\nprint(f\"\\nInference completed in {inference_time*1000:.1f} ms\")\nprint(f\"\\nPredicted action (normalized [-1, 1]):\")\nprint(f\"  Shape: {action.shape}\")\nprint(f\"  Values:\")\naction_names = ['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper']\nfor i, (name, val) in enumerate(zip(action_names, action)):\n    print(f\"    {name:8s}: {val:+.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Manual generation (for more control)\n",
    "print(\"Manual generation method:\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate action tokens\n",
    "    generated_ids = vla.generate(\n",
    "        input_ids=inputs_device['input_ids'],\n",
    "        attention_mask=inputs_device['attention_mask'],\n",
    "        pixel_values=inputs_device['pixel_values'],\n",
    "        max_new_tokens=7,\n",
    "        do_sample=False,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    # Extract generated tokens (excluding input)\n",
    "    input_len = inputs_device['input_ids'].shape[1]\n",
    "    action_token_ids = generated_ids[0, input_len:].cpu().numpy()\n",
    "    \n",
    "print(f\"\\nGenerated token IDs: {action_token_ids}\")\n",
    "print(f\"Vocabulary size: {processor.tokenizer.vocab_size}\")\n",
    "print(f\"Action token range: [{processor.tokenizer.vocab_size - 256}, {processor.tokenizer.vocab_size - 1}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Understanding Action Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the action prediction\n",
    "def visualize_action(action, title=\"Predicted Action\"):\n",
    "    \"\"\"\n",
    "    Visualize a 7-DoF robot action.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    action_names = ['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper']\n",
    "    colors = ['steelblue'] * 3 + ['coral'] * 3 + ['green']\n",
    "    \n",
    "    # Bar chart\n",
    "    ax1 = axes[0]\n",
    "    bars = ax1.bar(action_names, action, color=colors)\n",
    "    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax1.axhline(y=1, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    ax1.axhline(y=-1, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "    ax1.set_ylabel('Normalized Value')\n",
    "    ax1.set_title(f'{title}\\nPosition (blue), Rotation (orange), Gripper (green)')\n",
    "    \n",
    "    # Interpretation panel\n",
    "    ax2 = axes[1]\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    interpretation = f\"\"\"\n",
    "    Action Interpretation:\n",
    "    \n",
    "    Position Changes (normalized):\n",
    "      X: {action[0]:+.3f} ({\"right\" if action[0] > 0 else \"left\"})\n",
    "      Y: {action[1]:+.3f} ({\"forward\" if action[1] > 0 else \"backward\"})\n",
    "      Z: {action[2]:+.3f} ({\"up\" if action[2] > 0 else \"down\"})\n",
    "    \n",
    "    Rotation Changes:\n",
    "      Roll:  {action[3]:+.3f}\n",
    "      Pitch: {action[4]:+.3f}\n",
    "      Yaw:   {action[5]:+.3f}\n",
    "    \n",
    "    Gripper:\n",
    "      Value: {action[6]:+.3f} ({\"close\" if action[6] > 0 else \"open\"})\n",
    "    \n",
    "    Note: Actions are normalized to [-1, 1].\n",
    "    Actual robot motion depends on un-normalization\n",
    "    using dataset-specific statistics.\n",
    "    \"\"\"\n",
    "    ax2.text(0.1, 0.9, interpretation, transform=ax2.transAxes,\n",
    "             fontfamily='monospace', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_action(action, f\"Predicted Action for: '{instruction}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Action Un-normalization\n",
    "\n",
    "To get actual robot commands, we need to un-normalize using dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Un-normalize using Bridge dataset statistics\n",
    "# These are approximate values - actual values stored in model\n",
    "\n",
    "bridge_stats = {\n",
    "    'mean': np.array([0.0002, 0.0001, -0.0003, 0.0001, 0.0003, -0.0002, 0.49]),\n",
    "    'std': np.array([0.0074, 0.0058, 0.0074, 0.026, 0.024, 0.052, 0.50]),\n",
    "}\n",
    "\n",
    "def unnormalize_action(normalized_action, stats):\n",
    "    \"\"\"Convert normalized [-1,1] action to real robot units.\"\"\"\n",
    "    # OpenVLA uses: normalized = (action - mean) / std\n",
    "    # So: action = normalized * std + mean\n",
    "    return normalized_action * stats['std'] + stats['mean']\n",
    "\n",
    "# Un-normalize the predicted action\n",
    "real_action = unnormalize_action(action, bridge_stats)\n",
    "\n",
    "print(\"Un-normalized Action (Bridge dataset):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Dimension':<10} {'Normalized':>12} {'Real':>12} {'Unit':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "units = ['m', 'm', 'm', 'rad', 'rad', 'rad', '']\n",
    "for i, (name, norm, real, unit) in enumerate(zip(action_names, action, real_action, units)):\n",
    "    print(f\"{name:<10} {norm:>12.4f} {real:>12.6f} {unit:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenVLA's built-in un-normalization\n",
    "# The model stores statistics for datasets it was trained on\n",
    "\n",
    "# List available un-normalization keys\n",
    "if hasattr(vla, 'norm_stats'):\n",
    "    print(\"Available un-normalization keys:\")\n",
    "    for key in vla.norm_stats.keys():\n",
    "        print(f\"  - {key}\")\n",
    "else:\n",
    "    print(\"Checking model config for normalization statistics...\")\n",
    "    # Statistics may be stored differently depending on model version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Multi-GPU Inference\n",
    "\n",
    "With 4×40GB GPUs, we can run multiple model instances in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Strategy 1: Load model on multiple GPUs for parallel rollouts\ndef load_multi_gpu_models(model_id, devices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"]):\n    \"\"\"\n    Load separate model instances on each GPU.\n    \n    Useful for:\n    - Running multiple environment rollouts in parallel\n    - Different tasks on different GPUs\n    \"\"\"\n    # Check Flash Attention availability\n    try:\n        import flash_attn\n        attn_impl = \"flash_attention_2\"\n    except ImportError:\n        attn_impl = None\n    \n    model_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        \"low_cpu_mem_usage\": True,\n        \"trust_remote_code\": True,\n    }\n    if attn_impl:\n        model_kwargs[\"attn_implementation\"] = attn_impl\n    \n    models = {}\n    \n    for device in devices:\n        print(f\"Loading model on {device}...\")\n        model = AutoModelForVision2Seq.from_pretrained(\n            model_id,\n            **model_kwargs\n        ).to(device)\n        model.eval()\n        models[device] = model\n    \n    return models\n\n# Example usage (commented out to save memory)\n# multi_gpu_models = load_multi_gpu_models(MODEL_ID)\nprint(\"Multi-GPU strategy: One model per GPU for parallel rollouts\")\nprint(\"  GPU 0: Environment 1\")\nprint(\"  GPU 1: Environment 2\")\nprint(\"  GPU 2: Environment 3\")\nprint(\"  GPU 3: Environment 4\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Model parallelism with device_map=\"auto\"\n",
    "# Splits model across GPUs for larger batch sizes\n",
    "\n",
    "model_parallel_config = \"\"\"\n",
    "# For model parallelism:\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",  # Automatically distribute across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# The model will be split across available GPUs\n",
    "# Useful for:\n",
    "# - Larger batch sizes than single GPU can handle\n",
    "# - Running very large models (e.g., 70B)\n",
    "\"\"\"\n",
    "print(\"Model Parallelism Configuration:\")\n",
    "print(model_parallel_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Batch inference for multiple observations\ndef batch_predict_actions(model, processor, images, instructions, device, unnorm_key=\"bridge_orig\"):\n    \"\"\"\n    Predict actions for multiple observations in a batch.\n    \n    Args:\n        model: OpenVLA model\n        processor: OpenVLA processor\n        images: List of PIL Images\n        instructions: List of instruction strings\n        device: Target device\n        unnorm_key: Dataset statistics to use for un-normalization\n    \n    Returns:\n        Array of actions, shape (batch_size, 7)\n    \"\"\"\n    batch_size = len(images)\n    \n    # Format prompts\n    prompts = [\n        f\"In: What action should the robot take to {inst}?\\nOut:\"\n        for inst in instructions\n    ]\n    \n    # Process batch\n    # Note: This is a simplified version - actual batching may need\n    # special handling depending on processor implementation\n    batch_actions = []\n    \n    for prompt, image in zip(prompts, images):\n        inputs = processor(prompt, image)\n        inputs_device = {\n            k: v.to(device, dtype=torch.bfloat16) if isinstance(v, torch.Tensor) else v\n            for k, v in inputs.items()\n        }\n        \n        with torch.no_grad():\n            action = model.predict_action(\n                **inputs_device,\n                unnorm_key=unnorm_key,  # REQUIRED\n                do_sample=False,\n            )\n        batch_actions.append(action)\n    \n    return np.stack(batch_actions)\n\n# Test batch inference\ntest_images = [create_sample_observation() for _ in range(4)]\ntest_instructions = [\n    \"pick up the red block\",\n    \"push the blue cube\",\n    \"move the green object\",\n    \"grasp the red block\",\n]\n\nprint(\"Running batch inference on 4 observations...\")\nstart = time.time()\nbatch_actions = batch_predict_actions(vla, processor, test_images, test_instructions, DEVICE, UNNORM_KEY)\nbatch_time = time.time() - start\n\nprint(f\"\\nBatch inference completed in {batch_time*1000:.1f} ms\")\nprint(f\"Average per sample: {batch_time/4*1000:.1f} ms\")\nprint(f\"\\nBatch actions shape: {batch_actions.shape}\")\n\nfor i, (inst, action) in enumerate(zip(test_instructions, batch_actions)):\n    print(f\"\\nTask {i+1}: '{inst}'\")\n    print(f\"  Action: {action}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference for multiple observations\n",
    "def batch_predict_actions(model, processor, images, instructions, device):\n",
    "    \"\"\"\n",
    "    Predict actions for multiple observations in a batch.\n",
    "    \n",
    "    Args:\n",
    "        model: OpenVLA model\n",
    "        processor: OpenVLA processor\n",
    "        images: List of PIL Images\n",
    "        instructions: List of instruction strings\n",
    "        device: Target device\n",
    "    \n",
    "    Returns:\n",
    "        Array of actions, shape (batch_size, 7)\n",
    "    \"\"\"\n",
    "    batch_size = len(images)\n",
    "    \n",
    "    # Format prompts\n",
    "    prompts = [\n",
    "        f\"In: What action should the robot take to {inst}?\\nOut:\"\n",
    "        for inst in instructions\n",
    "    ]\n",
    "    \n",
    "    # Process batch\n",
    "    # Note: This is a simplified version - actual batching may need\n",
    "    # special handling depending on processor implementation\n",
    "    batch_actions = []\n",
    "    \n",
    "    for prompt, image in zip(prompts, images):\n",
    "        inputs = processor(prompt, image)\n",
    "        inputs_device = {\n",
    "            k: v.to(device, dtype=torch.bfloat16) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in inputs.items()\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = model.predict_action(\n",
    "                **inputs_device,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=7,\n",
    "            )\n",
    "        batch_actions.append(action)\n",
    "    \n",
    "    return np.stack(batch_actions)\n",
    "\n",
    "# Test batch inference\n",
    "test_images = [create_sample_observation() for _ in range(4)]\n",
    "test_instructions = [\n",
    "    \"pick up the red block\",\n",
    "    \"push the blue cube\",\n",
    "    \"move the green object\",\n",
    "    \"grasp the red block\",\n",
    "]\n",
    "\n",
    "print(\"Running batch inference on 4 observations...\")\n",
    "start = time.time()\n",
    "batch_actions = batch_predict_actions(vla, processor, test_images, test_instructions, DEVICE)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"\\nBatch inference completed in {batch_time*1000:.1f} ms\")\n",
    "print(f\"Average per sample: {batch_time/4*1000:.1f} ms\")\n",
    "print(f\"\\nBatch actions shape: {batch_actions.shape}\")\n",
    "\n",
    "for i, (inst, action) in enumerate(zip(test_instructions, batch_actions)):\n",
    "    print(f\"\\nTask {i+1}: '{inst}'\")\n",
    "    print(f\"  Action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Inference Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark inference speed\ndef benchmark_inference(model, processor, device, unnorm_key=\"bridge_orig\", n_runs=10, warmup=3):\n    \"\"\"Benchmark inference speed.\"\"\"\n    image = create_sample_observation()\n    instruction = \"pick up the red block\"\n    prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n    \n    inputs = processor(prompt, image)\n    inputs_device = {\n        k: v.to(device, dtype=torch.bfloat16) if isinstance(v, torch.Tensor) else v\n        for k, v in inputs.items()\n    }\n    \n    # Warmup runs\n    print(f\"Warming up ({warmup} runs)...\")\n    for _ in range(warmup):\n        with torch.no_grad():\n            _ = model.predict_action(**inputs_device, unnorm_key=unnorm_key, do_sample=False)\n    \n    # Benchmark runs\n    torch.cuda.synchronize()\n    times = []\n    \n    print(f\"Benchmarking ({n_runs} runs)...\")\n    for _ in range(n_runs):\n        torch.cuda.synchronize()\n        start = time.time()\n        \n        with torch.no_grad():\n            _ = model.predict_action(**inputs_device, unnorm_key=unnorm_key, do_sample=False)\n        \n        torch.cuda.synchronize()\n        times.append(time.time() - start)\n    \n    times = np.array(times) * 1000  # Convert to ms\n    \n    print(f\"\\nInference Benchmark Results:\")\n    print(f\"  Mean: {times.mean():.1f} ms\")\n    print(f\"  Std:  {times.std():.1f} ms\")\n    print(f\"  Min:  {times.min():.1f} ms\")\n    print(f\"  Max:  {times.max():.1f} ms\")\n    print(f\"  Throughput: {1000/times.mean():.1f} actions/sec\")\n    \n    return times\n\ntimes = benchmark_inference(vla, processor, DEVICE, UNNORM_KEY)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark inference speed\n",
    "def benchmark_inference(model, processor, device, n_runs=10, warmup=3):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    image = create_sample_observation()\n",
    "    instruction = \"pick up the red block\"\n",
    "    prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "    \n",
    "    inputs = processor(prompt, image)\n",
    "    inputs_device = {\n",
    "        k: v.to(device, dtype=torch.bfloat16) if isinstance(v, torch.Tensor) else v\n",
    "        for k, v in inputs.items()\n",
    "    }\n",
    "    \n",
    "    # Warmup runs\n",
    "    print(f\"Warming up ({warmup} runs)...\")\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = model.predict_action(**inputs_device, do_sample=False, max_new_tokens=7)\n",
    "    \n",
    "    # Benchmark runs\n",
    "    torch.cuda.synchronize()\n",
    "    times = []\n",
    "    \n",
    "    print(f\"Benchmarking ({n_runs} runs)...\")\n",
    "    for _ in range(n_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model.predict_action(**inputs_device, do_sample=False, max_new_tokens=7)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    times = np.array(times) * 1000  # Convert to ms\n",
    "    \n",
    "    print(f\"\\nInference Benchmark Results:\")\n",
    "    print(f\"  Mean: {times.mean():.1f} ms\")\n",
    "    print(f\"  Std:  {times.std():.1f} ms\")\n",
    "    print(f\"  Min:  {times.min():.1f} ms\")\n",
    "    print(f\"  Max:  {times.max():.1f} ms\")\n",
    "    print(f\"  Throughput: {1000/times.mean():.1f} actions/sec\")\n",
    "    \n",
    "    return times\n",
    "\n",
    "times = benchmark_inference(vla, processor, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Steps for OpenVLA Inference\n",
    "\n",
    "1. **Load Model**: Use HuggingFace transformers with Flash Attention 2\n",
    "\n",
    "2. **Prepare Inputs**:\n",
    "   - RGB image from robot camera\n",
    "   - Natural language instruction\n",
    "   - Format as: \"In: What action should the robot take to {task}?\\nOut:\"\n",
    "\n",
    "3. **Generate Action**:\n",
    "   - `model.predict_action()` returns normalized action\n",
    "   - Or manual generation with `model.generate()`\n",
    "\n",
    "4. **Un-normalize**: Convert [-1,1] to actual robot commands\n",
    "\n",
    "### Your 4×40GB GPU Setup\n",
    "- **Option 1**: 4 parallel model instances for 4 parallel rollouts\n",
    "- **Option 2**: Model parallelism for larger batch sizes\n",
    "- **Memory per model**: ~14GB in BF16\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **07_libero_setup.ipynb** to set up LIBERO simulation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del vla\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}