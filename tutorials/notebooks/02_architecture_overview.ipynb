{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. OpenVLA Architecture Overview\n",
    "\n",
    "**Goal**: Understand the high-level architecture of OpenVLA and how its components work together.\n",
    "\n",
    "## What We'll Learn\n",
    "1. Vision-Language-Action (VLA) model concept\n",
    "2. OpenVLA's three main components\n",
    "3. Information flow from image → action\n",
    "4. Key design decisions and their rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is a VLA Model?\n",
    "\n",
    "A **Vision-Language-Action (VLA)** model is a neural network that:\n",
    "- **Takes in**: RGB image(s) from robot camera(s) + natural language instruction\n",
    "- **Outputs**: Robot action (typically 7D: 6 DoF pose + gripper)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     VLA Model Pipeline                       │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│   [Camera Image]     [Language Instruction]                  │\n",
    "│         │                     │                              │\n",
    "│         ▼                     ▼                              │\n",
    "│   ┌──────────┐         ┌──────────┐                         │\n",
    "│   │  Vision  │         │   Text   │                         │\n",
    "│   │ Encoder  │         │ Encoder  │                         │\n",
    "│   └────┬─────┘         └────┬─────┘                         │\n",
    "│        │                    │                                │\n",
    "│        └──────────┬─────────┘                                │\n",
    "│                   ▼                                          │\n",
    "│           ┌──────────────┐                                   │\n",
    "│           │   Backbone   │                                   │\n",
    "│           │     LLM      │                                   │\n",
    "│           └──────┬───────┘                                   │\n",
    "│                  ▼                                           │\n",
    "│           ┌──────────────┐                                   │\n",
    "│           │    Action    │                                   │\n",
    "│           │    Output    │                                   │\n",
    "│           └──────────────┘                                   │\n",
    "│                  │                                           │\n",
    "│                  ▼                                           │\n",
    "│     [7D Action: x, y, z, rx, ry, rz, gripper]               │\n",
    "│                                                              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. OpenVLA's Architecture Components\n",
    "\n",
    "OpenVLA builds on **PrismaticVLM** and consists of three main components:\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                        OpenVLA Architecture                         │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  Component 1: VISION BACKBONE                                       │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │  DINOv2-ViT (semantic features) + SigLIP-ViT (text-aligned) │   │\n",
    "│  │                                                              │   │\n",
    "│  │  Image (224×224) → Patch Embeddings (576 tokens × 1024 dim) │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "│                              │                                      │\n",
    "│                              ▼                                      │\n",
    "│  Component 2: PROJECTOR                                            │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │           MLP Projector (2-layer with GeLU)                  │   │\n",
    "│  │                                                              │   │\n",
    "│  │  Vision dim (1024) → LLM dim (4096)                         │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "│                              │                                      │\n",
    "│                              ▼                                      │\n",
    "│  Component 3: LLM BACKBONE                                         │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │               Llama-2 7B (autoregressive)                    │   │\n",
    "│  │                                                              │   │\n",
    "│  │  [Vision tokens] + [Text tokens] → [Action tokens]          │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "│                              │                                      │\n",
    "│                              ▼                                      │\n",
    "│  OUTPUT: Action Tokens → ActionTokenizer → Continuous Actions       │\n",
    "│                                                                     │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CRITICAL: Set environment variables BEFORE importing packages!\n# ============================================================\nimport os\n\n# For NERSC Perlmutter, use your $PSCRATCH directory\nPSCRATCH = \"/pscratch/sd/d/dpark1\"  # CHANGE THIS TO YOUR PATH\nCACHE_DIR = f\"{PSCRATCH}/.cache\"\n\nos.environ['XDG_CACHE_HOME'] = CACHE_DIR\nos.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\nos.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\nos.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n\nfor path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n    os.makedirs(path, exist_ok=True)\n\nprint(f\"✅ All caches → {CACHE_DIR}\")\n\n# Now import packages\nimport torch\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\n\nMODEL_ID = \"openvla/openvla-7b\"\n\nprint(\"\\nLoading OpenVLA model...\")\nvla = AutoModelForVision2Seq.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True,\n)\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model's top-level structure\n",
    "print(\"OpenVLA Top-Level Components:\")\n",
    "print(\"=\"*60)\n",
    "for name, child in vla.named_children():\n",
    "    num_params = sum(p.numel() for p in child.parameters())\n",
    "    print(f\"{name}: {num_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown of model architecture\n",
    "def print_model_structure(model, max_depth=2, prefix=\"\"):\n",
    "    \"\"\"Print model architecture with parameter counts.\"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        num_params = sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{prefix}{name}: {num_params/1e6:.1f}M params\")\n",
    "        \n",
    "        if max_depth > 1:\n",
    "            print_model_structure(child, max_depth-1, prefix + \"  \")\n",
    "\n",
    "print(\"\\nDetailed Model Structure:\")\n",
    "print(\"=\"*60)\n",
    "print_model_structure(vla, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Component Deep Dive\n",
    "\n",
    "### 3.1 Vision Backbone\n",
    "\n",
    "OpenVLA uses a **dual vision encoder**:\n",
    "- **DINOv2**: Self-supervised, captures rich semantic features\n",
    "- **SigLIP**: Text-aligned, optimized for language-vision correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore vision backbone - discover actual attribute names\nprint(\"Vision Backbone Configuration:\")\nprint(\"=\"*60)\n\n# Find vision-related components dynamically\nfor name, child in vla.named_children():\n    if 'vision' in name.lower() or 'image' in name.lower():\n        params = sum(p.numel() for p in child.parameters())\n        print(f\"Found: {name} ({params/1e6:.1f}M params)\")\n        print(f\"  Type: {type(child).__name__}\")\n\n# Alternative: print full model to see structure\nprint(\"\\nFull model structure (first level):\")\nfor name, child in vla.named_children():\n    params = sum(p.numel() for p in child.parameters())\n    print(f\"  {name}: {type(child).__name__} ({params/1e6:.1f}M params)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image transform configuration\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"\\nImage Processing Configuration:\")\n",
    "print(\"=\"*60)\n",
    "if hasattr(processor, 'image_processor'):\n",
    "    img_proc = processor.image_processor\n",
    "    print(f\"Image size: {getattr(img_proc, 'size', 'N/A')}\")\n",
    "    print(f\"Mean: {getattr(img_proc, 'image_mean', 'N/A')}\")\n",
    "    print(f\"Std: {getattr(img_proc, 'image_std', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Projector\n",
    "\n",
    "The projector maps vision features to the LLM's embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore projector - discover actual attribute names\nprint(\"Projector Configuration:\")\nprint(\"=\"*60)\n\n# Find projector-related components\nfor name, child in vla.named_children():\n    if 'project' in name.lower() or 'mlp' in name.lower() or 'connector' in name.lower():\n        params = sum(p.numel() for p in child.parameters())\n        print(f\"Found: {name} ({params/1e6:.1f}M params)\")\n        print(f\"  Type: {type(child).__name__}\")\n        \n        # Try to get layer dimensions\n        for subname, layer in child.named_modules():\n            if hasattr(layer, 'in_features'):\n                print(f\"    {subname}: {layer.in_features} → {layer.out_features}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LLM Backbone\n",
    "\n",
    "OpenVLA uses Llama-2 7B as its language model backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore LLM backbone - discover actual attribute names\nprint(\"LLM Backbone Configuration:\")\nprint(\"=\"*60)\n\n# Find language model components\nfor name, child in vla.named_children():\n    if 'llm' in name.lower() or 'language' in name.lower() or 'lm' in name.lower() or 'model' in name.lower():\n        params = sum(p.numel() for p in child.parameters())\n        print(f\"Found: {name} ({params/1e6:.1f}M params)\")\n        print(f\"  Type: {type(child).__name__}\")\n        \n        # Try to get config\n        if hasattr(child, 'config'):\n            config = child.config\n            print(f\"\\n  Config attributes:\")\n            for attr in ['hidden_size', 'num_hidden_layers', 'num_attention_heads', 'vocab_size']:\n                if hasattr(config, attr):\n                    print(f\"    {attr}: {getattr(config, attr)}\")\n\n# Also check top-level config\nif hasattr(vla, 'config'):\n    print(f\"\\nTop-level model config type: {type(vla.config).__name__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Information Flow: Image → Action\n",
    "\n",
    "Let's trace how an image and instruction become a robot action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample input\n",
    "sample_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "sample_instruction = \"Pick up the red block\"\n",
    "\n",
    "print(\"Step 1: Raw Input\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Image shape: {np.array(sample_image).shape}\")\n",
    "print(f\"Instruction: '{sample_instruction}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process inputs with processor\n",
    "print(\"\\nStep 2: Processor Output\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Format instruction as prompt\n",
    "prompt = f\"In: What action should the robot take to {sample_instruction.lower()}?\\nOut:\"\n",
    "\n",
    "# Process image and text\n",
    "inputs = processor(prompt, sample_image)\n",
    "\n",
    "print(f\"Input keys: {list(inputs.keys())}\")\n",
    "print(f\"\\nPixel values shape: {inputs['pixel_values'].shape}\")\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Decode input tokens to see the prompt structure\n",
    "print(\"\\nStep 3: Tokenized Prompt\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "decoded = tokenizer.decode(inputs['input_ids'][0])\n",
    "print(f\"Decoded prompt:\\n{decoded}\")\n",
    "\n",
    "print(f\"\\nNumber of text tokens: {len(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Vision encoding (trace the forward pass)\nprint(\"\\nStep 4: Vision Encoding\")\nprint(\"=\"*60)\n\n# Move model and inputs to device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nvla = vla.to(device)\n\n# IMPORTANT: Convert inputs to correct dtype (bfloat16 to match model)\ninputs_device = {}\nfor k, v in inputs.items():\n    if isinstance(v, torch.Tensor):\n        if v.dtype == torch.float32:\n            # Convert float tensors to bfloat16 to match model\n            inputs_device[k] = v.to(device, dtype=torch.bfloat16)\n        else:\n            inputs_device[k] = v.to(device)\n    else:\n        inputs_device[k] = v\n\nprint(f\"Device: {device}\")\nprint(f\"Pixel values shape: {inputs_device['pixel_values'].shape}\")\nprint(f\"Pixel values dtype: {inputs_device['pixel_values'].dtype}\")  # Should be bfloat16\n\nprint(\"\\nVision encoding converts the image into tokens that the LLM can process.\")\nprint(\"Typical flow: Image → Patch embeddings → Vision transformer → Projected tokens\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Projection to LLM Space\nprint(\"\\nStep 5: Projection to LLM Space\")\nprint(\"=\"*60)\n\nprint(\"The projector (MLP) maps vision features to the LLM's embedding dimension.\")\nprint(\"Typical transformation: vision_dim (e.g., 1024) → llm_dim (e.g., 4096)\")\nprint(\"\\nThis allows the LLM to 'see' the image as if it were a sequence of tokens.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Action generation\n",
    "print(\"\\nStep 6: Action Token Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# The model generates action tokens autoregressively\n",
    "# These tokens are from the extended vocabulary (last 256 tokens)\n",
    "\n",
    "vocab_size = vla.config.text_config.vocab_size\n",
    "print(f\"Total vocabulary size: {vocab_size}\")\n",
    "print(f\"Action tokens occupy: {vocab_size - 256} to {vocab_size - 1}\")\n",
    "print(f\"Number of action bins: 256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Generate actual action\nprint(\"\\nStep 7: Full Forward Pass (Inference)\")\nprint(\"=\"*60)\n\n# IMPORTANT: Must specify unnorm_key since model was trained on multiple datasets\n# Available keys can be found with: list(vla.norm_stats.keys())\nUNNORM_KEY = \"bridge_orig\"  # Good for tabletop manipulation\n\nprint(f\"Using unnorm_key: '{UNNORM_KEY}'\")\nprint(f\"\\nAvailable unnorm_key options: {len(vla.norm_stats)} datasets\")\nprint(\"  (Use list(vla.norm_stats.keys()) to see all)\")\n\nwith torch.no_grad():\n    # Use the built-in predict_action method\n    action = vla.predict_action(\n        **inputs_device,\n        unnorm_key=UNNORM_KEY,  # REQUIRED for multi-dataset models\n        do_sample=False,        # Greedy decoding for determinism\n    )\n    \nprint(f\"\\nOutput action shape: {action.shape}\")\nprint(f\"Action values (normalized [-1, 1]):\")\nprint(f\"  x:       {action[0]:+.4f}\")\nprint(f\"  y:       {action[1]:+.4f}\")\nprint(f\"  z:       {action[2]:+.4f}\")\nprint(f\"  roll:    {action[3]:+.4f}\")\nprint(f\"  pitch:   {action[4]:+.4f}\")\nprint(f\"  yaw:     {action[5]:+.4f}\")\nprint(f\"  gripper: {action[6]:+.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Key Design Decisions\n",
    "\n",
    "### 5.1 Why Dual Vision Encoders?\n",
    "\n",
    "| Encoder | Strength | What it Captures |\n",
    "|---------|----------|------------------|\n",
    "| DINOv2 | Self-supervised learning | Rich semantic features, object boundaries, spatial understanding |\n",
    "| SigLIP | Contrastive text-image learning | Text-aligned representations, compositional understanding |\n",
    "\n",
    "**Combination**: DINOv2's semantic features + SigLIP's language alignment = better instruction following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dual encoder concept\n",
    "dual_encoder_diagram = \"\"\"\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                    Dual Vision Encoder                          │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│                    [Input Image 224×224]                        │\n",
    "│                           │                                     │\n",
    "│              ┌────────────┴────────────┐                       │\n",
    "│              │                         │                        │\n",
    "│              ▼                         ▼                        │\n",
    "│     ┌──────────────┐          ┌──────────────┐                 │\n",
    "│     │   DINOv2     │          │   SigLIP     │                 │\n",
    "│     │   ViT-L/14   │          │   ViT-L/14   │                 │\n",
    "│     │              │          │              │                 │\n",
    "│     │ Self-        │          │ Text-aligned │                 │\n",
    "│     │ supervised   │          │ contrastive  │                 │\n",
    "│     └──────┬───────┘          └──────┬───────┘                 │\n",
    "│            │                         │                          │\n",
    "│            │  [B, 256, 1024]        │  [B, 256, 1024]          │\n",
    "│            │                         │                          │\n",
    "│            └──────────┬──────────────┘                         │\n",
    "│                       │                                         │\n",
    "│                       ▼                                         │\n",
    "│              ┌──────────────┐                                   │\n",
    "│              │ Concatenate  │                                   │\n",
    "│              │ + Project    │                                   │\n",
    "│              └──────┬───────┘                                   │\n",
    "│                     │                                           │\n",
    "│                     ▼                                           │\n",
    "│             [B, 576, 4096]                                      │\n",
    "│         (LLM-compatible vision tokens)                          │\n",
    "│                                                                 │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "print(dual_encoder_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Why Discretize Actions?\n",
    "\n",
    "OpenVLA treats action prediction as a **token classification problem**, not regression.\n",
    "\n",
    "**Benefits**:\n",
    "1. **Leverages LLM strengths**: LLMs excel at discrete token prediction\n",
    "2. **Unified training**: Same loss function for language and actions\n",
    "3. **Multi-modal robustness**: Better generalization across different robot embodiments\n",
    "4. **Simple integration**: Works with standard autoregressive decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate action discretization\n",
    "import numpy as np\n",
    "\n",
    "def discretize_action(continuous_action, n_bins=256):\n",
    "    \"\"\"Convert continuous action [-1, 1] to discrete bin.\"\"\"\n",
    "    # Clip to valid range\n",
    "    clipped = np.clip(continuous_action, -1, 1)\n",
    "    # Map [-1, 1] to [0, n_bins-1]\n",
    "    bin_idx = np.digitize(clipped, np.linspace(-1, 1, n_bins)) - 1\n",
    "    return np.clip(bin_idx, 0, n_bins - 1)\n",
    "\n",
    "def undiscretize_action(bin_idx, n_bins=256):\n",
    "    \"\"\"Convert discrete bin back to continuous action.\"\"\"\n",
    "    # Map [0, n_bins-1] to [-1, 1]\n",
    "    bin_centers = np.linspace(-1, 1, n_bins)\n",
    "    return bin_centers[bin_idx]\n",
    "\n",
    "# Example\n",
    "original = 0.35\n",
    "discretized = discretize_action(original)\n",
    "reconstructed = undiscretize_action(discretized)\n",
    "\n",
    "print(\"Action Discretization Example:\")\n",
    "print(f\"  Original continuous: {original}\")\n",
    "print(f\"  Discretized bin: {discretized}\")\n",
    "print(f\"  Reconstructed: {reconstructed:.4f}\")\n",
    "print(f\"  Quantization error: {abs(original - reconstructed):.6f}\")\n",
    "print(f\"  Bin resolution: {2/256:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Why Llama-2 as Backbone?\n",
    "\n",
    "| Aspect | Benefit |\n",
    "|--------|--------|\n",
    "| **Pre-training** | 2T tokens of diverse text → strong language understanding |\n",
    "| **Instruction following** | Fine-tuned for following complex instructions |\n",
    "| **Long context** | 4096 tokens → can process detailed task descriptions |\n",
    "| **Size (7B)** | Balance between capability and inference speed |\n",
    "| **Open weights** | Allows fine-tuning on domain-specific robot data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Memory and Compute Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model statistics\n",
    "def model_stats(model):\n",
    "    \"\"\"Calculate detailed model statistics.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Memory in different precisions\n",
    "    mem_fp32 = total_params * 4 / 1e9  # GB\n",
    "    mem_fp16 = total_params * 2 / 1e9  # GB\n",
    "    mem_int8 = total_params * 1 / 1e9  # GB\n",
    "    \n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'mem_fp32_gb': mem_fp32,\n",
    "        'mem_fp16_gb': mem_fp16,\n",
    "        'mem_int8_gb': mem_int8,\n",
    "    }\n",
    "\n",
    "stats = model_stats(vla)\n",
    "\n",
    "print(\"OpenVLA-7B Model Statistics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {stats['total_params']/1e9:.2f}B\")\n",
    "print(f\"Trainable parameters: {stats['trainable_params']/1e9:.2f}B\")\n",
    "print(f\"\\nMemory Requirements:\")\n",
    "print(f\"  FP32 (training): {stats['mem_fp32_gb']:.1f} GB\")\n",
    "print(f\"  FP16/BF16 (inference): {stats['mem_fp16_gb']:.1f} GB\")\n",
    "print(f\"  INT8 (quantized): {stats['mem_int8_gb']:.1f} GB\")\n",
    "print(f\"\\nYour GPU Setup:\")\n",
    "print(f\"  4 × 40GB GPUs = 160 GB total\")\n",
    "print(f\"  Can run {int(160 / stats['mem_fp16_gb'])} model instances in BF16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del vla\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **OpenVLA Architecture**: Vision Backbone → Projector → LLM → Action Tokens\n",
    "\n",
    "2. **Three Components**:\n",
    "   - Vision: DINOv2 + SigLIP (dual encoder for rich + aligned features)\n",
    "   - Projector: MLP mapping vision to LLM space\n",
    "   - LLM: Llama-2 7B for instruction understanding and action generation\n",
    "\n",
    "3. **Action Representation**: \n",
    "   - Continuous actions discretized to 256 bins\n",
    "   - Generated as tokens by the LLM\n",
    "   - 7 dimensions: x, y, z, roll, pitch, yaw, gripper\n",
    "\n",
    "4. **Memory**: ~14GB in BF16, fits easily on your 40GB GPUs\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **03_vision_backbone_deep_dive.ipynb** to understand the dual vision encoder in detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}