{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. OpenVLA Architecture Overview\n",
    "\n",
    "**Goal**: Understand the high-level architecture of OpenVLA and how its components work together.\n",
    "\n",
    "## What We'll Learn\n",
    "1. Vision-Language-Action (VLA) model concept\n",
    "2. OpenVLA's three main components\n",
    "3. Information flow from image \u2192 action\n",
    "4. Key design decisions and their rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is a VLA Model?\n",
    "\n",
    "A **Vision-Language-Action (VLA)** model is a neural network that:\n",
    "- **Takes in**: RGB image(s) from robot camera(s) + natural language instruction\n",
    "- **Outputs**: Robot action (typically 7D: 6 DoF pose + gripper)\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     VLA Model Pipeline                       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                              \u2502\n",
    "\u2502   [Camera Image]     [Language Instruction]                  \u2502\n",
    "\u2502         \u2502                     \u2502                              \u2502\n",
    "\u2502         \u25bc                     \u25bc                              \u2502\n",
    "\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n",
    "\u2502   \u2502  Vision  \u2502         \u2502   Text   \u2502                         \u2502\n",
    "\u2502   \u2502 Encoder  \u2502         \u2502 Encoder  \u2502                         \u2502\n",
    "\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n",
    "\u2502        \u2502                    \u2502                                \u2502\n",
    "\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n",
    "\u2502                   \u25bc                                          \u2502\n",
    "\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n",
    "\u2502           \u2502   Backbone   \u2502                                   \u2502\n",
    "\u2502           \u2502     LLM      \u2502                                   \u2502\n",
    "\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n",
    "\u2502                  \u25bc                                           \u2502\n",
    "\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n",
    "\u2502           \u2502    Action    \u2502                                   \u2502\n",
    "\u2502           \u2502    Output    \u2502                                   \u2502\n",
    "\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n",
    "\u2502                  \u2502                                           \u2502\n",
    "\u2502                  \u25bc                                           \u2502\n",
    "\u2502     [7D Action: x, y, z, rx, ry, rz, gripper]               \u2502\n",
    "\u2502                                                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. OpenVLA's Architecture Components\n",
    "\n",
    "OpenVLA builds on **PrismaticVLM** and consists of three main components:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                        OpenVLA Architecture                         \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  Component 1: VISION BACKBONE                                       \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502  DINOv2-ViT (semantic features) + SigLIP-ViT (text-aligned) \u2502   \u2502\n",
    "\u2502  \u2502                                                              \u2502   \u2502\n",
    "\u2502  \u2502  Image (224\u00d7224) \u2192 Patch Embeddings (576 tokens \u00d7 1024 dim) \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                      \u2502\n",
    "\u2502                              \u25bc                                      \u2502\n",
    "\u2502  Component 2: PROJECTOR                                            \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502           MLP Projector (2-layer with GeLU)                  \u2502   \u2502\n",
    "\u2502  \u2502                                                              \u2502   \u2502\n",
    "\u2502  \u2502  Vision dim (1024) \u2192 LLM dim (4096)                         \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                      \u2502\n",
    "\u2502                              \u25bc                                      \u2502\n",
    "\u2502  Component 3: LLM BACKBONE                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502               Llama-2 7B (autoregressive)                    \u2502   \u2502\n",
    "\u2502  \u2502                                                              \u2502   \u2502\n",
    "\u2502  \u2502  [Vision tokens] + [Text tokens] \u2192 [Action tokens]          \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                      \u2502\n",
    "\u2502                              \u25bc                                      \u2502\n",
    "\u2502  OUTPUT: Action Tokens \u2192 ActionTokenizer \u2192 Continuous Actions       \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL: Set environment variables BEFORE importing packages!\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# For NERSC Perlmutter, use your $SCRATCH directory\n",
    "# Auto-detect environment (NERSC vs SciServer)\n",
    "import os\n",
    "if os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']  # NERSC Perlmutter\n",
    "elif os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']  # Generic scratch\n",
    "else:\n",
    "    SCRATCH = \"/home/idies/workspace/Temporary/dpark1/scratch\"  # SciServer default  # CHANGE THIS TO YOUR PATH\n",
    "CACHE_DIR = f\"{SCRATCH}/.cache\"\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = CACHE_DIR\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\n",
    "os.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n",
    "\n",
    "for path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"\u2705 All caches \u2192 {CACHE_DIR}\")\n",
    "\n",
    "# Now import packages\n",
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "MODEL_ID = \"openvla/openvla-7b\"\n",
    "\n",
    "print(\"\\nLoading OpenVLA model...\")\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model's top-level structure\n",
    "print(\"OpenVLA Top-Level Components:\")\n",
    "print(\"=\"*60)\n",
    "for name, child in vla.named_children():\n",
    "    num_params = sum(p.numel() for p in child.parameters())\n",
    "    print(f\"{name}: {num_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown of model architecture\n",
    "def print_model_structure(model, max_depth=2, prefix=\"\"):\n",
    "    \"\"\"Print model architecture with parameter counts.\"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        num_params = sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{prefix}{name}: {num_params/1e6:.1f}M params\")\n",
    "        \n",
    "        if max_depth > 1:\n",
    "            print_model_structure(child, max_depth-1, prefix + \"  \")\n",
    "\n",
    "print(\"\\nDetailed Model Structure:\")\n",
    "print(\"=\"*60)\n",
    "print_model_structure(vla, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Component Deep Dive\n",
    "\n",
    "### 3.1 Vision Backbone\n",
    "\n",
    "OpenVLA uses a **dual vision encoder**:\n",
    "- **DINOv2**: Self-supervised, captures rich semantic features\n",
    "- **SigLIP**: Text-aligned, optimized for language-vision correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore vision backbone - discover actual attribute names\nprint(\"Vision Backbone Configuration:\")\nprint(\"=\"*60)\n\n# Find vision-related components dynamically\nfor name, child in vla.named_children():\n    if 'vision' in name.lower() or 'image' in name.lower():\n        params = sum(p.numel() for p in child.parameters())\n        print(f\"Found: {name} ({params/1e6:.1f}M params)\")\n        print(f\"  Type: {type(child).__name__}\")\n\n# Alternative: print full model to see structure\nprint(\"\\nFull model structure (first level):\")\nfor name, child in vla.named_children():\n    params = sum(p.numel() for p in child.parameters())\n    print(f\"  {name}: {type(child).__name__} ({params/1e6:.1f}M params)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image transform configuration\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"\\nImage Processing Configuration:\")\n",
    "print(\"=\"*60)\n",
    "if hasattr(processor, 'image_processor'):\n",
    "    img_proc = processor.image_processor\n",
    "    print(f\"Image size: {getattr(img_proc, 'size', 'N/A')}\")\n",
    "    print(f\"Mean: {getattr(img_proc, 'image_mean', 'N/A')}\")\n",
    "    print(f\"Std: {getattr(img_proc, 'image_std', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Projector\n",
    "\n",
    "The projector maps vision features to the LLM's embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore projector - discover actual attribute names\nprint(\"Projector Configuration:\")\nprint(\"=\"*60)\n\n# Find projector-related components\nfor name, child in vla.named_children():\n    if 'project' in name.lower() or 'mlp' in name.lower() or 'connector' in name.lower():\n        params = sum(p.numel() for p in child.parameters())\n        print(f\"Found: {name} ({params/1e6:.1f}M params)\")\n        print(f\"  Type: {type(child).__name__}\")\n        \n        # Try to get layer dimensions\n        for subname, layer in child.named_modules():\n            if hasattr(layer, 'in_features'):\n                print(f\"    {subname}: {layer.in_features} \u2192 {layer.out_features}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LLM Backbone\n",
    "\n",
    "OpenVLA uses Llama-2 7B as its language model backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore LLM backbone - discover actual attribute names\nprint(\"LLM Backbone Configuration:\")\nprint(\"=\"*60)\n\n# Find language model components\nfor name, child in vla.named_children():\n    if 'llm' in name.lower() or 'language' in name.lower() or 'lm' in name.lower() or 'model' in name.lower():\n        params = sum(p.numel() for p in child.parameters())\n        print(f\"Found: {name} ({params/1e6:.1f}M params)\")\n        print(f\"  Type: {type(child).__name__}\")\n        \n        # Try to get config\n        if hasattr(child, 'config'):\n            config = child.config\n            print(f\"\\n  Config attributes:\")\n            for attr in ['hidden_size', 'num_hidden_layers', 'num_attention_heads', 'vocab_size']:\n                if hasattr(config, attr):\n                    print(f\"    {attr}: {getattr(config, attr)}\")\n\n# Also check top-level config\nif hasattr(vla, 'config'):\n    print(f\"\\nTop-level model config type: {type(vla.config).__name__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Information Flow: Image \u2192 Action\n",
    "\n",
    "Let's trace how an image and instruction become a robot action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample input\n",
    "sample_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "sample_instruction = \"Pick up the red block\"\n",
    "\n",
    "print(\"Step 1: Raw Input\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Image shape: {np.array(sample_image).shape}\")\n",
    "print(f\"Instruction: '{sample_instruction}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process inputs with processor\n",
    "print(\"\\nStep 2: Processor Output\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Format instruction as prompt\n",
    "prompt = f\"In: What action should the robot take to {sample_instruction.lower()}?\\nOut:\"\n",
    "\n",
    "# Process image and text\n",
    "inputs = processor(prompt, sample_image)\n",
    "\n",
    "print(f\"Input keys: {list(inputs.keys())}\")\n",
    "print(f\"\\nPixel values shape: {inputs['pixel_values'].shape}\")\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Decode input tokens to see the prompt structure\n",
    "print(\"\\nStep 3: Tokenized Prompt\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "decoded = tokenizer.decode(inputs['input_ids'][0])\n",
    "print(f\"Decoded prompt:\\n{decoded}\")\n",
    "\n",
    "print(f\"\\nNumber of text tokens: {len(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Vision encoding (trace the forward pass)\nprint(\"\\nStep 4: Vision Encoding\")\nprint(\"=\"*60)\n\n# Move model and inputs to device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nvla = vla.to(device)\n\n# IMPORTANT: Convert inputs to correct dtype (bfloat16 to match model)\ninputs_device = {}\nfor k, v in inputs.items():\n    if isinstance(v, torch.Tensor):\n        if v.dtype == torch.float32:\n            # Convert float tensors to bfloat16 to match model\n            inputs_device[k] = v.to(device, dtype=torch.bfloat16)\n        else:\n            inputs_device[k] = v.to(device)\n    else:\n        inputs_device[k] = v\n\nprint(f\"Device: {device}\")\nprint(f\"Pixel values shape: {inputs_device['pixel_values'].shape}\")\nprint(f\"Pixel values dtype: {inputs_device['pixel_values'].dtype}\")  # Should be bfloat16\n\nprint(\"\\nVision encoding converts the image into tokens that the LLM can process.\")\nprint(\"Typical flow: Image \u2192 Patch embeddings \u2192 Vision transformer \u2192 Projected tokens\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Projection to LLM Space\nprint(\"\\nStep 5: Projection to LLM Space\")\nprint(\"=\"*60)\n\nprint(\"The projector (MLP) maps vision features to the LLM's embedding dimension.\")\nprint(\"Typical transformation: vision_dim (e.g., 1024) \u2192 llm_dim (e.g., 4096)\")\nprint(\"\\nThis allows the LLM to 'see' the image as if it were a sequence of tokens.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Action generation\n",
    "print(\"\\nStep 6: Action Token Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# The model generates action tokens autoregressively\n",
    "# These tokens are from the extended vocabulary (last 256 tokens)\n",
    "\n",
    "vocab_size = vla.config.text_config.vocab_size\n",
    "print(f\"Total vocabulary size: {vocab_size}\")\n",
    "print(f\"Action tokens occupy: {vocab_size - 256} to {vocab_size - 1}\")\n",
    "print(f\"Number of action bins: 256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Generate actual action\nprint(\"\\nStep 7: Full Forward Pass (Inference)\")\nprint(\"=\"*60)\n\n# IMPORTANT: Must specify unnorm_key since model was trained on multiple datasets\n# Available keys can be found with: list(vla.norm_stats.keys())\nUNNORM_KEY = \"bridge_orig\"  # Good for tabletop manipulation\n\nprint(f\"Using unnorm_key: '{UNNORM_KEY}'\")\nprint(f\"\\nAvailable unnorm_key options: {len(vla.norm_stats)} datasets\")\nprint(\"  (Use list(vla.norm_stats.keys()) to see all)\")\n\nwith torch.no_grad():\n    # Use the built-in predict_action method\n    action = vla.predict_action(\n        **inputs_device,\n        unnorm_key=UNNORM_KEY,  # REQUIRED for multi-dataset models\n        do_sample=False,        # Greedy decoding for determinism\n    )\n    \nprint(f\"\\nOutput action shape: {action.shape}\")\nprint(f\"Action values (normalized [-1, 1]):\")\nprint(f\"  x:       {action[0]:+.4f}\")\nprint(f\"  y:       {action[1]:+.4f}\")\nprint(f\"  z:       {action[2]:+.4f}\")\nprint(f\"  roll:    {action[3]:+.4f}\")\nprint(f\"  pitch:   {action[4]:+.4f}\")\nprint(f\"  yaw:     {action[5]:+.4f}\")\nprint(f\"  gripper: {action[6]:+.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Key Design Decisions\n",
    "\n",
    "### 5.1 Why Dual Vision Encoders?\n",
    "\n",
    "| Encoder | Strength | What it Captures |\n",
    "|---------|----------|------------------|\n",
    "| DINOv2 | Self-supervised learning | Rich semantic features, object boundaries, spatial understanding |\n",
    "| SigLIP | Contrastive text-image learning | Text-aligned representations, compositional understanding |\n",
    "\n",
    "**Combination**: DINOv2's semantic features + SigLIP's language alignment = better instruction following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dual encoder concept\n",
    "dual_encoder_diagram = \"\"\"\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    Dual Vision Encoder                          \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502                    [Input Image 224\u00d7224]                        \u2502\n",
    "\u2502                           \u2502                                     \u2502\n",
    "\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n",
    "\u2502              \u2502                         \u2502                        \u2502\n",
    "\u2502              \u25bc                         \u25bc                        \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n",
    "\u2502     \u2502   DINOv2     \u2502          \u2502   SigLIP     \u2502                 \u2502\n",
    "\u2502     \u2502   ViT-L/14   \u2502          \u2502   ViT-L/14   \u2502                 \u2502\n",
    "\u2502     \u2502              \u2502          \u2502              \u2502                 \u2502\n",
    "\u2502     \u2502 Self-        \u2502          \u2502 Text-aligned \u2502                 \u2502\n",
    "\u2502     \u2502 supervised   \u2502          \u2502 contrastive  \u2502                 \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n",
    "\u2502            \u2502                         \u2502                          \u2502\n",
    "\u2502            \u2502  [B, 256, 1024]        \u2502  [B, 256, 1024]          \u2502\n",
    "\u2502            \u2502                         \u2502                          \u2502\n",
    "\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n",
    "\u2502                       \u2502                                         \u2502\n",
    "\u2502                       \u25bc                                         \u2502\n",
    "\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n",
    "\u2502              \u2502 Concatenate  \u2502                                   \u2502\n",
    "\u2502              \u2502 + Project    \u2502                                   \u2502\n",
    "\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n",
    "\u2502                     \u2502                                           \u2502\n",
    "\u2502                     \u25bc                                           \u2502\n",
    "\u2502             [B, 576, 4096]                                      \u2502\n",
    "\u2502         (LLM-compatible vision tokens)                          \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\"\"\"\n",
    "print(dual_encoder_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Why Discretize Actions?\n",
    "\n",
    "OpenVLA treats action prediction as a **token classification problem**, not regression.\n",
    "\n",
    "**Benefits**:\n",
    "1. **Leverages LLM strengths**: LLMs excel at discrete token prediction\n",
    "2. **Unified training**: Same loss function for language and actions\n",
    "3. **Multi-modal robustness**: Better generalization across different robot embodiments\n",
    "4. **Simple integration**: Works with standard autoregressive decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate action discretization\n",
    "import numpy as np\n",
    "\n",
    "def discretize_action(continuous_action, n_bins=256):\n",
    "    \"\"\"Convert continuous action [-1, 1] to discrete bin.\"\"\"\n",
    "    # Clip to valid range\n",
    "    clipped = np.clip(continuous_action, -1, 1)\n",
    "    # Map [-1, 1] to [0, n_bins-1]\n",
    "    bin_idx = np.digitize(clipped, np.linspace(-1, 1, n_bins)) - 1\n",
    "    return np.clip(bin_idx, 0, n_bins - 1)\n",
    "\n",
    "def undiscretize_action(bin_idx, n_bins=256):\n",
    "    \"\"\"Convert discrete bin back to continuous action.\"\"\"\n",
    "    # Map [0, n_bins-1] to [-1, 1]\n",
    "    bin_centers = np.linspace(-1, 1, n_bins)\n",
    "    return bin_centers[bin_idx]\n",
    "\n",
    "# Example\n",
    "original = 0.35\n",
    "discretized = discretize_action(original)\n",
    "reconstructed = undiscretize_action(discretized)\n",
    "\n",
    "print(\"Action Discretization Example:\")\n",
    "print(f\"  Original continuous: {original}\")\n",
    "print(f\"  Discretized bin: {discretized}\")\n",
    "print(f\"  Reconstructed: {reconstructed:.4f}\")\n",
    "print(f\"  Quantization error: {abs(original - reconstructed):.6f}\")\n",
    "print(f\"  Bin resolution: {2/256:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Why Llama-2 as Backbone?\n",
    "\n",
    "| Aspect | Benefit |\n",
    "|--------|--------|\n",
    "| **Pre-training** | 2T tokens of diverse text \u2192 strong language understanding |\n",
    "| **Instruction following** | Fine-tuned for following complex instructions |\n",
    "| **Long context** | 4096 tokens \u2192 can process detailed task descriptions |\n",
    "| **Size (7B)** | Balance between capability and inference speed |\n",
    "| **Open weights** | Allows fine-tuning on domain-specific robot data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Memory and Compute Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model statistics\n",
    "def model_stats(model):\n",
    "    \"\"\"Calculate detailed model statistics.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Memory in different precisions\n",
    "    mem_fp32 = total_params * 4 / 1e9  # GB\n",
    "    mem_fp16 = total_params * 2 / 1e9  # GB\n",
    "    mem_int8 = total_params * 1 / 1e9  # GB\n",
    "    \n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'mem_fp32_gb': mem_fp32,\n",
    "        'mem_fp16_gb': mem_fp16,\n",
    "        'mem_int8_gb': mem_int8,\n",
    "    }\n",
    "\n",
    "stats = model_stats(vla)\n",
    "\n",
    "print(\"OpenVLA-7B Model Statistics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {stats['total_params']/1e9:.2f}B\")\n",
    "print(f\"Trainable parameters: {stats['trainable_params']/1e9:.2f}B\")\n",
    "print(f\"\\nMemory Requirements:\")\n",
    "print(f\"  FP32 (training): {stats['mem_fp32_gb']:.1f} GB\")\n",
    "print(f\"  FP16/BF16 (inference): {stats['mem_fp16_gb']:.1f} GB\")\n",
    "print(f\"  INT8 (quantized): {stats['mem_int8_gb']:.1f} GB\")\n",
    "print(f\"\\nYour GPU Setup:\")\n",
    "print(f\"  4 \u00d7 40GB GPUs = 160 GB total\")\n",
    "print(f\"  Can run {int(160 / stats['mem_fp16_gb'])} model instances in BF16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del vla\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **OpenVLA Architecture**: Vision Backbone \u2192 Projector \u2192 LLM \u2192 Action Tokens\n",
    "\n",
    "2. **Three Components**:\n",
    "   - Vision: DINOv2 + SigLIP (dual encoder for rich + aligned features)\n",
    "   - Projector: MLP mapping vision to LLM space\n",
    "   - LLM: Llama-2 7B for instruction understanding and action generation\n",
    "\n",
    "3. **Action Representation**: \n",
    "   - Continuous actions discretized to 256 bins\n",
    "   - Generated as tokens by the LLM\n",
    "   - 7 dimensions: x, y, z, roll, pitch, yaw, gripper\n",
    "\n",
    "4. **Memory**: ~14GB in BF16, fits easily on your 40GB GPUs\n",
    "\n",
    "### Next Steps\n",
    "\u2192 Continue to **03_vision_backbone_deep_dive.ipynb** to understand the dual vision encoder in detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}