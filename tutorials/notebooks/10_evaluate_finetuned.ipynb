{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluate Fine-tuned OpenVLA on LIBERO\n",
    "\n",
    "**Goal**: Evaluate the fine-tuned OpenVLA model using simulation rollouts in LIBERO.\n",
    "\n",
    "## Evaluation Strategy\n",
    "\n",
    "For Vision-Language-Action (VLA) models, proper evaluation means:\n",
    "- Running the policy in **simulation rollouts** (not just prediction loss)\n",
    "- Measuring **task success rate** (did the robot complete the task?)\n",
    "- Using **held-out tasks** for generalization testing\n",
    "\n",
    "## LIBERO Train/Test Split\n",
    "\n",
    "| Suite | Tasks | Purpose |\n",
    "|-------|-------|--------|\n",
    "| libero_spatial | 10 | Spatial reasoning |\n",
    "| libero_object | 10 | Object manipulation |\n",
    "| libero_goal | 10 | Goal-conditioned |\n",
    "| libero_90 | 90 | Training set |\n",
    "| libero_10 | 10 | **Held-out test set** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CRITICAL: Set paths BEFORE importing packages\n# ============================================================\nimport os\nimport sys\n\n# Auto-detect environment (NERSC Perlmutter vs SciServer)\nif os.environ.get('PSCRATCH'):\n    SCRATCH = os.environ['PSCRATCH']  # NERSC Perlmutter\nelif os.environ.get('SCRATCH'):\n    SCRATCH = os.environ['SCRATCH']  # Generic scratch\nelse:\n    SCRATCH = \"/home/idies/workspace/Temporary/dpark1/scratch\"  # SciServer default\n\nCACHE_DIR = f\"{SCRATCH}/.cache\"\nos.environ['XDG_CACHE_HOME'] = CACHE_DIR\nos.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\nos.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF warnings\n\n# Add LIBERO to Python path (required for editable install)\nLIBERO_PATH = f\"{SCRATCH}/LIBERO\"\nif LIBERO_PATH not in sys.path:\n    sys.path.insert(0, LIBERO_PATH)\n\n# Import LIBERO EARLY (before transformers trust_remote_code can interfere)\nfrom libero.libero import benchmark\nfrom libero.libero.envs import OffScreenRenderEnv\nprint(\"LIBERO imported successfully!\")\n\n# Fine-tuned model checkpoint path\nCHECKPOINT_PATH = f\"{SCRATCH}/openvla_finetune/final\"\n\nprint(f\"Base directory: {SCRATCH}\")\nprint(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_finetuned_model(checkpoint_path, device=\"cuda:0\"):\n    \"\"\"\n    Load a fine-tuned OpenVLA model with LoRA weights.\n    \n    Args:\n        checkpoint_path: Path to the fine-tuned checkpoint\n        device: Device to load model on\n    \n    Returns:\n        model, processor\n    \"\"\"\n    from transformers import AutoModelForVision2Seq, AutoProcessor\n    \n    checkpoint_path = Path(checkpoint_path)\n    print(f\"Loading fine-tuned model from: {checkpoint_path}\")\n    \n    # Debug: Show checkpoint contents\n    if checkpoint_path.exists():\n        print(f\"\\nCheckpoint contents:\")\n        for f in sorted(checkpoint_path.iterdir()):\n            size = f.stat().st_size / 1024 / 1024  # MB\n            print(f\"  {f.name} ({size:.1f} MB)\")\n    else:\n        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n    \n    # Detect checkpoint type\n    has_adapter_config = (checkpoint_path / \"adapter_config.json\").exists()\n    has_adapter_model = (checkpoint_path / \"adapter_model.safetensors\").exists() or \\\n                        (checkpoint_path / \"adapter_model.bin\").exists()\n    has_model_safetensors = (checkpoint_path / \"model.safetensors\").exists()\n    has_pytorch_model = (checkpoint_path / \"pytorch_model.bin\").exists()\n    \n    print(f\"\\nCheckpoint type detection:\")\n    print(f\"  adapter_config.json: {has_adapter_config}\")\n    print(f\"  adapter_model.*: {has_adapter_model}\")\n    print(f\"  model.safetensors: {has_model_safetensors}\")\n    print(f\"  pytorch_model.bin: {has_pytorch_model}\")\n    \n    # Determine loading strategy\n    is_lora = has_adapter_config and has_adapter_model\n    is_full_model = has_model_safetensors or has_pytorch_model\n    \n    if is_lora:\n        from peft import PeftModel\n        \n        print(\"\\nDetected LoRA checkpoint - loading base model + adapters...\")\n        \n        # Load base model\n        print(\"Loading base model...\")\n        base_model = AutoModelForVision2Seq.from_pretrained(\n            \"openvla/openvla-7b\",\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            cache_dir=f\"{CACHE_DIR}/huggingface\",\n            attn_implementation=\"eager\",\n        )\n        \n        # Load LoRA weights\n        print(\"Loading LoRA adapters...\")\n        model = PeftModel.from_pretrained(base_model, str(checkpoint_path))\n        \n        # Merge for faster inference\n        print(\"Merging LoRA weights...\")\n        model = model.merge_and_unload()\n        \n    elif is_full_model:\n        print(\"\\nDetected full model checkpoint - loading directly...\")\n        model = AutoModelForVision2Seq.from_pretrained(\n            str(checkpoint_path),\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            attn_implementation=\"eager\",\n        )\n    else:\n        # Fallback: try loading as LoRA anyway (might be HF Trainer checkpoint)\n        print(\"\\nUnknown checkpoint format - attempting LoRA load...\")\n        print(\"If this fails, check that training completed and saved properly.\")\n        \n        from peft import PeftModel\n        \n        base_model = AutoModelForVision2Seq.from_pretrained(\n            \"openvla/openvla-7b\",\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            cache_dir=f\"{CACHE_DIR}/huggingface\",\n            attn_implementation=\"eager\",\n        )\n        \n        try:\n            model = PeftModel.from_pretrained(base_model, str(checkpoint_path))\n            model = model.merge_and_unload()\n        except Exception as e:\n            print(f\"\\nError loading checkpoint: {e}\")\n            print(\"\\nFalling back to base model (NOT fine-tuned!)...\")\n            print(\"WARNING: This will give zero-shot performance, not fine-tuned!\")\n            model = base_model\n    \n    model.to(device)\n    model.eval()\n    \n    # Load processor\n    processor = AutoProcessor.from_pretrained(\n        \"openvla/openvla-7b\",\n        trust_remote_code=True,\n        cache_dir=f\"{CACHE_DIR}/huggingface\",\n    )\n    \n    print(f\"\\nModel loaded on {device}\")\n    return model, processor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if checkpoint exists\n",
    "checkpoint_exists = Path(CHECKPOINT_PATH).exists()\n",
    "print(f\"Checkpoint exists: {checkpoint_exists}\")\n",
    "\n",
    "if checkpoint_exists:\n",
    "    print(f\"\\nCheckpoint contents:\")\n",
    "    for f in Path(CHECKPOINT_PATH).iterdir():\n",
    "        print(f\"  {f.name}\")\n",
    "else:\n",
    "    print(f\"\\nCheckpoint not found at {CHECKPOINT_PATH}\")\n",
    "    print(\"Available checkpoints:\")\n",
    "    finetune_dir = Path(f\"{SCRATCH}/openvla_finetune\")\n",
    "    if finetune_dir.exists():\n",
    "        for f in finetune_dir.iterdir():\n",
    "            print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "# Change CHECKPOINT_PATH above if needed\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "model, processor = load_finetuned_model(CHECKPOINT_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Create Policy Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class OpenVLAPolicy:\n    \"\"\"\n    Policy wrapper for OpenVLA inference in LIBERO.\n    \"\"\"\n    \n    def __init__(self, model, processor, device=\"cuda:0\", unnorm_key=\"bridge_orig\"):\n        self.model = model\n        self.processor = processor\n        self.device = device\n        self.unnorm_key = unnorm_key  # Default to bridge_orig for LIBERO\n    \n    def predict(self, obs, instruction):\n        \"\"\"\n        Predict action from observation and instruction.\n        \n        Args:\n            obs: Dictionary with image observation\n            instruction: Natural language task instruction\n        \n        Returns:\n            action: 7-DoF action array\n        \"\"\"\n        # Get image from observation - LIBERO uses 'agentview_image' key\n        if isinstance(obs, dict):\n            # Try different possible keys for the image\n            image = None\n            for key in ['agentview_image', 'agentview_rgb', 'image', 'pixels']:\n                if key in obs and obs[key] is not None:\n                    image = obs[key]\n                    break\n            \n            if image is None:\n                # Debug: print available keys\n                print(f\"Warning: No image found in obs. Available keys: {list(obs.keys())}\")\n                return np.zeros(7)\n        else:\n            image = obs\n        \n        # Ensure image is a proper array\n        if not isinstance(image, np.ndarray) or image.ndim < 2:\n            print(f\"Warning: Invalid image type={type(image)}, ndim={getattr(image, 'ndim', 'N/A')}\")\n            return np.zeros(7)\n        \n        # Rotate image (LIBERO convention)\n        image = np.rot90(image, k=2)\n        \n        # Convert to PIL\n        pil_image = Image.fromarray(image.astype(np.uint8))\n        \n        # Format prompt\n        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n        \n        # Process inputs\n        inputs = self.processor(prompt, pil_image, return_tensors=\"pt\")\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        # Convert pixel_values to bfloat16\n        if 'pixel_values' in inputs:\n            inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n        \n        # Predict action\n        with torch.no_grad():\n            action = self.model.predict_action(\n                **inputs,\n                unnorm_key=self.unnorm_key,\n                do_sample=False,\n            )\n        \n        # Post-process action\n        action = np.array(action)\n        \n        # Invert gripper for LIBERO convention\n        if len(action) >= 7:\n            action[6] = -action[6]\n        \n        return action\n\n# Create policy with bridge_orig unnorm_key (good for LIBERO-like manipulation)\npolicy = OpenVLAPolicy(model, processor, DEVICE, unnorm_key=\"bridge_orig\")\nprint(\"Policy created with unnorm_key='bridge_orig'!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Setup LIBERO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LIBERO was imported at the top to avoid trust_remote_code interference\n# benchmark and OffScreenRenderEnv are already available\n\ndef get_benchmark_instance(suite_name):\n    \"\"\"Get an instantiated benchmark object for a suite.\"\"\"\n    BenchmarkClass = benchmark.get_benchmark(suite_name)\n    return BenchmarkClass()\n\ndef create_libero_env(task_id, benchmark_instance, image_size=256):\n    \"\"\"Create LIBERO environment for a task.\"\"\"\n    bddl_file = benchmark_instance.get_task_bddl_file_path(task_id)\n    \n    env_args = {\n        \"bddl_file_name\": bddl_file,\n        \"camera_heights\": image_size,\n        \"camera_widths\": image_size,\n    }\n    \n    env = OffScreenRenderEnv(**env_args)\n    env.seed(0)\n    return env\n\n# List available suites\nprint(\"Available LIBERO suites:\")\nfor suite in [\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_90\", \"libero_10\"]:\n    try:\n        bench = get_benchmark_instance(suite)\n        print(f\"  {suite}: {bench.n_tasks} tasks\")\n    except Exception as e:\n        print(f\"  {suite}: Not available ({e})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_single_episode(policy, env, instruction, max_steps=300, render=False):\n    \"\"\"\n    Run a single evaluation episode.\n    \n    Returns:\n        success: bool\n        frames: list of frames (if render=True)\n    \"\"\"\n    obs = env.reset()\n    frames = []\n    \n    for step in range(max_steps):\n        # Get action from policy\n        action = policy.predict(obs, instruction)\n        \n        # Clip action to valid range\n        action = np.clip(action, -1, 1)\n        \n        # Step environment\n        obs, reward, done, info = env.step(action)\n        \n        if render:\n            # Get image for rendering - try multiple keys\n            for key in ['agentview_image', 'agentview_rgb', 'image']:\n                if key in obs and obs[key] is not None:\n                    frames.append(obs[key].copy())\n                    break\n        \n        if done:\n            break\n    \n    success = info.get('success', False) or reward > 0\n    return success, frames\n\n\ndef evaluate_task(policy, task_id, benchmark_instance, n_trials=10, max_steps=300):\n    \"\"\"\n    Evaluate policy on a single task.\n    \n    Returns:\n        dict with success_rate, successes, trials\n    \"\"\"\n    task = benchmark_instance.get_task(task_id)\n    instruction = task.language\n    task_name = benchmark_instance.get_task_names()[task_id]\n    \n    env = create_libero_env(task_id, benchmark_instance)\n    \n    successes = 0\n    for trial in range(n_trials):\n        success, _ = evaluate_single_episode(policy, env, instruction, max_steps)\n        if success:\n            successes += 1\n    \n    env.close()\n    \n    return {\n        'task_id': task_id,\n        'task_name': task_name,\n        'instruction': instruction,\n        'successes': successes,\n        'trials': n_trials,\n        'success_rate': successes / n_trials,\n    }\n\n\ndef evaluate_suite(policy, suite_name, n_trials=10, max_tasks=None, max_steps=300):\n    \"\"\"\n    Evaluate policy on an entire LIBERO suite.\n    \n    Returns:\n        dict with per-task results and overall success rate\n    \"\"\"\n    print(f\"\\nEvaluating on {suite_name}\")\n    print(\"=\" * 60)\n    \n    bench = get_benchmark_instance(suite_name)\n    n_tasks = bench.n_tasks if max_tasks is None else min(bench.n_tasks, max_tasks)\n    \n    print(f\"Tasks: {n_tasks}\")\n    print(f\"Trials per task: {n_trials}\")\n    print()\n    \n    results = []\n    \n    for task_id in tqdm(range(n_tasks), desc=f\"Evaluating {suite_name}\"):\n        result = evaluate_task(policy, task_id, bench, n_trials=n_trials, max_steps=max_steps)\n        results.append(result)\n        \n        print(f\"  {result['task_name']}: {result['success_rate']*100:.1f}% ({result['successes']}/{result['trials']})\")\n    \n    # Calculate overall success rate\n    total_successes = sum(r['successes'] for r in results)\n    total_trials = sum(r['trials'] for r in results)\n    overall_success_rate = total_successes / total_trials\n    \n    return {\n        'suite': suite_name,\n        'tasks': results,\n        'overall_success_rate': overall_success_rate,\n        'total_successes': total_successes,\n        'total_trials': total_trials,\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Run Evaluation\n",
    "\n",
    "### 6.1 Quick Test (1 task, 1 trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test on a single task\n",
    "SUITE = \"libero_spatial\"\n",
    "\n",
    "bench = get_benchmark_instance(SUITE)\n",
    "print(f\"Testing on {SUITE}\")\n",
    "print(f\"Task 0: {bench.get_task_names()[0]}\")\n",
    "print(f\"Instruction: {bench.get_task(0).language}\")\n",
    "\n",
    "# Single trial\n",
    "result = evaluate_task(policy, task_id=0, benchmark_instance=bench, n_trials=1)\n",
    "print(f\"\\nResult: {'SUCCESS' if result['success_rate'] > 0 else 'FAILURE'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualize a Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize a single rollout\nSUITE = \"libero_spatial\"\nTASK_ID = 0\n\nbench = get_benchmark_instance(SUITE)\ntask = bench.get_task(TASK_ID)\ninstruction = task.language\n\nenv = create_libero_env(TASK_ID, bench)\nsuccess, frames = evaluate_single_episode(policy, env, instruction, max_steps=100, render=True)\nenv.close()\n\nprint(f\"Task: {instruction}\")\nprint(f\"Success: {success}\")\nprint(f\"Frames captured: {len(frames)}\")\n\n# Show frames\nif frames:\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    step_indices = np.linspace(0, len(frames)-1, 10, dtype=int)\n    \n    for idx, ax in enumerate(axes.flat):\n        frame_idx = step_indices[idx]\n        # Rotate for display (LIBERO convention)\n        ax.imshow(np.rot90(frames[frame_idx], k=2))\n        ax.set_title(f\"Step {frame_idx}\")\n        ax.axis('off')\n    \n    plt.suptitle(f\"Task: {instruction}\\nSuccess: {success}\", fontsize=12)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No frames captured - check render=True and observation keys\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Full Suite Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Adjust these settings\n",
    "# ============================================================\n",
    "\n",
    "EVAL_SUITE = \"libero_spatial\"  # Suite to evaluate on\n",
    "N_TRIALS = 10                   # Trials per task (10 for full eval, 1 for quick test)\n",
    "MAX_TASKS = None                # None for all tasks, or set a number for quick test\n",
    "MAX_STEPS = 300                 # Max steps per episode\n",
    "\n",
    "print(f\"Evaluation Configuration:\")\n",
    "print(f\"  Suite: {EVAL_SUITE}\")\n",
    "print(f\"  Trials per task: {N_TRIALS}\")\n",
    "print(f\"  Max tasks: {MAX_TASKS or 'All'}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "results = evaluate_suite(\n",
    "    policy,\n",
    "    EVAL_SUITE,\n",
    "    n_trials=N_TRIALS,\n",
    "    max_tasks=MAX_TASKS,\n",
    "    max_steps=MAX_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Suite: {results['suite']}\")\n",
    "print(f\"Overall Success Rate: {results['overall_success_rate']*100:.1f}%\")\n",
    "print(f\"Total: {results['total_successes']}/{results['total_trials']}\")\n",
    "print()\n",
    "\n",
    "print(\"Per-task results:\")\n",
    "for task_result in results['tasks']:\n",
    "    print(f\"  {task_result['task_name']}: {task_result['success_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "task_names = [r['task_name'][:30] for r in results['tasks']]\n",
    "success_rates = [r['success_rate'] * 100 for r in results['tasks']]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(task_names, success_rates, color='steelblue')\n",
    "plt.xlabel('Success Rate (%)')\n",
    "plt.title(f\"Fine-tuned OpenVLA on {EVAL_SUITE}\\nOverall: {results['overall_success_rate']*100:.1f}%\")\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{rate:.0f}%', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"{SCRATCH}/openvla_finetune/eval_{EVAL_SUITE}_{timestamp}.json\"\n",
    "\n",
    "results['checkpoint'] = CHECKPOINT_PATH\n",
    "results['timestamp'] = datetime.now().isoformat()\n",
    "results['config'] = {\n",
    "    'n_trials': N_TRIALS,\n",
    "    'max_tasks': MAX_TASKS,\n",
    "    'max_steps': MAX_STEPS,\n",
    "}\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Compare with Baseline\n",
    "\n",
    "### Expected Results Reference\n",
    "\n",
    "| Model | LIBERO-Spatial | LIBERO-Object | LIBERO-Goal |\n",
    "|-------|----------------|---------------|-------------|\n",
    "| Zero-shot (no fine-tuning) | 0-10% | 0-10% | 0-10% |\n",
    "| After fine-tuning | 70-80% | 75-85% | 65-75% |\n",
    "| Paper reported | 84.7% | 88.4% | 79.2% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baselines\n",
    "baselines = {\n",
    "    'Zero-shot': 5.0,\n",
    "    'Paper (fine-tuned)': 84.7,\n",
    "}\n",
    "\n",
    "our_result = results['overall_success_rate'] * 100\n",
    "\n",
    "print(\"\\nComparison with Baselines:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Zero-shot OpenVLA:        ~5%\")\n",
    "print(f\"Our fine-tuned model:     {our_result:.1f}%\")\n",
    "print(f\"Paper reported:           84.7%\")\n",
    "print()\n",
    "\n",
    "if our_result > 50:\n",
    "    print(\"Fine-tuning successful! Model shows significant improvement.\")\n",
    "elif our_result > 20:\n",
    "    print(\"Some improvement, but may need more training epochs or data.\")\n",
    "else:\n",
    "    print(\"Limited improvement. Check training configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Evaluate on Held-Out Tasks (Optional)\n",
    "\n",
    "To test generalization, evaluate on `libero_10` which contains tasks **not seen during training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out tasks\n",
    "# Only run this if you trained on libero_90\n",
    "\n",
    "EVAL_HELD_OUT = False  # Set to True to evaluate on held-out tasks\n",
    "\n",
    "if EVAL_HELD_OUT:\n",
    "    held_out_results = evaluate_suite(\n",
    "        policy,\n",
    "        \"libero_10\",\n",
    "        n_trials=10,\n",
    "        max_tasks=None,\n",
    "        max_steps=300,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHeld-out (libero_10) Success Rate: {held_out_results['overall_success_rate']*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Set EVAL_HELD_OUT = True to evaluate on held-out tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook evaluated the fine-tuned OpenVLA model on LIBERO tasks.\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. VLA models should be evaluated via **simulation rollouts**, not just loss\n",
    "2. **Task success rate** is the primary metric\n",
    "3. Use **held-out tasks** (libero_10) to test generalization\n",
    "4. Expected fine-tuned performance: **70-80%** success rate\n",
    "\n",
    "**Next Steps:**\n",
    "- If success rate is low, try more training epochs\n",
    "- Try evaluating on different suites (object, goal)\n",
    "- Fine-tune on libero_90, evaluate on libero_10 for generalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}