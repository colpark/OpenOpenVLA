{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Data Pipeline\n",
    "\n",
    "**Goal**: Understand how OpenVLA loads and processes robot demonstration data.\n",
    "\n",
    "## What We'll Learn\n",
    "1. RLDS (Robot Learning Dataset Standard) format\n",
    "2. Open X-Embodiment (OXE) dataset collection\n",
    "3. Data loading and preprocessing\n",
    "4. Dataset mixtures and sampling\n",
    "5. Preparing custom data for OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CRITICAL: Set these BEFORE importing any packages!\n# ============================================================\nimport os\n\n# For NERSC Perlmutter, use your $PSCRATCH directory\nPSCRATCH = \"/pscratch/sd/d/dpark1\"  # CHANGE THIS TO YOUR PATH\n\n# HuggingFace cache (models, tokenizers)\nos.environ['HF_HOME'] = f\"{PSCRATCH}/.cache/huggingface\"\n\n# TensorFlow Datasets (RLDS data) - THIS IS WHERE OXE DATA GOES\nos.environ['TFDS_DATA_DIR'] = f\"{PSCRATCH}/tensorflow_datasets\"\n\n# Torch Hub cache\nos.environ['TORCH_HOME'] = f\"{PSCRATCH}/.cache/torch\"\n\n# Create directories\nfor env_var in ['HF_HOME', 'TFDS_DATA_DIR', 'TORCH_HOME']:\n    os.makedirs(os.environ[env_var], exist_ok=True)\n\nprint(f\"✅ TFDS_DATA_DIR = {os.environ['TFDS_DATA_DIR']}\")\nprint(\"   (All RLDS/OXE datasets will be stored here)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. RLDS: Robot Learning Dataset Standard\n",
    "\n",
    "OpenVLA uses **RLDS** format for robot demonstration data.\n",
    "\n",
    "### Why RLDS?\n",
    "- Standardized format across different robots\n",
    "- Efficient storage and loading with TensorFlow\n",
    "- Supports large-scale datasets\n",
    "- Compatible with many existing robot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlds_structure = \"\"\"\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                      RLDS Dataset Structure                         │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  Dataset                                                            │\n",
    "│  └── Episode (trajectory)                                           │\n",
    "│      └── Step (timestep)                                            │\n",
    "│          ├── observation                                            │\n",
    "│          │   ├── image_primary: (H, W, 3) RGB                      │\n",
    "│          │   ├── image_wrist: (H, W, 3) RGB (optional)             │\n",
    "│          │   └── state: Robot proprioception (optional)             │\n",
    "│          ├── action: (action_dim,) Robot action                    │\n",
    "│          ├── reward: Scalar (optional)                              │\n",
    "│          ├── is_terminal: Boolean                                   │\n",
    "│          └── language_instruction: String                           │\n",
    "│                                                                     │\n",
    "│  Example Episode:                                                   │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │ Task: \"pick up the red block\"                                │   │\n",
    "│  │                                                               │   │\n",
    "│  │ Step 0: [image] [action: approach]                           │   │\n",
    "│  │ Step 1: [image] [action: lower]                              │   │\n",
    "│  │ Step 2: [image] [action: grasp]                              │   │\n",
    "│  │ ...                                                           │   │\n",
    "│  │ Step N: [image] [action: done] is_terminal=True              │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                     │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "print(rlds_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard RLDS data fields\n",
    "import numpy as np\n",
    "\n",
    "# Example step data structure\n",
    "example_step = {\n",
    "    'observation': {\n",
    "        'image_primary': np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8),\n",
    "        'image_wrist': np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8),\n",
    "        'state': np.random.randn(7),  # Joint positions, velocities, etc.\n",
    "    },\n",
    "    'action': np.array([0.01, -0.02, 0.05, 0.0, 0.0, 0.1, 1.0]),  # 7-DoF\n",
    "    'reward': 0.0,\n",
    "    'is_terminal': False,\n",
    "    'is_first': False,\n",
    "    'is_last': False,\n",
    "}\n",
    "\n",
    "print(\"RLDS Step Structure:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in example_step.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for k, v in value.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\n",
    "            else:\n",
    "                print(f\"  {k}: {v}\")\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        print(f\"{key}: shape={value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Open X-Embodiment (OXE) Dataset Collection\n",
    "\n",
    "OpenVLA was trained on **OXE**, a collection of 970K robot trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OXE dataset information\n",
    "oxe_datasets = {\n",
    "    'bridge_orig': {\n",
    "        'trajectories': 60000,\n",
    "        'robot': 'WidowX',\n",
    "        'tasks': 'Kitchen manipulation',\n",
    "        'source': 'Berkeley'\n",
    "    },\n",
    "    'fractal20220817_data': {\n",
    "        'trajectories': 130000,\n",
    "        'robot': 'Everyday Robot',\n",
    "        'tasks': 'RT-1 dataset',\n",
    "        'source': 'Google'\n",
    "    },\n",
    "    'taco_play': {\n",
    "        'trajectories': 15000,\n",
    "        'robot': 'KUKA iiwa',\n",
    "        'tasks': 'Table-top manipulation',\n",
    "        'source': 'TU Darmstadt'\n",
    "    },\n",
    "    'kuka': {\n",
    "        'trajectories': 50000,\n",
    "        'robot': 'KUKA iiwa',\n",
    "        'tasks': 'Grasping',\n",
    "        'source': 'Google'\n",
    "    },\n",
    "    'droid': {\n",
    "        'trajectories': 350000,\n",
    "        'robot': 'Franka',\n",
    "        'tasks': 'Diverse manipulation',\n",
    "        'source': 'DROID consortium'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"OXE Dataset Collection (sample):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Dataset':<25} {'Trajs':>10} {'Robot':<15} {'Tasks'}\")\n",
    "print(\"-\"*70)\n",
    "for name, info in oxe_datasets.items():\n",
    "    print(f\"{name:<25} {info['trajectories']:>10,} {info['robot']:<15} {info['tasks']}\")\n",
    "print(\"-\"*70)\n",
    "total = sum(d['trajectories'] for d in oxe_datasets.values())\n",
    "print(f\"{'Sample total':<25} {total:>10,}\")\n",
    "print(f\"\\nFull OXE: ~970,000 trajectories from 22 robot embodiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Configuration in OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REPO_ROOT = \"/Users/davidpark/Documents/Claude/openvla\"\n",
    "\n",
    "# Read OXE config file\n",
    "oxe_config_path = os.path.join(REPO_ROOT, \"prismatic/vla/datasets/rlds/oxe/configs.py\")\n",
    "print(f\"OXE Config location: {oxe_config_path}\")\n",
    "print(\"\\nThis file defines per-dataset standardization:\")\n",
    "print(\"  - Image keys to extract\")\n",
    "print(\"  - Action normalization type\")\n",
    "print(\"  - Proprioceptive state keys\")\n",
    "print(\"  - Dataset-specific transforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset config structure\n",
    "example_dataset_config = {\n",
    "    'bridge_orig': {\n",
    "        'image_obs_keys': {\n",
    "            'primary': 'image_0',\n",
    "            'wrist': None,\n",
    "        },\n",
    "        'depth_obs_keys': {'primary': None},\n",
    "        'proprio_obs_key': 'state',\n",
    "        'language_key': 'language_instruction',\n",
    "        'action_normalization_type': 'normal',  # mean/std\n",
    "        'action_proprio_normalization_type': 'normal',\n",
    "    },\n",
    "    'libero_spatial_no_noops': {\n",
    "        'image_obs_keys': {\n",
    "            'primary': 'agentview_image',\n",
    "            'wrist': None,\n",
    "        },\n",
    "        'depth_obs_keys': {'primary': None},\n",
    "        'proprio_obs_key': 'ee_states',\n",
    "        'language_key': 'language_instruction',\n",
    "        'action_normalization_type': 'bounds',  # min/max\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Dataset Configuration Examples:\")\n",
    "print(\"=\"*60)\n",
    "for dataset, config in example_dataset_config.items():\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Mixtures and Sampling\n",
    "\n",
    "OpenVLA uses **dataset mixtures** to balance training across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data mixture configuration\n",
    "oxe_magic_soup = [\n",
    "    (\"fractal20220817_data\", 0.54),\n",
    "    (\"bridge_orig\", 1.0),\n",
    "    (\"taco_play\", 2.0),\n",
    "    (\"berkeley_cable_routing\", 3.0),\n",
    "    (\"roboturk\", 1.0),\n",
    "    (\"nyu_door_opening_surprising_effectiveness\", 5.0),\n",
    "    (\"viola\", 2.0),\n",
    "    (\"berkeley_autolab_ur5\", 2.0),\n",
    "    (\"toto\", 1.0),\n",
    "    (\"language_table\", 0.1),\n",
    "]\n",
    "\n",
    "print(\"OXE Magic Soup Mixture (sample):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Dataset':<45} {'Weight':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Normalize weights for visualization\n",
    "total_weight = sum(w for _, w in oxe_magic_soup)\n",
    "for dataset, weight in oxe_magic_soup:\n",
    "    pct = weight / total_weight * 100\n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"{dataset:<45} {weight:>6.2f}  {bar}\")\n",
    "\n",
    "print(f\"\\nTotal weight: {total_weight}\")\n",
    "print(\"\\nWeights control sampling frequency during training.\")\n",
    "print(\"Higher weight = more samples from that dataset per epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate mixture sampling\n",
    "import numpy as np\n",
    "\n",
    "def sample_from_mixture(mixture, n_samples=1000):\n",
    "    \"\"\"Simulate sampling from a dataset mixture.\"\"\"\n",
    "    datasets = [d for d, _ in mixture]\n",
    "    weights = np.array([w for _, w in mixture])\n",
    "    probs = weights / weights.sum()\n",
    "    \n",
    "    samples = np.random.choice(datasets, size=n_samples, p=probs)\n",
    "    \n",
    "    # Count samples per dataset\n",
    "    counts = {}\n",
    "    for d in datasets:\n",
    "        counts[d] = (samples == d).sum()\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Simulate 10000 samples\n",
    "sample_counts = sample_from_mixture(oxe_magic_soup, n_samples=10000)\n",
    "\n",
    "print(\"\\nSimulated Sampling (10,000 samples):\")\n",
    "print(\"=\"*60)\n",
    "for dataset, count in sorted(sample_counts.items(), key=lambda x: -x[1]):\n",
    "    pct = count / 10000 * 100\n",
    "    print(f\"{dataset:<45} {count:>5} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = \"\"\"\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    OpenVLA Data Loading Pipeline                    │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  RLDS Files (TFRecord)                                              │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  ┌──────────────────────────────────────────────────────────────┐  │\n",
    "│  │ 1. LOAD: TensorFlow data loader                              │  │\n",
    "│  │    - Efficient parallel reading                               │  │\n",
    "│  │    - Automatic sharding for multi-GPU                        │  │\n",
    "│  └──────────────────────────────────────────────────────────────┘  │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  ┌──────────────────────────────────────────────────────────────┐  │\n",
    "│  │ 2. STANDARDIZE: Unified format across datasets                │  │\n",
    "│  │    - Map image keys (agentview → primary)                    │  │\n",
    "│  │    - Extract language instruction                             │  │\n",
    "│  │    - Normalize actions                                        │  │\n",
    "│  └──────────────────────────────────────────────────────────────┘  │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  ┌──────────────────────────────────────────────────────────────┐  │\n",
    "│  │ 3. TRANSFORM: Apply data augmentation                         │  │\n",
    "│  │    - Resize images (224×224)                                 │  │\n",
    "│  │    - Task augmentation (rephrase instructions)               │  │\n",
    "│  │    - Goal relabeling (optional)                              │  │\n",
    "│  └──────────────────────────────────────────────────────────────┘  │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  ┌──────────────────────────────────────────────────────────────┐  │\n",
    "│  │ 4. BATCH: Create training batches                            │  │\n",
    "│  │    - Shuffle within/across trajectories                      │  │\n",
    "│  │    - Sample from mixture weights                              │  │\n",
    "│  │    - Collate into PyTorch tensors                            │  │\n",
    "│  └──────────────────────────────────────────────────────────────┘  │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Training Batch:                                                    │\n",
    "│  {                                                                  │\n",
    "│    'pixel_values': (B, C, H, W),                                   │\n",
    "│    'input_ids': (B, seq_len),                                      │\n",
    "│    'attention_mask': (B, seq_len),                                 │\n",
    "│    'labels': (B, seq_len),  # action tokens                        │\n",
    "│  }                                                                  │\n",
    "│                                                                     │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "print(data_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Action Statistics for Normalization\n",
    "\n",
    "Each dataset has computed statistics for action normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example action statistics (from OpenVLA)\n",
    "action_statistics = {\n",
    "    'bridge_orig': {\n",
    "        'mean': [0.00021961, 0.00## 015, -0.00028, 0.00013, 0.00033, -0.00019, 0.49],\n",
    "        'std': [0.0074, 0.0058, 0.0074, 0.026, 0.024, 0.052, 0.50],\n",
    "        'min': [-0.05, -0.04, -0.05, -0.17, -0.16, -0.35, 0.0],\n",
    "        'max': [0.05, 0.04, 0.05, 0.17, 0.16, 0.35, 1.0],\n",
    "    },\n",
    "    'libero_spatial_no_noops': {\n",
    "        'q01': [-0.065, -0.065, -0.055, -0.17, -0.17, -0.42, -1.0],\n",
    "        'q99': [0.065, 0.065, 0.055, 0.17, 0.17, 0.42, 1.0],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Action Statistics Examples:\")\n",
    "print(\"=\"*60)\n",
    "for dataset, stats in action_statistics.items():\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    for stat_name, values in stats.items():\n",
    "        print(f\"  {stat_name}: {[f'{v:.4f}' for v in values]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Preparing Custom Data for OpenVLA\n",
    "\n",
    "To fine-tune OpenVLA on your own data, convert it to RLDS format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Converting custom data to RLDS-compatible format\n",
    "def create_rlds_example(image, action, instruction, is_terminal=False):\n",
    "    \"\"\"\n",
    "    Create a single RLDS step from custom data.\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image array (H, W, 3)\n",
    "        action: Robot action array (7,)\n",
    "        instruction: Task description string\n",
    "        is_terminal: Whether this is the last step\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'observation': {\n",
    "            'image_primary': image,\n",
    "        },\n",
    "        'action': action,\n",
    "        'language_instruction': instruction,\n",
    "        'is_terminal': is_terminal,\n",
    "        'is_first': False,\n",
    "        'is_last': is_terminal,\n",
    "    }\n",
    "\n",
    "# Example trajectory\n",
    "trajectory = []\n",
    "instruction = \"pick up the red cube\"\n",
    "for i in range(10):\n",
    "    image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "    action = np.random.randn(7) * 0.01  # Small random actions\n",
    "    step = create_rlds_example(image, action, instruction, is_terminal=(i == 9))\n",
    "    trajectory.append(step)\n",
    "\n",
    "print(f\"Example trajectory: {len(trajectory)} steps\")\n",
    "print(f\"Task: '{instruction}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom dataset with OpenVLA\n",
    "custom_dataset_config = \"\"\"\n",
    "# Add to prismatic/vla/datasets/rlds/oxe/configs.py:\n",
    "\n",
    "OXE_DATASET_CONFIGS = {\n",
    "    # ... existing configs ...\n",
    "    \n",
    "    'my_custom_dataset': {\n",
    "        'image_obs_keys': {\n",
    "            'primary': 'image_primary',\n",
    "            'wrist': None,\n",
    "        },\n",
    "        'depth_obs_keys': {'primary': None},\n",
    "        'proprio_obs_key': None,\n",
    "        'language_key': 'language_instruction',\n",
    "        'action_normalization_type': 'normal',  # or 'bounds'\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "print(\"Custom Dataset Registration:\")\n",
    "print(custom_dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **RLDS Format**: Standardized format for robot trajectory data\n",
    "   - Episodes contain steps with observations, actions, instructions\n",
    "   - Efficient TensorFlow-based storage\n",
    "\n",
    "2. **OXE Collection**: 970K trajectories from 22+ robot embodiments\n",
    "   - Diverse tasks and environments\n",
    "   - Unified under RLDS standard\n",
    "\n",
    "3. **Data Mixtures**: Weighted sampling across datasets\n",
    "   - Balances data distribution during training\n",
    "   - Configurable for different training objectives\n",
    "\n",
    "4. **Normalization**: Per-dataset action statistics\n",
    "   - Maps actions to [-1, 1] range\n",
    "   - Critical for action tokenization\n",
    "\n",
    "5. **Custom Data**: Convert your data to RLDS format\n",
    "   - Register with dataset configs\n",
    "   - Compute action statistics\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **06_basic_inference.ipynb** to run OpenVLA on sample data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}