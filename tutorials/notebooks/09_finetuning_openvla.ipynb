{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 09. Fine-tuning OpenVLA on LIBERO\n",
    "\n",
    "**Goal**: Fine-tune OpenVLA on LIBERO demonstration data to achieve good task performance.\n",
    "\n",
    "## Why Fine-tuning is Required\n",
    "\n",
    "OpenVLA was trained on Open X-Embodiment datasets (Bridge, RT-1, Fractal, etc.) but **NOT on LIBERO**.\n",
    "The paper's reported 70-80% success rates on LIBERO require fine-tuning on LIBERO demonstrations.\n",
    "\n",
    "## What We'll Cover\n",
    "1. Download LIBERO demonstration data\n",
    "2. Convert demonstrations to training format\n",
    "3. Configure fine-tuning parameters\n",
    "4. Run fine-tuning with LoRA (memory efficient)\n",
    "5. Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL: Set paths BEFORE importing packages\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# For NERSC Perlmutter\n",
    "PSCRATCH = \"/pscratch/sd/d/dpark1\"  # CHANGE THIS TO YOUR PATH\n",
    "CACHE_DIR = f\"{PSCRATCH}/.cache\"\n",
    "\n",
    "# Training output directory\n",
    "OUTPUT_DIR = f\"{PSCRATCH}/openvla_finetune\"\n",
    "\n",
    "# LIBERO demo data directory\n",
    "LIBERO_DATA_DIR = f\"{PSCRATCH}/libero_data\"\n",
    "\n",
    "# Set cache directories\n",
    "os.environ['XDG_CACHE_HOME'] = CACHE_DIR\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n",
    "\n",
    "# Create directories\n",
    "for path in [OUTPUT_DIR, LIBERO_DATA_DIR, CACHE_DIR]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"LIBERO data: {LIBERO_DATA_DIR}\")\n",
    "print(f\"Cache: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Download LIBERO Demonstration Data\n",
    "\n",
    "LIBERO provides 50 demonstration episodes per task.\n",
    "We'll download the data from the official source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBERO dataset information\n",
    "LIBERO_DATASETS = {\n",
    "    \"libero_spatial\": {\n",
    "        \"url\": \"https://utexas.box.com/shared/static/libero_spatial.zip\",\n",
    "        \"n_tasks\": 10,\n",
    "        \"demos_per_task\": 50,\n",
    "    },\n",
    "    \"libero_object\": {\n",
    "        \"url\": \"https://utexas.box.com/shared/static/libero_object.zip\",\n",
    "        \"n_tasks\": 10,\n",
    "        \"demos_per_task\": 50,\n",
    "    },\n",
    "    \"libero_goal\": {\n",
    "        \"url\": \"https://utexas.box.com/shared/static/libero_goal.zip\",\n",
    "        \"n_tasks\": 10,\n",
    "        \"demos_per_task\": 50,\n",
    "    },\n",
    "    \"libero_90\": {\n",
    "        \"url\": \"https://utexas.box.com/shared/static/libero_90.zip\",\n",
    "        \"n_tasks\": 90,\n",
    "        \"demos_per_task\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"LIBERO Dataset Summary:\")\n",
    "print(\"=\"*60)\n",
    "for name, info in LIBERO_DATASETS.items():\n",
    "    total_demos = info['n_tasks'] * info['demos_per_task']\n",
    "    print(f\"{name}: {info['n_tasks']} tasks x {info['demos_per_task']} demos = {total_demos} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download script for LIBERO data\n",
    "# Run this in terminal for faster download:\n",
    "\n",
    "download_script = f'''\n",
    "#!/bin/bash\n",
    "# Download LIBERO demonstration data\n",
    "\n",
    "cd {LIBERO_DATA_DIR}\n",
    "\n",
    "# Option 1: Use LIBERO's official download script\n",
    "# pip install gdown\n",
    "# python -c \"from libero.libero import get_libero_path; print(get_libero_path('datasets'))\"\n",
    "\n",
    "# Option 2: Download from HuggingFace (recommended)\n",
    "# The LIBERO team also hosts data on HuggingFace\n",
    "pip install huggingface_hub\n",
    "\n",
    "python << 'EOF'\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Download LIBERO datasets\n",
    "# Check https://huggingface.co/datasets/libero-project for available datasets\n",
    "snapshot_download(\n",
    "    repo_id=\"libero-project/libero\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"{LIBERO_DATA_DIR}\",\n",
    "    allow_patterns=[\"libero_spatial/*\", \"libero_object/*\"],  # Start with smaller suites\n",
    ")\n",
    "EOF\n",
    "\n",
    "echo \"Download complete!\"\n",
    "ls -la {LIBERO_DATA_DIR}\n",
    "'''\n",
    "\n",
    "print(\"Run this script in terminal to download LIBERO data:\")\n",
    "print(\"=\"*60)\n",
    "print(download_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download using LIBERO's built-in script\n",
    "try:\n",
    "    from libero.libero import benchmark\n",
    "    from libero.libero.utils import download_libero_datasets\n",
    "    \n",
    "    print(\"LIBERO download utilities available!\")\n",
    "    print(\"\\nTo download, run:\")\n",
    "    print(f\"  python -m libero.libero.benchmark_scripts.download_libero_datasets --save_dir {LIBERO_DATA_DIR}\")\n",
    "except ImportError:\n",
    "    print(\"LIBERO download utilities not found.\")\n",
    "    print(\"Use the HuggingFace download method above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Explore Demonstration Data Format\n",
    "\n",
    "LIBERO demonstrations are stored in HDF5 format with:\n",
    "- `obs/agentview_rgb`: RGB images from agent camera\n",
    "- `obs/ee_pos`: End-effector position\n",
    "- `obs/ee_ori`: End-effector orientation\n",
    "- `actions`: 7-DoF actions (position delta, rotation delta, gripper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_hdf5_structure(filepath):\n",
    "    \"\"\"Explore the structure of a LIBERO HDF5 demo file.\"\"\"\n",
    "    print(f\"Exploring: {filepath}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        def print_structure(name, obj):\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                print(f\"  {name}: shape={obj.shape}, dtype={obj.dtype}\")\n",
    "            else:\n",
    "                print(f\"  {name}/\")\n",
    "        \n",
    "        f.visititems(print_structure)\n",
    "        \n",
    "        # Print attributes\n",
    "        print(\"\\nAttributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# Find and explore a demo file\n",
    "demo_files = list(Path(LIBERO_DATA_DIR).rglob(\"*.hdf5\"))\n",
    "if demo_files:\n",
    "    explore_hdf5_structure(demo_files[0])\n",
    "else:\n",
    "    print(f\"No HDF5 files found in {LIBERO_DATA_DIR}\")\n",
    "    print(\"Please download LIBERO data first using the script above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demo_episode(filepath, demo_idx=0):\n",
    "    \"\"\"\n",
    "    Load a single demonstration episode from LIBERO HDF5 file.\n",
    "    \n",
    "    Returns:\n",
    "        images: List of RGB images\n",
    "        actions: Array of actions (T, 7)\n",
    "        language: Task instruction string\n",
    "    \"\"\"\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        # Get demo group\n",
    "        demo_key = f\"demo_{demo_idx}\"\n",
    "        if demo_key not in f['data']:\n",
    "            raise ValueError(f\"Demo {demo_idx} not found\")\n",
    "        \n",
    "        demo = f['data'][demo_key]\n",
    "        \n",
    "        # Load images\n",
    "        images = demo['obs']['agentview_rgb'][:]\n",
    "        \n",
    "        # Load actions\n",
    "        actions = demo['actions'][:]\n",
    "        \n",
    "        # Get language instruction\n",
    "        language = f.attrs.get('language_instruction', 'unknown task')\n",
    "        if isinstance(language, bytes):\n",
    "            language = language.decode('utf-8')\n",
    "    \n",
    "    return images, actions, language\n",
    "\n",
    "# Test loading a demo\n",
    "if demo_files:\n",
    "    try:\n",
    "        images, actions, language = load_demo_episode(demo_files[0])\n",
    "        print(f\"Loaded demo:\")\n",
    "        print(f\"  Task: {language}\")\n",
    "        print(f\"  Images: {len(images)} frames, shape={images[0].shape}\")\n",
    "        print(f\"  Actions: shape={actions.shape}\")\n",
    "        print(f\"  Action range: [{actions.min():.3f}, {actions.max():.3f}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading demo: {e}\")\n",
    "        print(\"HDF5 structure may be different. Check explore_hdf5_structure output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Convert to OpenVLA Training Format\n",
    "\n",
    "OpenVLA expects data in RLDS (Reinforcement Learning Datasets) format.\n",
    "We'll convert LIBERO HDF5 demos to this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIBERODataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for LIBERO demonstrations.\n",
    "    \n",
    "    Each sample is a (image, instruction, action) tuple for one timestep.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, suite_name=\"libero_spatial\", transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.suite_name = suite_name\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Find all demo files for this suite\n",
    "        self.demo_files = sorted(self.data_dir.glob(f\"**/{suite_name}*/*.hdf5\"))\n",
    "        if not self.demo_files:\n",
    "            self.demo_files = sorted(self.data_dir.glob(f\"**/*.hdf5\"))\n",
    "        \n",
    "        # Build index of (file_idx, demo_idx, timestep) tuples\n",
    "        self.samples = []\n",
    "        self.file_cache = {}  # Cache open files\n",
    "        \n",
    "        print(f\"Indexing {len(self.demo_files)} demo files...\")\n",
    "        for file_idx, filepath in enumerate(tqdm(self.demo_files)):\n",
    "            with h5py.File(filepath, 'r') as f:\n",
    "                # Get language instruction\n",
    "                language = f.attrs.get('language_instruction', 'perform task')\n",
    "                if isinstance(language, bytes):\n",
    "                    language = language.decode('utf-8')\n",
    "                \n",
    "                # Index each demo and timestep\n",
    "                for demo_key in f['data'].keys():\n",
    "                    if not demo_key.startswith('demo_'):\n",
    "                        continue\n",
    "                    demo = f['data'][demo_key]\n",
    "                    n_timesteps = len(demo['actions'])\n",
    "                    \n",
    "                    for t in range(n_timesteps):\n",
    "                        self.samples.append({\n",
    "                            'file_idx': file_idx,\n",
    "                            'demo_key': demo_key,\n",
    "                            'timestep': t,\n",
    "                            'language': language,\n",
    "                        })\n",
    "        \n",
    "        print(f\"Total samples: {len(self.samples)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        filepath = self.demo_files[sample['file_idx']]\n",
    "        \n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            demo = f['data'][sample['demo_key']]\n",
    "            t = sample['timestep']\n",
    "            \n",
    "            # Load image\n",
    "            image = demo['obs']['agentview_rgb'][t]\n",
    "            \n",
    "            # Rotate 180 degrees (LIBERO convention)\n",
    "            image = np.rot90(image, k=2)\n",
    "            \n",
    "            # Load action\n",
    "            action = demo['actions'][t]\n",
    "        \n",
    "        # Convert to PIL\n",
    "        pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "        \n",
    "        if self.transform:\n",
    "            pil_image = self.transform(pil_image)\n",
    "        \n",
    "        return {\n",
    "            'image': pil_image,\n",
    "            'instruction': sample['language'],\n",
    "            'action': torch.tensor(action, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "# Test dataset\n",
    "if demo_files:\n",
    "    try:\n",
    "        dataset = LIBERODataset(LIBERO_DATA_DIR)\n",
    "        print(f\"\\nDataset created with {len(dataset)} samples\")\n",
    "        \n",
    "        # Get a sample\n",
    "        sample = dataset[0]\n",
    "        print(f\"Sample image: {sample['image'].size}\")\n",
    "        print(f\"Sample instruction: {sample['instruction']}\")\n",
    "        print(f\"Sample action: {sample['action']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Fine-tuning Configuration\n",
    "\n",
    "We'll use **LoRA (Low-Rank Adaptation)** for memory-efficient fine-tuning:\n",
    "- Only trains ~0.1% of parameters\n",
    "- Fits on a single 40GB GPU\n",
    "- Fast training (few hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "FINETUNE_CONFIG = {\n",
    "    # Model\n",
    "    \"model_id\": \"openvla/openvla-7b\",\n",
    "    \n",
    "    # LoRA parameters\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 32,           # LoRA rank\n",
    "    \"lora_alpha\": 32,       # LoRA alpha\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    \n",
    "    # Training parameters\n",
    "    \"batch_size\": 8,        # Per-GPU batch size\n",
    "    \"gradient_accumulation_steps\": 4,  # Effective batch = 32\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 10,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Data\n",
    "    \"image_size\": 224,\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"save_steps\": 500,\n",
    "    \"logging_steps\": 50,\n",
    "    \n",
    "    # Hardware\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,           # Use BFloat16 for A100\n",
    "    \"dataloader_num_workers\": 4,\n",
    "}\n",
    "\n",
    "print(\"Fine-tuning Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in FINETUNE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save config\n",
    "config_path = f\"{OUTPUT_DIR}/finetune_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(FINETUNE_CONFIG, f, indent=2)\n",
    "print(f\"\\nConfig saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory estimation\n",
    "def estimate_memory():\n",
    "    \"\"\"\n",
    "    Estimate GPU memory requirements for fine-tuning.\n",
    "    \"\"\"\n",
    "    model_params = 7e9  # 7B parameters\n",
    "    \n",
    "    # Full fine-tuning (not recommended for 7B)\n",
    "    # Model: 7B * 2 bytes (bf16) = 14 GB\n",
    "    # Optimizer: 7B * 8 bytes (Adam states) = 56 GB\n",
    "    # Gradients: 7B * 2 bytes = 14 GB\n",
    "    # Total: ~84 GB (doesn't fit on 40GB)\n",
    "    \n",
    "    # LoRA fine-tuning\n",
    "    # Model: 14 GB (frozen)\n",
    "    # LoRA params: ~0.1% = 7M * 2 bytes = 14 MB\n",
    "    # LoRA optimizer: 7M * 8 bytes = 56 MB\n",
    "    # Activations: ~5-10 GB (depends on batch size)\n",
    "    # Total: ~20-25 GB (fits on 40GB!)\n",
    "    \n",
    "    print(\"GPU Memory Estimation:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Full Fine-tuning (NOT recommended):\")\n",
    "    print(\"  Model (bf16): 14 GB\")\n",
    "    print(\"  Optimizer states: 56 GB\")\n",
    "    print(\"  Gradients: 14 GB\")\n",
    "    print(\"  Total: ~84 GB (EXCEEDS 40GB!)\")\n",
    "    print(\"\")\n",
    "    print(\"LoRA Fine-tuning (recommended):\")\n",
    "    print(\"  Model (bf16, frozen): 14 GB\")\n",
    "    print(\"  LoRA params + optimizer: <1 GB\")\n",
    "    print(\"  Activations (batch=8): ~8 GB\")\n",
    "    print(\"  Total: ~23 GB (FITS on 40GB!)\")\n",
    "    print(\"\")\n",
    "    print(\"Recommendation: Use LoRA with batch_size=8\")\n",
    "\n",
    "estimate_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Set Up LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT for LoRA\n",
    "# !pip install peft accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    print(\"PEFT (LoRA) library loaded!\")\n",
    "except ImportError:\n",
    "    print(\"PEFT not installed. Run: pip install peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lora_model(model_id, lora_config):\n",
    "    \"\"\"\n",
    "    Load OpenVLA model and add LoRA adapters.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    # Load processor\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lora_config['lora_r'],\n",
    "        lora_alpha=lora_config['lora_alpha'],\n",
    "        lora_dropout=lora_config['lora_dropout'],\n",
    "        target_modules=lora_config['lora_target_modules'],\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Add LoRA to model\n",
    "    print(\"Adding LoRA adapters...\")\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Test setup (don't actually load - takes time)\n",
    "print(\"LoRA setup function defined.\")\n",
    "print(\"Call setup_lora_model() to load model with LoRA adapters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenVLATrainer:\n",
    "    \"\"\"\n",
    "    Custom trainer for OpenVLA fine-tuning on LIBERO.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, processor, dataset, config):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Setup optimizer (only LoRA params)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "        )\n",
    "        \n",
    "        # Setup dataloader\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=config['dataloader_num_workers'],\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.global_step = 0\n",
    "        self.losses = []\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        pbar = tqdm(self.dataloader, desc=f\"Epoch {epoch}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Prepare inputs\n",
    "            images = batch['image']\n",
    "            instructions = batch['instruction']\n",
    "            actions = batch['action'].to(self.device)\n",
    "            \n",
    "            # Format prompts\n",
    "            prompts = [\n",
    "                f\"In: What action should the robot take to {inst.lower()}?\\nOut:\"\n",
    "                for inst in instructions\n",
    "            ]\n",
    "            \n",
    "            # Process inputs\n",
    "            inputs = self.processor(\n",
    "                prompts, \n",
    "                images,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            # Note: OpenVLA uses action tokens, need to format actions as target tokens\n",
    "            # This is a simplified version - actual training requires action tokenization\n",
    "            outputs = self.model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss = loss / self.config['gradient_accumulation_steps']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.global_step += 1\n",
    "            \n",
    "            # Logging\n",
    "            epoch_losses.append(loss.item())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if self.global_step % self.config['save_steps'] == 0:\n",
    "                self.save_checkpoint()\n",
    "        \n",
    "        return np.mean(epoch_losses)\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        save_path = f\"{self.config['output_dir']}/checkpoint-{self.global_step}\"\n",
    "        self.model.save_pretrained(save_path)\n",
    "        print(f\"Checkpoint saved: {save_path}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        print(f\"Starting training for {self.config['num_epochs']} epochs...\")\n",
    "        \n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            epoch_loss = self.train_epoch(epoch)\n",
    "            print(f\"Epoch {epoch} complete. Average loss: {epoch_loss:.4f}\")\n",
    "            self.losses.append(epoch_loss)\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_checkpoint()\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "        return self.losses\n",
    "\n",
    "print(\"OpenVLATrainer class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Training Script (For Terminal)\n",
    "\n",
    "For actual training, it's better to run as a script with proper distributed training.\n",
    "Here's a complete training script you can run in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script = f'''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "OpenVLA Fine-tuning Script for LIBERO\n",
    "\n",
    "Usage:\n",
    "    # Single GPU\n",
    "    python finetune_openvla.py\n",
    "    \n",
    "    # Multi-GPU with accelerate\n",
    "    accelerate launch --num_processes 4 finetune_openvla.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "PSCRATCH = \"{PSCRATCH}\"\n",
    "MODEL_ID = \"openvla/openvla-7b\"\n",
    "DATA_DIR = f\"{{PSCRATCH}}/libero_data\"\n",
    "OUTPUT_DIR = f\"{{PSCRATCH}}/openvla_finetune\"\n",
    "\n",
    "# Set cache\n",
    "os.environ['HF_HOME'] = f\"{{PSCRATCH}}/.cache/huggingface\"\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class LIBERODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.processor = processor\n",
    "        self.samples = self._index_samples()\n",
    "    \n",
    "    def _index_samples(self):\n",
    "        samples = []\n",
    "        for filepath in self.data_dir.rglob(\"*.hdf5\"):\n",
    "            with h5py.File(filepath, 'r') as f:\n",
    "                language = f.attrs.get('language_instruction', 'perform task')\n",
    "                if isinstance(language, bytes):\n",
    "                    language = language.decode('utf-8')\n",
    "                \n",
    "                for demo_key in f['data'].keys():\n",
    "                    if not demo_key.startswith('demo_'):\n",
    "                        continue\n",
    "                    n_steps = len(f['data'][demo_key]['actions'])\n",
    "                    for t in range(n_steps):\n",
    "                        samples.append({{\n",
    "                            'filepath': str(filepath),\n",
    "                            'demo_key': demo_key,\n",
    "                            'timestep': t,\n",
    "                            'language': language,\n",
    "                        }})\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        with h5py.File(sample['filepath'], 'r') as f:\n",
    "            demo = f['data'][sample['demo_key']]\n",
    "            t = sample['timestep']\n",
    "            \n",
    "            image = demo['obs']['agentview_rgb'][t]\n",
    "            image = np.rot90(image, k=2)  # LIBERO convention\n",
    "            action = demo['actions'][t]\n",
    "        \n",
    "        # Format for OpenVLA\n",
    "        prompt = f\"In: What action should the robot take to {{sample['language'].lower()}}?\\\\nOut:\"\n",
    "        pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "        \n",
    "        # Process with OpenVLA processor\n",
    "        inputs = self.processor(prompt, pil_image)\n",
    "        inputs['action'] = torch.tensor(action, dtype=torch.float32)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "def main():\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    \n",
    "    # Add LoRA\n",
    "    print(\"Adding LoRA...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=32,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = LIBERODataset(DATA_DIR, processor)\n",
    "    print(f\"Dataset size: {{len(dataset)}}\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=10,\n",
    "        warmup_ratio=0.03,\n",
    "        bf16=True,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save\n",
    "    model.save_pretrained(f\"{{OUTPUT_DIR}}/final\")\n",
    "    print(f\"Model saved to {{OUTPUT_DIR}}/final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save script\n",
    "script_path = f\"{OUTPUT_DIR}/finetune_openvla.py\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(f\"Training script saved to: {script_path}\")\n",
    "print(\"\")\n",
    "print(\"To run training:\")\n",
    "print(f\"  cd {OUTPUT_DIR}\")\n",
    "print(f\"  python finetune_openvla.py\")\n",
    "print(\"\")\n",
    "print(\"For multi-GPU:\")\n",
    "print(f\"  accelerate launch --num_processes 4 finetune_openvla.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Load and Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned OpenVLA model with LoRA weights.\n",
    "    \"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        \"openvla/openvla-7b\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    \n",
    "    # Merge LoRA weights for faster inference (optional)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"openvla/openvla-7b\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Example usage\n",
    "print(\"To load fine-tuned model:\")\n",
    "print(f\"  model, processor = load_finetuned_model('{OUTPUT_DIR}/final')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate on LIBERO after fine-tuning\n",
    "evaluation_code = '''\n",
    "# After fine-tuning, evaluate on LIBERO:\n",
    "\n",
    "from libero.libero import benchmark\n",
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "\n",
    "# Load fine-tuned model\n",
    "model, processor = load_finetuned_model(f\"{OUTPUT_DIR}/final\")\n",
    "model.to(\"cuda:0\")\n",
    "model.eval()\n",
    "\n",
    "# Create policy with fine-tuned model\n",
    "# Now the model understands LIBERO tasks!\n",
    "policy = OpenVLAPolicy(model, processor, \"cuda:0\", unnorm_key=\"libero\")\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_suite(policy, \"libero_spatial\", n_trials=50)\n",
    "\n",
    "# Expected results after fine-tuning:\n",
    "# - LIBERO-Spatial: 70-80% success\n",
    "# - LIBERO-Object: 75-85% success\n",
    "# - LIBERO-Goal: 65-75% success\n",
    "'''\n",
    "\n",
    "print(\"After fine-tuning, run this code to evaluate:\")\n",
    "print(\"=\"*60)\n",
    "print(evaluation_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Fine-tuning Pipeline\n",
    "\n",
    "1. **Download LIBERO data** (50 demos per task)\n",
    "2. **Convert to training format** (image + instruction + action)\n",
    "3. **Fine-tune with LoRA** (memory efficient, ~23GB)\n",
    "4. **Evaluate** on LIBERO tasks\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "| Stage | LIBERO-Spatial Success |\n",
    "|-------|------------------------|\n",
    "| Zero-shot (no fine-tuning) | 0-10% |\n",
    "| After fine-tuning | 70-80% |\n",
    "\n",
    "### Training Time\n",
    "\n",
    "- **Single A100 (40GB)**: ~4-6 hours for 10 epochs\n",
    "- **4x A100**: ~1-2 hours with distributed training\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Download LIBERO demonstration data\n",
    "2. Run the fine-tuning script\n",
    "3. Evaluate the fine-tuned model\n",
    "4. Try different task suites (object, goal, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fine-tuning notebook complete!\")\n",
    "print(\"\")\n",
    "print(\"Quick Start:\")\n",
    "print(\"1. Download data:\")\n",
    "print(f\"   python -m libero.libero.benchmark_scripts.download_libero_datasets --save_dir {LIBERO_DATA_DIR}\")\n",
    "print(\"\")\n",
    "print(\"2. Run fine-tuning:\")\n",
    "print(f\"   python {OUTPUT_DIR}/finetune_openvla.py\")\n",
    "print(\"\")\n",
    "print(\"3. Evaluate:\")\n",
    "print(\"   Use notebook 08 with the fine-tuned model path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
