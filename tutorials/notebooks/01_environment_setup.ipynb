{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Environment Setup for OpenVLA\n",
    "\n",
    "**Goal**: Set up a remote server environment with 4×40GB GPUs for OpenVLA inference and LIBERO simulation.\n",
    "\n",
    "## Prerequisites\n",
    "- Remote server with 4× NVIDIA A100 40GB (or similar)\n",
    "- SSH access to the server\n",
    "- Basic familiarity with conda/pip environments\n",
    "\n",
    "## What We'll Cover\n",
    "1. CUDA and driver verification\n",
    "2. Conda environment creation\n",
    "3. Core dependencies installation\n",
    "4. LIBERO and MuJoCo setup\n",
    "5. Multi-GPU configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Verify GPU Setup\n",
    "\n",
    "First, let's verify your GPU configuration on the remote server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA driver and CUDA version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have 4 GPUs with 40GB each\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output should show 4 GPUs with ~40GB memory each (e.g., A100-40GB, A6000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Create Conda Environment\n",
    "\n",
    "We'll create a dedicated conda environment for OpenVLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and activate conda environment (run in terminal if needed)\n",
    "# !conda create -n openvla python=3.10 -y\n",
    "# !conda activate openvla\n",
    "\n",
    "# Verify Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "assert sys.version_info >= (3, 10), \"Python 3.10+ required\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Install Core OpenVLA Dependencies\n",
    "\n",
    "### 3.1 PyTorch with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA 12.1 support\n",
    "# Adjust cu121 to match your CUDA version (cu118, cu121, cu124)\n",
    "!pip install torch==2.2.0 torchvision --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# List all GPUs\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1e9:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 HuggingFace Transformers and Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers and related packages\n",
    "!pip install transformers==4.40.1 accelerate timm==0.9.10 tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flash Attention 2 for optimized inference\n",
    "# This significantly speeds up inference (2-3x faster)\n",
    "!pip install flash-attn==2.5.5 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Flash Attention installation\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Flash Attention not installed - inference will still work but slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Additional Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install remaining dependencies\n",
    "!pip install pillow numpy scipy matplotlib seaborn\n",
    "!pip install einops sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Install LIBERO and MuJoCo\n",
    "\n",
    "LIBERO is a simulation benchmark built on robosuite/MuJoCo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MuJoCo\n",
    "!pip install mujoco==2.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify MuJoCo installation\n",
    "import mujoco\n",
    "print(f\"MuJoCo version: {mujoco.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install robosuite (LIBERO dependency)\n",
    "!pip install robosuite==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LIBERO\n",
    "!pip install libero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify LIBERO installation\n",
    "try:\n",
    "    from libero.libero import benchmark\n",
    "    print(\"LIBERO installed successfully!\")\n",
    "    \n",
    "    # List available task suites\n",
    "    print(\"\\nAvailable task suites:\")\n",
    "    for suite_name in [\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_90\"]:\n",
    "        suite = benchmark.get_benchmark_dict(suite_name)\n",
    "        print(f\"  {suite_name}: {suite['n_tasks']} tasks\")\n",
    "except ImportError as e:\n",
    "    print(f\"LIBERO import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 For Headless Servers (No Display)\n",
    "\n",
    "If your remote server has no display, you need to set up virtual display for rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system packages for headless rendering (run as sudo if needed)\n",
    "# These commands should be run in terminal:\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install -y xvfb libgl1-mesa-glx libosmesa6-dev\n",
    "\n",
    "# For MuJoCo rendering without display\n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'osmesa'  # or 'egl' for GPU rendering\n",
    "print(f\"MUJOCO_GL set to: {os.environ.get('MUJOCO_GL')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use EGL for GPU-accelerated headless rendering\n",
    "# os.environ['MUJOCO_GL'] = 'egl'\n",
    "# os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Install OpenVLA from Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to OpenVLA repository root\n",
    "import os\n",
    "REPO_ROOT = \"/Users/davidpark/Documents/Claude/openvla\"  # Adjust to your path\n",
    "os.chdir(REPO_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenVLA in development mode\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify OpenVLA installation\n",
    "from prismatic.models import load_vla\n",
    "print(\"OpenVLA package loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Multi-GPU Configuration\n",
    "\n",
    "With 4×40GB GPUs, we have several options for model loading and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Single GPU (simplest, ~14GB for 7B model with BF16)\n",
    "GPU_CONFIG = {\n",
    "    \"single_gpu\": {\n",
    "        \"device_map\": \"cuda:0\",\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"description\": \"Load entire model on GPU 0\"\n",
    "    },\n",
    "    \n",
    "    # Option 2: Model parallelism across GPUs (for larger batch sizes)\n",
    "    \"multi_gpu_parallel\": {\n",
    "        \"device_map\": \"auto\",\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"description\": \"Automatically distribute model across available GPUs\"\n",
    "    },\n",
    "    \n",
    "    # Option 3: Multiple model instances (for parallel rollouts)\n",
    "    \"multi_instance\": {\n",
    "        \"devices\": [\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"],\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"description\": \"One model per GPU for parallel environment rollouts\"\n",
    "    },\n",
    "    \n",
    "    # Option 4: Quantized (for memory-constrained scenarios)\n",
    "    \"quantized_8bit\": {\n",
    "        \"device_map\": \"auto\",\n",
    "        \"load_in_8bit\": True,\n",
    "        \"description\": \"8-bit quantization for ~50% memory savings\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, config in GPU_CONFIG.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  {config['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate memory requirements\n",
    "def estimate_model_memory(num_params_billions, dtype=\"bfloat16\"):\n",
    "    \"\"\"Estimate GPU memory needed for model parameters.\"\"\"\n",
    "    bytes_per_param = {\n",
    "        \"float32\": 4,\n",
    "        \"float16\": 2,\n",
    "        \"bfloat16\": 2,\n",
    "        \"int8\": 1,\n",
    "        \"int4\": 0.5\n",
    "    }\n",
    "    \n",
    "    param_memory_gb = num_params_billions * bytes_per_param[dtype]\n",
    "    # Add ~20% overhead for activations, gradients, etc.\n",
    "    total_memory_gb = param_memory_gb * 1.2\n",
    "    \n",
    "    return param_memory_gb, total_memory_gb\n",
    "\n",
    "print(\"OpenVLA-7B Memory Estimates:\")\n",
    "print(\"=\"*50)\n",
    "for dtype in [\"float32\", \"bfloat16\", \"int8\", \"int4\"]:\n",
    "    param_mem, total_mem = estimate_model_memory(7, dtype)\n",
    "    print(f\"{dtype:>10}: {param_mem:.1f} GB params, ~{total_mem:.1f} GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Download OpenVLA Model\n",
    "\n",
    "Pre-download the model to avoid timeouts during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model (this may take a while - ~14GB)\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "MODEL_ID = \"openvla/openvla-7b\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID}...\")\n",
    "print(\"This will download ~14GB of model weights. Please be patient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download processor (tokenizer + image processor)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Processor downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model (without loading to GPU yet)\n",
    "# This caches the model for future use\n",
    "import torch\n",
    "\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    # Don't load Flash Attention yet - just caching\n",
    ")\n",
    "\n",
    "print(\"Model downloaded and cached!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in vla.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear model from memory (we'll load properly in later notebooks)\n",
    "del vla\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Final Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_environment():\n",
    "    \"\"\"Comprehensive environment check.\"\"\"\n",
    "    import sys\n",
    "    import torch\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Python\n",
    "    results[\"Python\"] = sys.version.split()[0]\n",
    "    \n",
    "    # PyTorch\n",
    "    results[\"PyTorch\"] = torch.__version__\n",
    "    results[\"CUDA Available\"] = torch.cuda.is_available()\n",
    "    results[\"GPU Count\"] = torch.cuda.device_count()\n",
    "    \n",
    "    # Transformers\n",
    "    import transformers\n",
    "    results[\"Transformers\"] = transformers.__version__\n",
    "    \n",
    "    # TIMM\n",
    "    import timm\n",
    "    results[\"TIMM\"] = timm.__version__\n",
    "    \n",
    "    # Flash Attention\n",
    "    try:\n",
    "        import flash_attn\n",
    "        results[\"Flash Attention\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        results[\"Flash Attention\"] = \"Not installed\"\n",
    "    \n",
    "    # MuJoCo\n",
    "    try:\n",
    "        import mujoco\n",
    "        results[\"MuJoCo\"] = mujoco.__version__\n",
    "    except ImportError:\n",
    "        results[\"MuJoCo\"] = \"Not installed\"\n",
    "    \n",
    "    # LIBERO\n",
    "    try:\n",
    "        import libero\n",
    "        results[\"LIBERO\"] = \"Installed\"\n",
    "    except ImportError:\n",
    "        results[\"LIBERO\"] = \"Not installed\"\n",
    "    \n",
    "    # OpenVLA\n",
    "    try:\n",
    "        from prismatic.models import load_vla\n",
    "        results[\"OpenVLA\"] = \"Installed\"\n",
    "    except ImportError:\n",
    "        results[\"OpenVLA\"] = \"Not installed\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run check\n",
    "print(\"Environment Status\")\n",
    "print(\"=\"*50)\n",
    "for key, value in check_environment().items():\n",
    "    status = \"✅\" if value not in [\"Not installed\", False, 0] else \"❌\"\n",
    "    print(f\"{status} {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You now have a complete environment for:\n",
    "- Running OpenVLA-7B inference on 4×40GB GPUs\n",
    "- Using Flash Attention for optimized inference\n",
    "- LIBERO simulation for evaluation\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **02_architecture_overview.ipynb** to understand OpenVLA's model architecture.\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| CUDA out of memory | Use `load_in_8bit=True` or single GPU |\n",
    "| Flash Attention build fails | Install from source: `pip install flash-attn --no-build-isolation` |\n",
    "| MuJoCo rendering fails | Set `MUJOCO_GL=osmesa` or install EGL |\n",
    "| LIBERO import error | Install with `pip install libero` |\n",
    "| Model download timeout | Use `huggingface-cli download openvla/openvla-7b` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
