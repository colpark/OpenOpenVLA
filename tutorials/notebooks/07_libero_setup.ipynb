{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. LIBERO Simulation Setup\n",
    "\n",
    "**Goal**: Set up the LIBERO simulation environment for evaluating OpenVLA.\n",
    "\n",
    "## What We'll Learn\n",
    "1. LIBERO benchmark overview\n",
    "2. Environment setup and configuration\n",
    "3. Task suites and scenarios\n",
    "4. Running rollouts in simulation\n",
    "5. Recording videos for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. LIBERO Benchmark Overview\n",
    "\n",
    "**LIBERO** (LIfelong robot BEnchmark for RObotics) is a simulation benchmark for evaluating robot manipulation policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libero_overview = \"\"\"\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                      LIBERO Benchmark                               │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  4 Task Suites, 90 Total Tasks                                      │\n",
    "│                                                                     │\n",
    "│  ┌──────────────────┬───────────────────────────────────────────┐  │\n",
    "│  │ LIBERO-Spatial   │ 10 tasks with spatial variations          │  │\n",
    "│  │                  │ Same objects, different positions          │  │\n",
    "│  ├──────────────────┼───────────────────────────────────────────┤  │\n",
    "│  │ LIBERO-Object    │ 10 tasks with object variations           │  │\n",
    "│  │                  │ Same positions, different objects          │  │\n",
    "│  ├──────────────────┼───────────────────────────────────────────┤  │\n",
    "│  │ LIBERO-Goal      │ 10 tasks with goal variations             │  │\n",
    "│  │                  │ Same setup, different target goals         │  │\n",
    "│  ├──────────────────┼───────────────────────────────────────────┤  │\n",
    "│  │ LIBERO-90        │ Full 90-task benchmark                    │  │\n",
    "│  │                  │ Comprehensive evaluation                   │  │\n",
    "│  └──────────────────┴───────────────────────────────────────────┘  │\n",
    "│                                                                     │\n",
    "│  Robot: Franka Emika Panda (7-DoF arm + gripper)                   │\n",
    "│  Physics: MuJoCo via robosuite                                      │\n",
    "│  Observation: 256×256 RGB image (agentview)                        │\n",
    "│  Action: 7-DoF (6D pose delta + gripper)                           │\n",
    "│                                                                     │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "print(libero_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CRITICAL: Set these BEFORE importing any packages!\n# ============================================================\nimport os\n\n# For NERSC Perlmutter, use your $PSCRATCH directory\nPSCRATCH = \"/pscratch/sd/d/dpark1\"  # CHANGE THIS TO YOUR PATH\nCACHE_DIR = f\"{PSCRATCH}/.cache\"\n\n# Set all cache directories to $PSCRATCH/.cache\nos.environ['XDG_CACHE_HOME'] = CACHE_DIR\nos.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\nos.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\nos.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n\n# ============================================================\n# MuJoCo/OpenGL rendering setup - MUST be set before imports!\n# ============================================================\n# Option 1: OSMesa (CPU rendering, headless) - requires libosmesa\n# Option 2: EGL (GPU rendering, headless) - requires GPU + EGL libs\n# Option 3: GLFW (display rendering) - requires display\n\n# For NERSC Perlmutter GPU nodes, try EGL first (faster)\n# For CPU-only or if EGL fails, use OSMesa\nRENDER_MODE = \"egl\"  # Change to \"osmesa\" if EGL doesn't work\n\nos.environ['MUJOCO_GL'] = RENDER_MODE\nos.environ['PYOPENGL_PLATFORM'] = RENDER_MODE  # Must match MUJOCO_GL!\n\n# Create directories\nfor path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n    os.makedirs(path, exist_ok=True)\n\nprint(f\"✅ All caches → {CACHE_DIR}\")\nprint(f\"✅ MUJOCO_GL = {os.environ.get('MUJOCO_GL')}\")\nprint(f\"✅ PYOPENGL_PLATFORM = {os.environ.get('PYOPENGL_PLATFORM')}\")\nprint(\"\")\nprint(\"If you see OpenGL errors, try changing RENDER_MODE to 'osmesa' or 'egl'\")\nprint(\"For OSMesa: module load osmesa  # or apt-get install libosmesa6-dev\")\nprint(\"For EGL: requires GPU node with EGL support\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\n\n# Import LIBERO - this may take a moment\nprint(\"Importing LIBERO (this loads MuJoCo and robosuite)...\")\ntry:\n    from libero.libero import benchmark\n    from libero.libero.envs import OffScreenRenderEnv\n    print(\"✅ LIBERO imported successfully!\")\nexcept ImportError as e:\n    print(f\"❌ LIBERO import error: {e}\")\n    print(\"\\nInstall with: pip install libero\")\nexcept AttributeError as e:\n    if \"glGetError\" in str(e) or \"GL\" in str(e):\n        print(f\"❌ OpenGL rendering error: {e}\")\n        print(\"\\nTroubleshooting:\")\n        print(\"1. Try changing RENDER_MODE in the cell above:\")\n        print(\"   - 'egl' for GPU nodes (faster)\")\n        print(\"   - 'osmesa' for CPU rendering\")\n        print(\"\")\n        print(\"2. For NERSC Perlmutter, load required modules:\")\n        print(\"   module load cudatoolkit  # for EGL\")\n        print(\"   # or\")\n        print(\"   module load osmesa  # for OSMesa\")\n        print(\"\")\n        print(\"3. Restart the kernel after changing RENDER_MODE\")\n    else:\n        raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import robosuite\n",
    "try:\n",
    "    import robosuite\n",
    "    print(f\"robosuite version: {robosuite.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"robosuite import error: {e}\")\n",
    "    print(\"Install with: pip install robosuite==1.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MuJoCo\n",
    "try:\n",
    "    import mujoco\n",
    "    print(f\"MuJoCo version: {mujoco.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"MuJoCo import error: {e}\")\n",
    "    print(\"Install with: pip install mujoco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load LIBERO Task Suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore LIBERO benchmark API\nprint(\"=\"*60)\nprint(\"LIBERO Benchmark API Exploration\")\nprint(\"=\"*60)\n\n# List all available methods on benchmark module\nprint(\"\\n1. Available benchmark methods:\")\nbenchmark_methods = [m for m in dir(benchmark) if not m.startswith('_')]\nfor m in benchmark_methods:\n    print(f\"   - {m}\")\n\n# Try to understand what get_benchmark_dict returns\nprint(\"\\n2. Testing get_benchmark_dict('libero_spatial'):\")\ntest_result = benchmark.get_benchmark_dict(\"libero_spatial\")\nprint(f\"   Type: {type(test_result)}\")\nprint(f\"   Value: {test_result}\")\n\n# Try getting a task directly\nprint(\"\\n3. Testing get_task('libero_spatial', 0):\")\ntry:\n    test_task = benchmark.get_task(\"libero_spatial\", 0)\n    print(f\"   Type: {type(test_task)}\")\n    print(f\"   Attributes: {[a for a in dir(test_task) if not a.startswith('_')]}\")\n    if hasattr(test_task, 'name'):\n        print(f\"   name: {test_task.name}\")\n    if hasattr(test_task, 'language'):\n        print(f\"   language: {test_task.language}\")\nexcept Exception as e:\n    print(f\"   Error: {e}\")\n\n# Check if there's a get_libero_path or similar\nprint(\"\\n4. Checking for other useful functions:\")\nfor func_name in ['get_libero_path', 'get_task_bddl', 'BENCHMARK_DICT', 'TASK_DICT']:\n    if hasattr(benchmark, func_name):\n        val = getattr(benchmark, func_name)\n        print(f\"   {func_name}: {type(val)}\")\n        if isinstance(val, dict):\n            print(f\"      Keys: {list(val.keys())[:5]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define helper function based on API exploration above\n# This function tries multiple approaches to get task names\n\ndef get_task_names(suite_name):\n    \"\"\"\n    Get task names from a LIBERO suite.\n    Works with different LIBERO versions by trying multiple approaches.\n    \"\"\"\n    # Approach 1: Try direct API call if it exists\n    if hasattr(benchmark, 'get_task_names'):\n        try:\n            names = benchmark.get_task_names(suite_name)\n            if names:\n                return names\n        except Exception:\n            pass\n    \n    # Approach 2: Try to get tasks one by one\n    # Known task counts per suite\n    suite_task_counts = {\n        'libero_spatial': 10,\n        'libero_object': 10,\n        'libero_goal': 10,\n        'libero_10': 10,\n        'libero_90': 90,\n        'libero_100': 100,\n    }\n    \n    n_tasks = suite_task_counts.get(suite_name, 10)\n    \n    names = []\n    for i in range(n_tasks):\n        try:\n            task = benchmark.get_task(suite_name, i)\n            if hasattr(task, 'name'):\n                names.append(task.name)\n            elif hasattr(task, 'language'):\n                names.append(task.language)\n            else:\n                names.append(f\"task_{i}\")\n        except Exception as e:\n            # Stop if we hit an error (might have fewer tasks)\n            if i == 0:\n                print(f\"Warning: Could not get task 0: {e}\")\n            break\n    \n    return names\n\n# Test the helper function\nprint(\"Testing get_task_names():\")\nprint(\"=\"*60)\n\ntask_suites = [\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_90\"]\n\nfor suite_name in task_suites:\n    names = get_task_names(suite_name)\n    print(f\"\\n{suite_name}: {len(names)} tasks\")\n    if names:\n        for name in names[:3]:\n            print(f\"   - {name}\")\n        if len(names) > 3:\n            print(f\"   ... and {len(names)-3} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create a LIBERO Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select a task suite and get task names\nSUITE_NAME = \"libero_spatial\"\ntask_names = get_task_names(SUITE_NAME)\n\nprint(f\"Selected suite: {SUITE_NAME}\")\nprint(f\"Number of tasks found: {len(task_names)}\")\n\nif len(task_names) == 0:\n    print(\"\\n⚠️ No tasks found! Let's try getting a task directly...\")\n    # Try to get task 0 directly\n    try:\n        task = benchmark.get_task(SUITE_NAME, 0)\n        print(f\"Direct task access works!\")\n        print(f\"Task type: {type(task)}\")\n        print(f\"Task attributes: {[a for a in dir(task) if not a.startswith('_')]}\")\n        task_names = [f\"task_{i}\" for i in range(10)]  # Use placeholder names\n    except Exception as e:\n        print(f\"Error getting task directly: {e}\")\n        print(\"\\nPlease check your LIBERO installation.\")\nelse:\n    print(f\"\\nTasks in {SUITE_NAME}:\")\n    for i, name in enumerate(task_names):\n        print(f\"  {i}: {name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select a specific task and get its configuration\nTASK_ID = 0\n\n# Get task directly (this works even if task_names list is empty)\ntask = benchmark.get_task(SUITE_NAME, TASK_ID)\n\nprint(\"Task Configuration:\")\nprint(\"=\"*60)\nprint(f\"Task ID: {TASK_ID}\")\n\n# Print available attributes\nif hasattr(task, 'name'):\n    print(f\"Name: {task.name}\")\nif hasattr(task, 'language'):\n    print(f\"Language instruction: {task.language}\")\nif hasattr(task, 'bddl_file'):\n    print(f\"BDDL file: {task.bddl_file}\")\nif hasattr(task, 'problem_info'):\n    print(f\"Problem info: {task.problem_info}\")\n\n# Show all available attributes for debugging\nprint(f\"\\nAll task attributes: {[a for a in dir(task) if not a.startswith('_')]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "def create_libero_env(task, image_size=256):\n",
    "    \"\"\"\n",
    "    Create a LIBERO environment for a given task.\n",
    "    \n",
    "    Args:\n",
    "        task: LIBERO task object\n",
    "        image_size: Size of rendered images\n",
    "    \n",
    "    Returns:\n",
    "        OffScreenRenderEnv environment\n",
    "    \"\"\"\n",
    "    env_args = {\n",
    "        \"bddl_file_name\": task.bddl_file,\n",
    "        \"camera_heights\": image_size,\n",
    "        \"camera_widths\": image_size,\n",
    "    }\n",
    "    \n",
    "    env = OffScreenRenderEnv(**env_args)\n",
    "    env.seed(0)  # For reproducibility\n",
    "    \n",
    "    return env\n",
    "\n",
    "print(\"Creating environment...\")\n",
    "env = create_libero_env(task)\n",
    "print(\"Environment created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore environment properties\n",
    "print(\"Environment Properties:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Action space: {env.action_spec}\")\n",
    "print(f\"Action dimension: {env.action_dim}\")\n",
    "print(f\"\\nObservation keys:\")\n",
    "\n",
    "# Reset to get observation structure\n",
    "obs = env.reset()\n",
    "for key, value in obs.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render initial observation\n",
    "def get_observation_image(obs, key='agentview_image'):\n",
    "    \"\"\"\n",
    "    Extract RGB image from observation dict.\n",
    "    \n",
    "    LIBERO convention: Images need to be rotated 180 degrees.\n",
    "    \"\"\"\n",
    "    image = obs[key]\n",
    "    \n",
    "    # LIBERO images are upside down - rotate 180 degrees\n",
    "    image = np.rot90(image, k=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Get and display the initial observation\n",
    "obs = env.reset()\n",
    "image = get_observation_image(obs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Initial Observation\\nTask: {task.language}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute some random actions to see the environment dynamics\n",
    "def run_random_rollout(env, n_steps=10):\n",
    "    \"\"\"\n",
    "    Run a random rollout and collect images.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    images = [get_observation_image(obs)]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Random action\n",
    "        action = np.random.uniform(-0.1, 0.1, size=env.action_dim)\n",
    "        action[-1] = 0  # Keep gripper open\n",
    "        \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        images.append(get_observation_image(obs))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Run random rollout\n",
    "print(\"Running random rollout (10 steps)...\")\n",
    "rollout_images = run_random_rollout(env, n_steps=10)\n",
    "\n",
    "# Display frames\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, (ax, img) in enumerate(zip(axes.flat, rollout_images[:10])):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Step {i}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Random Rollout\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Action Space Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_info = \"\"\"\n",
    "LIBERO Action Space (7-DoF)\n",
    "============================\n",
    "\n",
    "Action dimensions:\n",
    "  [0] dx:      End-effector X displacement\n",
    "  [1] dy:      End-effector Y displacement  \n",
    "  [2] dz:      End-effector Z displacement\n",
    "  [3] d_roll:  Rotation around X axis\n",
    "  [4] d_pitch: Rotation around Y axis\n",
    "  [5] d_yaw:   Rotation around Z axis\n",
    "  [6] gripper: Gripper action (-1: open, 1: close)\n",
    "\n",
    "Action bounds (typical):\n",
    "  Position: [-0.05, 0.05] meters per step\n",
    "  Rotation: [-0.17, 0.17] radians per step  \n",
    "  Gripper:  [-1.0, 1.0]\n",
    "\n",
    "Note: OpenVLA outputs normalized actions in [-1, 1]\n",
    "that need to be scaled to these bounds.\n",
    "\"\"\"\n",
    "print(action_space_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine action bounds\n",
    "print(\"Environment Action Specification:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "action_spec = env.action_spec\n",
    "print(f\"Action dimension: {env.action_dim}\")\n",
    "print(f\"Action spec shape: {action_spec[0].shape}\")\n",
    "\n",
    "# Typical LIBERO action bounds\n",
    "libero_action_bounds = {\n",
    "    'position': (-0.05, 0.05),    # meters\n",
    "    'rotation': (-0.17, 0.17),    # radians (~10 degrees)\n",
    "    'gripper': (-1.0, 1.0),\n",
    "}\n",
    "\n",
    "print(f\"\\nTypical action bounds:\")\n",
    "for dim, bounds in libero_action_bounds.items():\n",
    "    print(f\"  {dim}: [{bounds[0]}, {bounds[1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Image Preprocessing for OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBERO image preprocessing to match OpenVLA training\n",
    "import io\n",
    "\n",
    "def preprocess_libero_image_for_openvla(obs, key='agentview_image', target_size=224):\n",
    "    \"\"\"\n",
    "    Preprocess LIBERO observation image for OpenVLA inference.\n",
    "    \n",
    "    This matches the preprocessing used during OpenVLA training:\n",
    "    1. Rotate 180 degrees (LIBERO convention)\n",
    "    2. Encode as JPEG and decode (matches training augmentation)\n",
    "    3. Resize with Lanczos interpolation\n",
    "    \"\"\"\n",
    "    # Get image from observation\n",
    "    image = obs[key]\n",
    "    \n",
    "    # Rotate 180 degrees (LIBERO images are upside down)\n",
    "    image = np.rot90(image, k=2)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    \n",
    "    # JPEG encode/decode (matches training preprocessing)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format='JPEG', quality=95)\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    \n",
    "    # Resize to target size\n",
    "    pil_image = pil_image.resize((target_size, target_size), Image.LANCZOS)\n",
    "    \n",
    "    return pil_image\n",
    "\n",
    "# Test preprocessing\n",
    "obs = env.reset()\n",
    "processed_image = preprocess_libero_image_for_openvla(obs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(get_observation_image(obs))\n",
    "axes[0].set_title(f\"Raw (256×256)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(processed_image)\n",
    "axes[1].set_title(f\"Preprocessed for OpenVLA (224×224)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Running a Policy Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_rollout(\n",
    "    env,\n",
    "    policy_fn,\n",
    "    instruction,\n",
    "    max_steps=400,\n",
    "    record_video=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a policy rollout in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: LIBERO environment\n",
    "        policy_fn: Function(image, instruction) -> action\n",
    "        instruction: Task instruction string\n",
    "        max_steps: Maximum steps per episode\n",
    "        record_video: Whether to record frames\n",
    "    \n",
    "    Returns:\n",
    "        success: Whether task was completed\n",
    "        frames: List of observation images (if record_video)\n",
    "        actions: List of executed actions\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    \n",
    "    frames = []\n",
    "    actions = []\n",
    "    success = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Get preprocessed image\n",
    "        image = preprocess_libero_image_for_openvla(obs)\n",
    "        \n",
    "        if record_video:\n",
    "            frames.append(get_observation_image(obs))\n",
    "        \n",
    "        # Get action from policy\n",
    "        action = policy_fn(image, instruction)\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Execute action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            success = info.get('success', reward > 0)\n",
    "            break\n",
    "    \n",
    "    if record_video:\n",
    "        frames.append(get_observation_image(obs))\n",
    "    \n",
    "    return success, frames, actions\n",
    "\n",
    "# Example: Run with random policy\n",
    "def random_policy(image, instruction):\n",
    "    \"\"\"Random policy for testing.\"\"\"\n",
    "    action = np.random.uniform(-0.05, 0.05, size=7)\n",
    "    return action\n",
    "\n",
    "print(\"Running random policy rollout...\")\n",
    "success, frames, actions = run_policy_rollout(\n",
    "    env, \n",
    "    random_policy, \n",
    "    task.language,\n",
    "    max_steps=50,\n",
    ")\n",
    "\n",
    "print(f\"\\nRollout completed:\")\n",
    "print(f\"  Success: {success}\")\n",
    "print(f\"  Steps: {len(actions)}\")\n",
    "print(f\"  Frames recorded: {len(frames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Video Recording Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(frames, output_path, fps=20):\n",
    "    \"\"\"\n",
    "    Save frames as MP4 video.\n",
    "    \n",
    "    Args:\n",
    "        frames: List of numpy arrays (H, W, 3)\n",
    "        output_path: Path to save video\n",
    "        fps: Frames per second\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import imageio\n",
    "        \n",
    "        # Ensure frames are uint8\n",
    "        frames_uint8 = [f.astype(np.uint8) for f in frames]\n",
    "        \n",
    "        imageio.mimsave(output_path, frames_uint8, fps=fps)\n",
    "        print(f\"Video saved to: {output_path}\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"imageio not installed. Install with: pip install imageio imageio-ffmpeg\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving video: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the rollout video\n",
    "video_path = \"/tmp/libero_rollout.mp4\"\n",
    "save_video(frames, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display frames as animation (for Jupyter)\n",
    "def display_rollout_frames(frames, step_size=5):\n",
    "    \"\"\"\n",
    "    Display rollout frames in a grid.\n",
    "    \"\"\"\n",
    "    # Sample frames\n",
    "    sampled = frames[::step_size]\n",
    "    n_frames = len(sampled)\n",
    "    n_cols = min(5, n_frames)\n",
    "    n_rows = (n_frames + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "    axes = np.array(axes).flatten()\n",
    "    \n",
    "    for i, (ax, frame) in enumerate(zip(axes, sampled)):\n",
    "        ax.imshow(frame)\n",
    "        ax.set_title(f\"Step {i * step_size}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for ax in axes[len(sampled):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_rollout_frames(frames, step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Multiple Task Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_on_suite(\n    suite_name,\n    policy_fn,\n    n_trials_per_task=20,\n    max_steps=400,\n    verbose=True\n):\n    \"\"\"\n    Evaluate a policy on a full LIBERO task suite.\n    \n    Args:\n        suite_name: Name of task suite (e.g., 'libero_spatial')\n        policy_fn: Policy function(image, instruction) -> action\n        n_trials_per_task: Number of trials per task\n        max_steps: Maximum steps per episode\n        verbose: Print progress\n    \n    Returns:\n        results: Dict mapping task_id -> success_rate\n    \"\"\"\n    task_names_list = get_task_names(suite_name)  # Use our helper function\n    results = {}\n    \n    for task_id, task_name in enumerate(task_names_list):\n        if verbose:\n            print(f\"\\nEvaluating task {task_id}: {task_name}\")\n        \n        # Get task and create environment\n        task = benchmark.get_task(suite_name, task_id)\n        env = create_libero_env(task)\n        \n        successes = 0\n        for trial in range(n_trials_per_task):\n            env.seed(trial)  # Different initial state\n            \n            success, _, _ = run_policy_rollout(\n                env, policy_fn, task.language,\n                max_steps=max_steps,\n                record_video=False\n            )\n            \n            successes += int(success)\n            \n            if verbose and trial % 5 == 0:\n                print(f\"  Trial {trial}/{n_trials_per_task}\")\n        \n        success_rate = successes / n_trials_per_task\n        results[task_id] = success_rate\n        \n        if verbose:\n            print(f\"  Success rate: {success_rate:.1%}\")\n        \n        env.close()\n    \n    # Overall statistics\n    mean_success = np.mean(list(results.values()))\n    if verbose:\n        print(f\"\\nOverall success rate: {mean_success:.1%}\")\n    \n    return results\n\n# Example: Quick evaluation on one task\nprint(\"Evaluation framework ready!\")\nprint(\"Full evaluation will be demonstrated in notebook 08.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### LIBERO Setup Complete\n",
    "\n",
    "1. **Environment**: MuJoCo-based simulation with Franka robot\n",
    "\n",
    "2. **Task Suites**: 4 suites with 90 total tasks\n",
    "   - libero_spatial (10 tasks)\n",
    "   - libero_object (10 tasks)\n",
    "   - libero_goal (10 tasks)\n",
    "   - libero_90 (90 tasks)\n",
    "\n",
    "3. **Observations**: 256×256 RGB agentview images\n",
    "\n",
    "4. **Actions**: 7-DoF (position, rotation, gripper)\n",
    "\n",
    "5. **Preprocessing**: Rotate 180°, JPEG encode/decode, resize to 224×224\n",
    "\n",
    "### For Remote Server\n",
    "- Set `MUJOCO_GL=osmesa` for CPU rendering\n",
    "- Or `MUJOCO_GL=egl` for GPU-accelerated rendering\n",
    "- Install: `sudo apt-get install libosmesa6-dev`\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **08_integrated_evaluation.ipynb** for full OpenVLA + LIBERO evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"Environment closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}