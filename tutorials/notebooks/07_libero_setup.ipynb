{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. LIBERO Simulation Setup\n",
    "\n",
    "**Goal**: Set up the LIBERO simulation environment for evaluating OpenVLA.\n",
    "\n",
    "## What We'll Learn\n",
    "1. LIBERO benchmark overview\n",
    "2. Environment setup and configuration\n",
    "3. Task suites and scenarios\n",
    "4. Running rollouts in simulation\n",
    "5. Recording videos for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. LIBERO Benchmark Overview\n",
    "\n",
    "**LIBERO** (LIfelong robot BEnchmark for RObotics) is a simulation benchmark for evaluating robot manipulation policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libero_overview = \"\"\"\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                      LIBERO Benchmark                               │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  4 Task Suites, 90 Total Tasks                                      │\n",
    "│                                                                     │\n",
    "│  ┌──────────────────┬───────────────────────────────────────────┐  │\n",
    "│  │ LIBERO-Spatial   │ 10 tasks with spatial variations          │  │\n",
    "│  │                  │ Same objects, different positions          │  │\n",
    "│  ├──────────────────┼───────────────────────────────────────────┤  │\n",
    "│  │ LIBERO-Object    │ 10 tasks with object variations           │  │\n",
    "│  │                  │ Same positions, different objects          │  │\n",
    "│  ├──────────────────┼───────────────────────────────────────────┤  │\n",
    "│  │ LIBERO-Goal      │ 10 tasks with goal variations             │  │\n",
    "│  │                  │ Same setup, different target goals         │  │\n",
    "│  ├──────────────────┼───────────────────────────────────────────┤  │\n",
    "│  │ LIBERO-90        │ Full 90-task benchmark                    │  │\n",
    "│  │                  │ Comprehensive evaluation                   │  │\n",
    "│  └──────────────────┴───────────────────────────────────────────┘  │\n",
    "│                                                                     │\n",
    "│  Robot: Franka Emika Panda (7-DoF arm + gripper)                   │\n",
    "│  Physics: MuJoCo via robosuite                                      │\n",
    "│  Observation: 256×256 RGB image (agentview)                        │\n",
    "│  Action: 7-DoF (6D pose delta + gripper)                           │\n",
    "│                                                                     │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "print(libero_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CRITICAL: Set these BEFORE importing any packages!\n# ============================================================\nimport os\n\n# For NERSC Perlmutter, use your $PSCRATCH directory\nPSCRATCH = \"/pscratch/sd/d/dpark1\"  # CHANGE THIS TO YOUR PATH\nCACHE_DIR = f\"{PSCRATCH}/.cache\"\n\n# Set all cache directories to $PSCRATCH/.cache\nos.environ['XDG_CACHE_HOME'] = CACHE_DIR\nos.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\nos.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\nos.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n\n# ============================================================\n# MuJoCo/OpenGL rendering setup - MUST be set before imports!\n# ============================================================\n# Option 1: OSMesa (CPU rendering, headless) - requires libosmesa\n# Option 2: EGL (GPU rendering, headless) - requires GPU + EGL libs\n# Option 3: GLFW (display rendering) - requires display\n\n# For NERSC Perlmutter GPU nodes, try EGL first (faster)\n# For CPU-only or if EGL fails, use OSMesa\nRENDER_MODE = \"egl\"  # Change to \"osmesa\" if EGL doesn't work\n\nos.environ['MUJOCO_GL'] = RENDER_MODE\nos.environ['PYOPENGL_PLATFORM'] = RENDER_MODE  # Must match MUJOCO_GL!\n\n# Create directories\nfor path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n    os.makedirs(path, exist_ok=True)\n\nprint(f\"✅ All caches → {CACHE_DIR}\")\nprint(f\"✅ MUJOCO_GL = {os.environ.get('MUJOCO_GL')}\")\nprint(f\"✅ PYOPENGL_PLATFORM = {os.environ.get('PYOPENGL_PLATFORM')}\")\nprint(\"\")\nprint(\"If you see OpenGL errors, try changing RENDER_MODE to 'osmesa' or 'egl'\")\nprint(\"For OSMesa: module load osmesa  # or apt-get install libosmesa6-dev\")\nprint(\"For EGL: requires GPU node with EGL support\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\n\n# Import LIBERO - this may take a moment\nprint(\"Importing LIBERO (this loads MuJoCo and robosuite)...\")\ntry:\n    from libero.libero import benchmark\n    from libero.libero.envs import OffScreenRenderEnv\n    print(\"✅ LIBERO imported successfully!\")\nexcept ImportError as e:\n    print(f\"❌ LIBERO import error: {e}\")\n    print(\"\\nInstall with: pip install libero\")\nexcept AttributeError as e:\n    if \"glGetError\" in str(e) or \"GL\" in str(e):\n        print(f\"❌ OpenGL rendering error: {e}\")\n        print(\"\\nTroubleshooting:\")\n        print(\"1. Try changing RENDER_MODE in the cell above:\")\n        print(\"   - 'egl' for GPU nodes (faster)\")\n        print(\"   - 'osmesa' for CPU rendering\")\n        print(\"\")\n        print(\"2. For NERSC Perlmutter, load required modules:\")\n        print(\"   module load cudatoolkit  # for EGL\")\n        print(\"   # or\")\n        print(\"   module load osmesa  # for OSMesa\")\n        print(\"\")\n        print(\"3. Restart the kernel after changing RENDER_MODE\")\n    else:\n        raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import robosuite\n",
    "try:\n",
    "    import robosuite\n",
    "    print(f\"robosuite version: {robosuite.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"robosuite import error: {e}\")\n",
    "    print(\"Install with: pip install robosuite==1.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MuJoCo\n",
    "try:\n",
    "    import mujoco\n",
    "    print(f\"MuJoCo version: {mujoco.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"MuJoCo import error: {e}\")\n",
    "    print(\"Install with: pip install mujoco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load LIBERO Task Suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore LIBERO benchmark API thoroughly\nprint(\"=\"*60)\nprint(\"LIBERO Benchmark API Exploration\")\nprint(\"=\"*60)\n\n# 1. List ALL available items in benchmark module\nprint(\"\\n1. Everything in benchmark module:\")\nall_items = [m for m in dir(benchmark) if not m.startswith('_')]\nfor item in all_items:\n    obj = getattr(benchmark, item)\n    print(f\"   {item}: {type(obj).__name__}\")\n\n# 2. Check for common class names\nprint(\"\\n2. Looking for Benchmark classes:\")\nfor name in ['Benchmark', 'BENCHMARK', 'BenchmarkDict', 'Task', 'TASK']:\n    if hasattr(benchmark, name):\n        print(f\"   Found: {name}\")\n        obj = getattr(benchmark, name)\n        print(f\"      Type: {type(obj)}\")\n        if isinstance(obj, dict):\n            print(f\"      Keys: {list(obj.keys())}\")\n\n# 3. Try get_benchmark_dict and explore its return value\nprint(\"\\n3. Exploring get_benchmark_dict('libero_spatial'):\")\nresult = benchmark.get_benchmark_dict('libero_spatial')\nprint(f\"   Type: {type(result)}\")\nprint(f\"   Value: {result}\")\n\n# If it's a class/object, explore its methods\nif hasattr(result, '__dict__'):\n    print(f\"   __dict__: {result.__dict__}\")\n\n# 4. Try different API patterns\nprint(\"\\n4. Trying different API patterns:\")\n\n# Pattern A: get_benchmark returns a Benchmark object\nif hasattr(benchmark, 'get_benchmark'):\n    print(\"   Found: get_benchmark()\")\n    try:\n        bench = benchmark.get_benchmark('libero_spatial')\n        print(f\"      Returns: {type(bench)}\")\n        print(f\"      Methods: {[m for m in dir(bench) if not m.startswith('_')]}\")\n    except Exception as e:\n        print(f\"      Error: {e}\")\n\n# Pattern B: Benchmark class that you instantiate\nfor cls_name in ['Benchmark', 'LIBERO', 'LiberoSuite', 'TaskSuite']:\n    if hasattr(benchmark, cls_name):\n        cls = getattr(benchmark, cls_name)\n        print(f\"   Found class: {cls_name}\")\n        try:\n            instance = cls('libero_spatial')\n            print(f\"      Instance methods: {[m for m in dir(instance) if not m.startswith('_')]}\")\n        except Exception as e:\n            print(f\"      Instantiation error: {e}\")\n\n# Pattern C: Direct task access functions\nfor func_name in ['get_task', 'get_tasks', 'load_task', 'get_task_class', 'get_task_names', 'get_task_bddls']:\n    if hasattr(benchmark, func_name):\n        print(f\"   Found: {func_name}()\")\n\n# 5. Check if benchmark itself is callable or iterable\nprint(\"\\n5. Checking benchmark module properties:\")\nprint(f\"   Is callable: {callable(benchmark)}\")\nprint(f\"   Has __iter__: {hasattr(benchmark, '__iter__')}\")\nprint(f\"   Module file: {getattr(benchmark, '__file__', 'N/A')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correct API: benchmark.get_benchmark() returns a CLASS that must be INSTANTIATED\n# BenchmarkClass = benchmark.get_benchmark('libero_spatial')\n# bench_instance = BenchmarkClass()  # <-- Need to instantiate!\n# task = bench_instance.get_task(task_id)\n\ndef get_benchmark_instance(suite_name):\n    \"\"\"Get an instantiated benchmark object for a suite.\"\"\"\n    BenchmarkClass = benchmark.get_benchmark(suite_name)\n    return BenchmarkClass()  # Instantiate the class\n\ndef get_task_names(suite_name):\n    \"\"\"Get task names from a LIBERO suite.\"\"\"\n    bench = get_benchmark_instance(suite_name)\n    return bench.get_task_names()\n\ndef get_task(suite_name, task_id):\n    \"\"\"Get a specific task from a suite.\"\"\"\n    bench = get_benchmark_instance(suite_name)\n    return bench.get_task(task_id)\n\ndef get_num_tasks(suite_name):\n    \"\"\"Get number of tasks in a suite.\"\"\"\n    bench = get_benchmark_instance(suite_name)\n    return bench.get_num_tasks()\n\n# Test the API\nprint(\"Testing LIBERO API:\")\nprint(\"=\"*60)\n\ntask_suites = [\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_90\"]\n\nfor suite_name in task_suites:\n    try:\n        n_tasks = get_num_tasks(suite_name)\n        names = get_task_names(suite_name)\n        print(f\"\\n{suite_name}: {n_tasks} tasks\")\n        for name in names[:3]:\n            print(f\"   - {name}\")\n        if len(names) > 3:\n            print(f\"   ... and {len(names)-3} more\")\n    except Exception as e:\n        print(f\"\\n{suite_name}: Error - {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create a LIBERO Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select a task suite\nSUITE_NAME = \"libero_spatial\"\n\ntask_names = get_task_names(SUITE_NAME)\nn_tasks = get_num_tasks(SUITE_NAME)\n\nprint(f\"Selected suite: {SUITE_NAME}\")\nprint(f\"Number of tasks: {n_tasks}\")\nprint(f\"\\nAll tasks in {SUITE_NAME}:\")\nfor i, name in enumerate(task_names):\n    print(f\"  {i}: {name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select a specific task\nTASK_ID = 0\n\n# Get task using our helper function\ntask = get_task(SUITE_NAME, TASK_ID)\n\nprint(\"Task Configuration:\")\nprint(\"=\"*60)\nprint(f\"Task ID: {TASK_ID}\")\nprint(f\"Name: {task.name}\")\nprint(f\"Language instruction: {task.language}\")\nprint(f\"BDDL file: {task.bddl_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LIBERO requires configuration for asset paths\n# The config file must be in libero/libero/.libero_config (same dir as __init__.py)\n\nimport yaml\nimport libero\nimport libero.libero\n\n# The config file location is where libero.libero module is\nlibero_libero_path = os.path.dirname(libero.libero.__file__)\nlibero_config_path = os.path.join(libero_libero_path, \".libero_config\")\n\nprint(f\"LIBERO libero submodule path: {libero_libero_path}\")\nprint(f\"LIBERO config file: {libero_config_path}\")\n\n# Find the bddl_files directory\nbddl_path = os.path.join(libero_libero_path, \"bddl_files\")\n\nprint(f\"\\nAsset paths:\")\nprint(f\"  bddl_files: {bddl_path} (exists: {os.path.exists(bddl_path)})\")\n\n# Create the config file\nconfig = {\n    'bddl_files': bddl_path,\n}\n\nprint(f\"\\nCreating config file with: {config}\")\n\nwith open(libero_config_path, 'w') as f:\n    yaml.dump(config, f)\n\nprint(f\"✅ Config file created at: {libero_config_path}\")\n\n# Verify content\nwith open(libero_config_path, 'r') as f:\n    print(f\"Config content: {f.read()}\")\n\n# Verify it works\nfrom libero.libero import get_libero_path\ntry:\n    bddl_check = get_libero_path(\"bddl_files\")\n    print(f\"✅ get_libero_path('bddl_files') = {bddl_check}\")\nexcept Exception as e:\n    print(f\"❌ Error: {e}\")"
  },
  {
   "cell_type": "code",
   "source": "# Now create the environment\ndef create_libero_env(task, suite_name, image_size=256):\n    \"\"\"\n    Create a LIBERO environment for a given task.\n    \"\"\"\n    # Get the full path to the BDDL file\n    bench = get_benchmark_instance(suite_name)\n    bddl_file_path = bench.get_task_bddl_file_path(task)\n    \n    print(f\"BDDL file path: {bddl_file_path}\")\n    \n    env_args = {\n        \"bddl_file_name\": bddl_file_path,\n        \"camera_heights\": image_size,\n        \"camera_widths\": image_size,\n    }\n    \n    env = OffScreenRenderEnv(**env_args)\n    env.seed(0)\n    \n    return env\n\nprint(\"Creating environment...\")\nenv = create_libero_env(task, SUITE_NAME)\nprint(\"✅ Environment created!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore environment properties\n",
    "print(\"Environment Properties:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Action space: {env.action_spec}\")\n",
    "print(f\"Action dimension: {env.action_dim}\")\n",
    "print(f\"\\nObservation keys:\")\n",
    "\n",
    "# Reset to get observation structure\n",
    "obs = env.reset()\n",
    "for key, value in obs.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render initial observation\n",
    "def get_observation_image(obs, key='agentview_image'):\n",
    "    \"\"\"\n",
    "    Extract RGB image from observation dict.\n",
    "    \n",
    "    LIBERO convention: Images need to be rotated 180 degrees.\n",
    "    \"\"\"\n",
    "    image = obs[key]\n",
    "    \n",
    "    # LIBERO images are upside down - rotate 180 degrees\n",
    "    image = np.rot90(image, k=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Get and display the initial observation\n",
    "obs = env.reset()\n",
    "image = get_observation_image(obs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Initial Observation\\nTask: {task.language}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute some random actions to see the environment dynamics\n",
    "def run_random_rollout(env, n_steps=10):\n",
    "    \"\"\"\n",
    "    Run a random rollout and collect images.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    images = [get_observation_image(obs)]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Random action\n",
    "        action = np.random.uniform(-0.1, 0.1, size=env.action_dim)\n",
    "        action[-1] = 0  # Keep gripper open\n",
    "        \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        images.append(get_observation_image(obs))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Run random rollout\n",
    "print(\"Running random rollout (10 steps)...\")\n",
    "rollout_images = run_random_rollout(env, n_steps=10)\n",
    "\n",
    "# Display frames\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, (ax, img) in enumerate(zip(axes.flat, rollout_images[:10])):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Step {i}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Random Rollout\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Action Space Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_info = \"\"\"\n",
    "LIBERO Action Space (7-DoF)\n",
    "============================\n",
    "\n",
    "Action dimensions:\n",
    "  [0] dx:      End-effector X displacement\n",
    "  [1] dy:      End-effector Y displacement  \n",
    "  [2] dz:      End-effector Z displacement\n",
    "  [3] d_roll:  Rotation around X axis\n",
    "  [4] d_pitch: Rotation around Y axis\n",
    "  [5] d_yaw:   Rotation around Z axis\n",
    "  [6] gripper: Gripper action (-1: open, 1: close)\n",
    "\n",
    "Action bounds (typical):\n",
    "  Position: [-0.05, 0.05] meters per step\n",
    "  Rotation: [-0.17, 0.17] radians per step  \n",
    "  Gripper:  [-1.0, 1.0]\n",
    "\n",
    "Note: OpenVLA outputs normalized actions in [-1, 1]\n",
    "that need to be scaled to these bounds.\n",
    "\"\"\"\n",
    "print(action_space_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine action bounds\n",
    "print(\"Environment Action Specification:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "action_spec = env.action_spec\n",
    "print(f\"Action dimension: {env.action_dim}\")\n",
    "print(f\"Action spec shape: {action_spec[0].shape}\")\n",
    "\n",
    "# Typical LIBERO action bounds\n",
    "libero_action_bounds = {\n",
    "    'position': (-0.05, 0.05),    # meters\n",
    "    'rotation': (-0.17, 0.17),    # radians (~10 degrees)\n",
    "    'gripper': (-1.0, 1.0),\n",
    "}\n",
    "\n",
    "print(f\"\\nTypical action bounds:\")\n",
    "for dim, bounds in libero_action_bounds.items():\n",
    "    print(f\"  {dim}: [{bounds[0]}, {bounds[1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Image Preprocessing for OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBERO image preprocessing to match OpenVLA training\n",
    "import io\n",
    "\n",
    "def preprocess_libero_image_for_openvla(obs, key='agentview_image', target_size=224):\n",
    "    \"\"\"\n",
    "    Preprocess LIBERO observation image for OpenVLA inference.\n",
    "    \n",
    "    This matches the preprocessing used during OpenVLA training:\n",
    "    1. Rotate 180 degrees (LIBERO convention)\n",
    "    2. Encode as JPEG and decode (matches training augmentation)\n",
    "    3. Resize with Lanczos interpolation\n",
    "    \"\"\"\n",
    "    # Get image from observation\n",
    "    image = obs[key]\n",
    "    \n",
    "    # Rotate 180 degrees (LIBERO images are upside down)\n",
    "    image = np.rot90(image, k=2)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    \n",
    "    # JPEG encode/decode (matches training preprocessing)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format='JPEG', quality=95)\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    \n",
    "    # Resize to target size\n",
    "    pil_image = pil_image.resize((target_size, target_size), Image.LANCZOS)\n",
    "    \n",
    "    return pil_image\n",
    "\n",
    "# Test preprocessing\n",
    "obs = env.reset()\n",
    "processed_image = preprocess_libero_image_for_openvla(obs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(get_observation_image(obs))\n",
    "axes[0].set_title(f\"Raw (256×256)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(processed_image)\n",
    "axes[1].set_title(f\"Preprocessed for OpenVLA (224×224)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Running a Policy Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_rollout(\n",
    "    env,\n",
    "    policy_fn,\n",
    "    instruction,\n",
    "    max_steps=400,\n",
    "    record_video=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a policy rollout in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: LIBERO environment\n",
    "        policy_fn: Function(image, instruction) -> action\n",
    "        instruction: Task instruction string\n",
    "        max_steps: Maximum steps per episode\n",
    "        record_video: Whether to record frames\n",
    "    \n",
    "    Returns:\n",
    "        success: Whether task was completed\n",
    "        frames: List of observation images (if record_video)\n",
    "        actions: List of executed actions\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    \n",
    "    frames = []\n",
    "    actions = []\n",
    "    success = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Get preprocessed image\n",
    "        image = preprocess_libero_image_for_openvla(obs)\n",
    "        \n",
    "        if record_video:\n",
    "            frames.append(get_observation_image(obs))\n",
    "        \n",
    "        # Get action from policy\n",
    "        action = policy_fn(image, instruction)\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Execute action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            success = info.get('success', reward > 0)\n",
    "            break\n",
    "    \n",
    "    if record_video:\n",
    "        frames.append(get_observation_image(obs))\n",
    "    \n",
    "    return success, frames, actions\n",
    "\n",
    "# Example: Run with random policy\n",
    "def random_policy(image, instruction):\n",
    "    \"\"\"Random policy for testing.\"\"\"\n",
    "    action = np.random.uniform(-0.05, 0.05, size=7)\n",
    "    return action\n",
    "\n",
    "print(\"Running random policy rollout...\")\n",
    "success, frames, actions = run_policy_rollout(\n",
    "    env, \n",
    "    random_policy, \n",
    "    task.language,\n",
    "    max_steps=50,\n",
    ")\n",
    "\n",
    "print(f\"\\nRollout completed:\")\n",
    "print(f\"  Success: {success}\")\n",
    "print(f\"  Steps: {len(actions)}\")\n",
    "print(f\"  Frames recorded: {len(frames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Video Recording Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(frames, output_path, fps=20):\n",
    "    \"\"\"\n",
    "    Save frames as MP4 video.\n",
    "    \n",
    "    Args:\n",
    "        frames: List of numpy arrays (H, W, 3)\n",
    "        output_path: Path to save video\n",
    "        fps: Frames per second\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import imageio\n",
    "        \n",
    "        # Ensure frames are uint8\n",
    "        frames_uint8 = [f.astype(np.uint8) for f in frames]\n",
    "        \n",
    "        imageio.mimsave(output_path, frames_uint8, fps=fps)\n",
    "        print(f\"Video saved to: {output_path}\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"imageio not installed. Install with: pip install imageio imageio-ffmpeg\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving video: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the rollout video\n",
    "video_path = \"/tmp/libero_rollout.mp4\"\n",
    "save_video(frames, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display frames as animation (for Jupyter)\n",
    "def display_rollout_frames(frames, step_size=5):\n",
    "    \"\"\"\n",
    "    Display rollout frames in a grid.\n",
    "    \"\"\"\n",
    "    # Sample frames\n",
    "    sampled = frames[::step_size]\n",
    "    n_frames = len(sampled)\n",
    "    n_cols = min(5, n_frames)\n",
    "    n_rows = (n_frames + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "    axes = np.array(axes).flatten()\n",
    "    \n",
    "    for i, (ax, frame) in enumerate(zip(axes, sampled)):\n",
    "        ax.imshow(frame)\n",
    "        ax.set_title(f\"Step {i * step_size}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for ax in axes[len(sampled):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_rollout_frames(frames, step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Multiple Task Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_on_suite(\n    suite_name,\n    policy_fn,\n    n_trials_per_task=20,\n    max_steps=400,\n    verbose=True\n):\n    \"\"\"\n    Evaluate a policy on a full LIBERO task suite.\n    \n    Args:\n        suite_name: Name of task suite (e.g., 'libero_spatial')\n        policy_fn: Policy function(image, instruction) -> action\n        n_trials_per_task: Number of trials per task\n        max_steps: Maximum steps per episode\n        verbose: Print progress\n    \n    Returns:\n        results: Dict mapping task_id -> success_rate\n    \"\"\"\n    task_names_list = get_task_names(suite_name)\n    n_tasks = get_num_tasks(suite_name)\n    results = {}\n    \n    for task_id in range(n_tasks):\n        task_name = task_names_list[task_id]\n        if verbose:\n            print(f\"\\nEvaluating task {task_id}: {task_name}\")\n        \n        # Get task and create environment\n        task = get_task(suite_name, task_id)\n        env = create_libero_env(task, suite_name)\n        \n        successes = 0\n        for trial in range(n_trials_per_task):\n            env.seed(trial)  # Different initial state\n            \n            success, _, _ = run_policy_rollout(\n                env, policy_fn, task.language,\n                max_steps=max_steps,\n                record_video=False\n            )\n            \n            successes += int(success)\n            \n            if verbose and trial % 5 == 0:\n                print(f\"  Trial {trial}/{n_trials_per_task}\")\n        \n        success_rate = successes / n_trials_per_task\n        results[task_id] = success_rate\n        \n        if verbose:\n            print(f\"  Success rate: {success_rate:.1%}\")\n        \n        env.close()\n    \n    # Overall statistics\n    mean_success = np.mean(list(results.values()))\n    if verbose:\n        print(f\"\\nOverall success rate: {mean_success:.1%}\")\n    \n    return results\n\n# Example: Quick evaluation on one task\nprint(\"Evaluation framework ready!\")\nprint(\"Full evaluation will be demonstrated in notebook 08.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### LIBERO Setup Complete\n",
    "\n",
    "1. **Environment**: MuJoCo-based simulation with Franka robot\n",
    "\n",
    "2. **Task Suites**: 4 suites with 90 total tasks\n",
    "   - libero_spatial (10 tasks)\n",
    "   - libero_object (10 tasks)\n",
    "   - libero_goal (10 tasks)\n",
    "   - libero_90 (90 tasks)\n",
    "\n",
    "3. **Observations**: 256×256 RGB agentview images\n",
    "\n",
    "4. **Actions**: 7-DoF (position, rotation, gripper)\n",
    "\n",
    "5. **Preprocessing**: Rotate 180°, JPEG encode/decode, resize to 224×224\n",
    "\n",
    "### For Remote Server\n",
    "- Set `MUJOCO_GL=osmesa` for CPU rendering\n",
    "- Or `MUJOCO_GL=egl` for GPU-accelerated rendering\n",
    "- Install: `sudo apt-get install libosmesa6-dev`\n",
    "\n",
    "### Next Steps\n",
    "→ Continue to **08_integrated_evaluation.ipynb** for full OpenVLA + LIBERO evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"Environment closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}