{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Evaluate Fine-tuned OpenVLA on LIBERO\n",
    "\n",
    "**Goal**: Evaluate a fine-tuned OpenVLA model on LIBERO demonstrations.\n",
    "\n",
    "## What We'll Cover\n",
    "1. Load fine-tuned model (with LoRA weights)\n",
    "2. Run inference on held-out test data\n",
    "3. Compute evaluation metrics\n",
    "4. Visualize predictions vs ground truth\n",
    "5. Compare with base model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Version Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Auto-detect environment\n",
    "if os.environ.get('PSCRATCH'):\n",
    "    SCRATCH = os.environ['PSCRATCH']\n",
    "elif os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']\n",
    "else:\n",
    "    SCRATCH = \"/home/idies/workspace/Temporary/dpark1/scratch\"\n",
    "\n",
    "CACHE_DIR = f\"{SCRATCH}/.cache\"\n",
    "LIBERO_DATA_DIR = f\"{SCRATCH}/libero_data\"\n",
    "FINETUNED_DIR = f\"{SCRATCH}/openvla_finetuned\"\n",
    "\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print(f\"Base directory: {SCRATCH}\")\n",
    "print(f\"Fine-tuned models: {FINETUNED_DIR}\")\n",
    "print(f\"LIBERO data: {LIBERO_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version checks (CRITICAL)\n",
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"tokenizers: {tokenizers.__version__}\")\n",
    "\n",
    "if transformers.__version__ != \"4.40.1\":\n",
    "    print(\"\\n[WARNING] transformers version mismatch!\")\n",
    "    print(\"Fix: pip install transformers==4.40.1 tokenizers==0.19.1\")\n",
    "else:\n",
    "    print(\"\\n[OK] Version check passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Action Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTokenizer:\n",
    "    \"\"\"OpenVLA-compatible action tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000, n_bins=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = np.linspace(-1, 1, n_bins)\n",
    "        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2\n",
    "        self.action_token_start = vocab_size - n_bins\n",
    "    \n",
    "    def encode(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        discretized = np.digitize(action, self.bins)\n",
    "        return self.vocab_size - discretized\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().numpy()\n",
    "        discretized = self.vocab_size - token_ids\n",
    "        indices = np.clip(discretized - 1, 0, len(self.bin_centers) - 1)\n",
    "        return self.bin_centers[indices]\n",
    "\n",
    "action_tokenizer = ActionTokenizer()\n",
    "print(f\"Action tokenizer ready\")\n",
    "print(f\"Token range: [{action_tokenizer.action_token_start}, {action_tokenizer.vocab_size - 1}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available checkpoints\n",
    "finetuned_path = Path(FINETUNED_DIR)\n",
    "\n",
    "if finetuned_path.exists():\n",
    "    runs = sorted(finetuned_path.iterdir())\n",
    "    print(\"Available fine-tuned models:\")\n",
    "    for i, run in enumerate(runs):\n",
    "        if run.is_dir():\n",
    "            checkpoints = list(run.glob(\"checkpoint-*\")) + list(run.glob(\"final\")) + list(run.glob(\"best\"))\n",
    "            print(f\"  [{i}] {run.name}\")\n",
    "            for cp in checkpoints[-3:]:\n",
    "                print(f\"      - {cp.name}\")\n",
    "else:\n",
    "    print(f\"No fine-tuned models found in {FINETUNED_DIR}\")\n",
    "    print(\"Run finetune_openvla_libero.py first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set checkpoint path\n",
    "# UPDATE THIS to your checkpoint path\n",
    "CHECKPOINT_PATH = None  # e.g., f\"{FINETUNED_DIR}/libero_spatial_20241230_120000/best\"\n",
    "\n",
    "# Auto-detect latest checkpoint if not specified\n",
    "if CHECKPOINT_PATH is None and finetuned_path.exists():\n",
    "    runs = sorted(finetuned_path.iterdir())\n",
    "    if runs:\n",
    "        latest_run = runs[-1]\n",
    "        # Prefer 'best', then 'final', then latest checkpoint\n",
    "        if (latest_run / \"best\").exists():\n",
    "            CHECKPOINT_PATH = str(latest_run / \"best\")\n",
    "        elif (latest_run / \"final\").exists():\n",
    "            CHECKPOINT_PATH = str(latest_run / \"final\")\n",
    "        else:\n",
    "            checkpoints = sorted(latest_run.glob(\"checkpoint-*\"))\n",
    "            if checkpoints:\n",
    "                CHECKPOINT_PATH = str(checkpoints[-1])\n",
    "\n",
    "if CHECKPOINT_PATH:\n",
    "    print(f\"Using checkpoint: {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Will evaluate BASE model only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load processor (same for base and fine-tuned)\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "# Load LoRA weights if checkpoint exists\n",
    "if CHECKPOINT_PATH and Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"Loading LoRA weights from {CHECKPOINT_PATH}...\")\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "    print(\"LoRA weights loaded!\")\n",
    "    \n",
    "    # Optionally merge weights for faster inference\n",
    "    # model = model.merge_and_unload()\n",
    "else:\n",
    "    model = base_model\n",
    "    print(\"Using BASE model (no fine-tuning)\")\n",
    "\n",
    "model = model.to(device).eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_libero_files(data_dir, suite_name=\"libero_spatial\"):\n",
    "    \"\"\"Find LIBERO HDF5 files.\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Try multiple patterns\n",
    "    for pattern in [f\"**/*{suite_name}*/*.hdf5\", \"**/*.hdf5\"]:\n",
    "        files = list(data_path.rglob(pattern.replace(\"**/*\", \"*\")))\n",
    "        if files:\n",
    "            return sorted(files)\n",
    "    \n",
    "    return []\n",
    "\n",
    "def load_test_samples(data_dir, suite_name=\"libero_spatial\", n_samples=50, seed=42):\n",
    "    \"\"\"Load a subset of samples for evaluation.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    files = find_libero_files(data_dir, suite_name)\n",
    "    if not files:\n",
    "        print(f\"No files found in {data_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(files)} HDF5 files\")\n",
    "    \n",
    "    all_samples = []\n",
    "    \n",
    "    for filepath in tqdm(files, desc=\"Loading\"):\n",
    "        try:\n",
    "            with h5py.File(filepath, 'r') as f:\n",
    "                # Get language\n",
    "                language = f.attrs.get('language_instruction', f.attrs.get('problem_info', 'unknown'))\n",
    "                if isinstance(language, bytes):\n",
    "                    language = language.decode('utf-8')\n",
    "                \n",
    "                if 'data' not in f:\n",
    "                    continue\n",
    "                \n",
    "                demo_keys = [k for k in f['data'].keys() if k.startswith('demo_')]\n",
    "                \n",
    "                # Sample a few frames from each demo\n",
    "                for demo_key in demo_keys[:5]:  # Limit demos per file\n",
    "                    demo = f['data'][demo_key]\n",
    "                    \n",
    "                    if 'actions' not in demo or 'obs' not in demo:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find image key\n",
    "                    img_key = None\n",
    "                    for k in ['agentview_rgb', 'agentview_image', 'rgb', 'image']:\n",
    "                        if k in demo['obs']:\n",
    "                            img_key = k\n",
    "                            break\n",
    "                    \n",
    "                    if img_key is None:\n",
    "                        continue\n",
    "                    \n",
    "                    n_steps = len(demo['actions'])\n",
    "                    \n",
    "                    # Sample 3 frames per demo (start, middle, end)\n",
    "                    for t in [0, n_steps // 2, n_steps - 1]:\n",
    "                        image = demo['obs'][img_key][t]\n",
    "                        image = np.rot90(image, k=2)  # LIBERO rotation\n",
    "                        \n",
    "                        action = demo['actions'][t]\n",
    "                        if len(action) < 7:\n",
    "                            action = np.pad(action, (0, 7 - len(action)))\n",
    "                        else:\n",
    "                            action = action[:7]\n",
    "                        \n",
    "                        all_samples.append({\n",
    "                            'image': Image.fromarray(image.astype(np.uint8)),\n",
    "                            'action': action.astype(np.float32),\n",
    "                            'instruction': language,\n",
    "                            'source': f\"{filepath.name}/{demo_key}/t{t}\",\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "    \n",
    "    # Randomly sample\n",
    "    if len(all_samples) > n_samples:\n",
    "        indices = np.random.choice(len(all_samples), n_samples, replace=False)\n",
    "        all_samples = [all_samples[i] for i in indices]\n",
    "    \n",
    "    print(f\"Loaded {len(all_samples)} test samples\")\n",
    "    return all_samples\n",
    "\n",
    "# Load test samples\n",
    "SUITE_NAME = \"libero_spatial\"  # Change as needed\n",
    "test_samples = load_test_samples(LIBERO_DATA_DIR, SUITE_NAME, n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_action(model, processor, image, instruction, device):\n",
    "    \"\"\"Run inference to predict action.\"\"\"\n",
    "    prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "    \n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "    \n",
    "    # Add empty token if needed (OpenVLA convention)\n",
    "    if inputs['input_ids'][0, -1] != 29871:\n",
    "        empty_token = torch.tensor([[29871]], device=device)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'], empty_token], dim=1)\n",
    "        if 'attention_mask' in inputs:\n",
    "            inputs['attention_mask'] = torch.cat([\n",
    "                inputs['attention_mask'],\n",
    "                torch.ones((1, 1), device=device, dtype=inputs['attention_mask'].dtype)\n",
    "            ], dim=1)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=7,\n",
    "        do_sample=False,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    action_tokens = outputs[0, -7:]\n",
    "    action = action_tokenizer.decode(action_tokens)\n",
    "    \n",
    "    return action, action_tokens.tolist()\n",
    "\n",
    "print(\"Inference function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = []\n",
    "\n",
    "print(f\"Evaluating {len(test_samples)} samples...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, sample in enumerate(tqdm(test_samples)):\n",
    "    pred_action, pred_tokens = predict_action(\n",
    "        model, processor, sample['image'], sample['instruction'], device\n",
    "    )\n",
    "    \n",
    "    gt_action = sample['action']\n",
    "    \n",
    "    # Normalize GT action to [-1, 1] for fair comparison\n",
    "    gt_action_norm = np.clip(gt_action, -1, 1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    l1_error = np.abs(pred_action - gt_action_norm).mean()\n",
    "    \n",
    "    # Sign accuracy (for significant actions)\n",
    "    significant_mask = np.abs(gt_action_norm) > 0.05\n",
    "    if significant_mask.sum() > 0:\n",
    "        sign_accuracy = (np.sign(pred_action) == np.sign(gt_action_norm))[significant_mask].mean()\n",
    "    else:\n",
    "        sign_accuracy = 1.0\n",
    "    \n",
    "    results.append({\n",
    "        'instruction': sample['instruction'],\n",
    "        'pred_action': pred_action,\n",
    "        'gt_action': gt_action_norm,\n",
    "        'pred_tokens': pred_tokens,\n",
    "        'l1_error': l1_error,\n",
    "        'sign_accuracy': sign_accuracy,\n",
    "    })\n",
    "    \n",
    "    # Print progress every 20 samples\n",
    "    if (i + 1) % 20 == 0:\n",
    "        avg_l1 = np.mean([r['l1_error'] for r in results])\n",
    "        avg_sign = np.mean([r['sign_accuracy'] for r in results])\n",
    "        print(f\"[{i+1}/{len(test_samples)}] L1: {avg_l1:.4f}, Sign: {avg_sign:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "l1_errors = [r['l1_error'] for r in results]\n",
    "sign_accs = [r['sign_accuracy'] for r in results]\n",
    "\n",
    "print(f\"\\nSamples evaluated: {len(results)}\")\n",
    "print(f\"\\nL1 Error:\")\n",
    "print(f\"  Mean: {np.mean(l1_errors):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_errors):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_errors):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_errors):.4f}\")\n",
    "\n",
    "print(f\"\\nSign Accuracy:\")\n",
    "print(f\"  Mean: {np.mean(sign_accs):.3f}\")\n",
    "print(f\"  Std:  {np.std(sign_accs):.3f}\")\n",
    "\n",
    "# Per-dimension analysis\n",
    "print(f\"\\nPer-Dimension L1 Error:\")\n",
    "dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "for dim in range(7):\n",
    "    dim_errors = [np.abs(r['pred_action'][dim] - r['gt_action'][dim]) for r in results]\n",
    "    print(f\"  {dim_names[dim]}: {np.mean(dim_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(sample, result, ax_img, ax_action):\n",
    "    \"\"\"Visualize a single prediction.\"\"\"\n",
    "    # Show image\n",
    "    ax_img.imshow(sample['image'])\n",
    "    ax_img.set_title(f\"L1: {result['l1_error']:.3f}\", fontsize=10)\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Show action comparison\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'grip']\n",
    "    x = np.arange(7)\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax_action.bar(x - width/2, result['gt_action'], width, label='Ground Truth', color='blue', alpha=0.7)\n",
    "    bars2 = ax_action.bar(x + width/2, result['pred_action'], width, label='Predicted', color='orange', alpha=0.7)\n",
    "    \n",
    "    ax_action.set_xticks(x)\n",
    "    ax_action.set_xticklabels(dim_names, fontsize=8)\n",
    "    ax_action.set_ylim(-1.2, 1.2)\n",
    "    ax_action.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax_action.legend(fontsize=7, loc='upper right')\n",
    "\n",
    "# Visualize best and worst predictions\n",
    "sorted_by_error = sorted(range(len(results)), key=lambda i: results[i]['l1_error'])\n",
    "\n",
    "# Best predictions (lowest error)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "fig.suptitle('Best Predictions (Lowest L1 Error)', fontsize=14)\n",
    "\n",
    "for i in range(6):\n",
    "    idx = sorted_by_error[i]\n",
    "    row = i // 2\n",
    "    col = (i % 2) * 2\n",
    "    visualize_prediction(test_samples[idx], results[idx], axes[row, col], axes[row, col+1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst predictions (highest error)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "fig.suptitle('Worst Predictions (Highest L1 Error)', fontsize=14)\n",
    "\n",
    "for i in range(6):\n",
    "    idx = sorted_by_error[-(i+1)]\n",
    "    row = i // 2\n",
    "    col = (i % 2) * 2\n",
    "    visualize_prediction(test_samples[idx], results[idx], axes[row, col], axes[row, col+1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# L1 Error histogram\n",
    "axes[0].hist(l1_errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(l1_errors), color='r', linestyle='--', label=f'Mean: {np.mean(l1_errors):.3f}')\n",
    "axes[0].set_xlabel('L1 Error')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('L1 Error Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sign accuracy histogram\n",
    "axes[1].hist(sign_accs, bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].axvline(np.mean(sign_accs), color='r', linestyle='--', label=f'Mean: {np.mean(sign_accs):.3f}')\n",
    "axes[1].set_xlabel('Sign Accuracy')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Sign Accuracy Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Check Token Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predicted tokens\n",
    "all_tokens = [t for r in results for t in r['pred_tokens']]\n",
    "\n",
    "print(\"Token Statistics:\")\n",
    "print(f\"  Min token: {min(all_tokens)}\")\n",
    "print(f\"  Max token: {max(all_tokens)}\")\n",
    "print(f\"  Expected range: [31744, 31999]\")\n",
    "\n",
    "# Check if all tokens are in valid action range\n",
    "in_range = all(31744 <= t <= 31999 for t in all_tokens)\n",
    "print(f\"\\nAll tokens in action range: {in_range}\")\n",
    "\n",
    "# Unique tokens\n",
    "unique_tokens = set(tuple(r['pred_tokens']) for r in results)\n",
    "print(f\"Unique output patterns: {len(unique_tokens)} / {len(results)}\")\n",
    "\n",
    "if len(unique_tokens) == 1:\n",
    "    print(\"\\n[WARNING] Model produces SAME output for all inputs!\")\n",
    "    print(\"This suggests inference issues.\")\n",
    "elif len(unique_tokens) < len(results) // 2:\n",
    "    print(\"\\n[WARNING] Low output diversity.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Model produces diverse outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\" EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mean_l1 = np.mean(l1_errors)\n",
    "mean_sign = np.mean(sign_accs)\n",
    "\n",
    "print(f\"\\nModel: {'Fine-tuned' if CHECKPOINT_PATH else 'Base'}\")\n",
    "if CHECKPOINT_PATH:\n",
    "    print(f\"Checkpoint: {Path(CHECKPOINT_PATH).name}\")\n",
    "print(f\"Suite: {SUITE_NAME}\")\n",
    "print(f\"Samples: {len(results)}\")\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Mean L1 Error:     {mean_l1:.4f}\")\n",
    "print(f\"  Mean Sign Accuracy: {mean_sign:.3f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if mean_l1 < 0.15:\n",
    "    print(\"  [EXCELLENT] L1 error < 0.15 indicates strong action prediction\")\n",
    "elif mean_l1 < 0.25:\n",
    "    print(\"  [GOOD] L1 error < 0.25 indicates reasonable action prediction\")\n",
    "elif mean_l1 < 0.35:\n",
    "    print(\"  [MODERATE] L1 error < 0.35 suggests model is learning\")\n",
    "else:\n",
    "    print(\"  [NEEDS IMPROVEMENT] L1 error >= 0.35 suggests more training needed\")\n",
    "\n",
    "if mean_sign > 0.8:\n",
    "    print(\"  [EXCELLENT] Sign accuracy > 80% - model predicts correct directions\")\n",
    "elif mean_sign > 0.6:\n",
    "    print(\"  [GOOD] Sign accuracy > 60% - model generally predicts correct directions\")\n",
    "else:\n",
    "    print(\"  [NEEDS IMPROVEMENT] Sign accuracy <= 60% - direction prediction needs work\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save evaluation results\n",
    "results_path = f\"{CACHE_DIR}/evaluation_results.pkl\"\n",
    "\n",
    "save_data = {\n",
    "    'model': 'finetuned' if CHECKPOINT_PATH else 'base',\n",
    "    'checkpoint': CHECKPOINT_PATH,\n",
    "    'suite': SUITE_NAME,\n",
    "    'n_samples': len(results),\n",
    "    'mean_l1_error': mean_l1,\n",
    "    'std_l1_error': np.std(l1_errors),\n",
    "    'mean_sign_accuracy': mean_sign,\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Evaluation Complete\n",
    "\n",
    "This notebook evaluated the fine-tuned OpenVLA model on LIBERO demonstrations.\n",
    "\n",
    "### Key Metrics\n",
    "- **L1 Error**: Mean absolute difference between predicted and ground truth actions\n",
    "- **Sign Accuracy**: Percentage of correct direction predictions for significant actions\n",
    "\n",
    "### Expected Results\n",
    "| Stage | L1 Error | Sign Accuracy |\n",
    "|-------|----------|---------------|\n",
    "| Base model (no fine-tuning) | 0.3-0.5 | 40-60% |\n",
    "| After fine-tuning | 0.1-0.2 | 70-85% |\n",
    "\n",
    "### Next Steps\n",
    "1. If results are poor, try more training epochs\n",
    "2. For real evaluation, run in LIBERO simulation (see notebook 07)\n",
    "3. Compare with paper-reported success rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
