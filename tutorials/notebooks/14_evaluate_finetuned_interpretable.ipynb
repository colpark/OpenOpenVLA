{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVLA Fine-tuned Model Evaluation: Understanding What the Model Learned\n",
    "\n",
    "This notebook helps you understand:\n",
    "1. **What do the metrics mean?** (Loss, L1 Error explained)\n",
    "2. **What does the model actually predict?** (Side-by-side comparison)\n",
    "3. **How accurate is each action dimension?** (Position, rotation, gripper)\n",
    "4. **Is the model making reasonable predictions?** (Visualizations)\n",
    "\n",
    "## Metrics Explained\n",
    "\n",
    "| Metric | What it measures | Good value |\n",
    "|--------|------------------|------------|\n",
    "| **Loss** | How wrong the predicted tokens are (cross-entropy) | < 2.0 |\n",
    "| **L1 Error** | Average absolute difference between predicted and true actions | < 0.15 |\n",
    "| **Gripper Accuracy** | % of times gripper open/close is correct | > 90% |\n",
    "| **Direction Accuracy** | % of times movement direction is correct | > 80% |\n",
    "\n",
    "### What does L1 Error = 0.15 mean?\n",
    "- Actions are in range [-1, 1]\n",
    "- L1 = 0.15 means predictions are off by ~15% on average\n",
    "- For a robot moving 10cm, that's ~1.5cm error per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "if 'SCRATCH' in os.environ:\n",
    "    BASE_DIR = os.environ['SCRATCH']\n",
    "else:\n",
    "    BASE_DIR = \"/home/idies/workspace/Temporary/dpark1/scratch\"\n",
    "\n",
    "CACHE_DIR = f\"{BASE_DIR}/.cache\"\n",
    "LIBERO_DATA_DIR = f\"{BASE_DIR}/libero_data\"\n",
    "CHECKPOINT_DIR = f\"{BASE_DIR}/openvla_finetuned\"\n",
    "\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"LIBERO data directory: {LIBERO_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available checkpoints\n",
    "checkpoint_path = Path(CHECKPOINT_DIR)\n",
    "if checkpoint_path.exists():\n",
    "    runs = sorted(checkpoint_path.glob(\"libero_*\"))\n",
    "    print(\"Available fine-tuning runs:\")\n",
    "    for i, run in enumerate(runs):\n",
    "        print(f\"  [{i}] {run.name}\")\n",
    "        # Check for best/final\n",
    "        if (run / \"best\").exists():\n",
    "            print(f\"      - best/ (recommended)\")\n",
    "        if (run / \"final\").exists():\n",
    "            print(f\"      - final/\")\n",
    "else:\n",
    "    print(f\"No checkpoints found at {CHECKPOINT_DIR}\")\n",
    "    print(\"Please run fine-tuning first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SELECT YOUR CHECKPOINT HERE\n",
    "# ============================================================\n",
    "# Option 1: Use the latest run's best model\n",
    "if runs:\n",
    "    SELECTED_RUN = runs[-1]  # Latest run\n",
    "    if (SELECTED_RUN / \"best\").exists():\n",
    "        LORA_CHECKPOINT = str(SELECTED_RUN / \"best\")\n",
    "    elif (SELECTED_RUN / \"final\").exists():\n",
    "        LORA_CHECKPOINT = str(SELECTED_RUN / \"final\")\n",
    "    else:\n",
    "        # Find latest checkpoint-XXXX\n",
    "        checkpoints = sorted(SELECTED_RUN.glob(\"checkpoint-*\"))\n",
    "        LORA_CHECKPOINT = str(checkpoints[-1]) if checkpoints else None\n",
    "    \n",
    "    print(f\"Selected checkpoint: {LORA_CHECKPOINT}\")\n",
    "else:\n",
    "    LORA_CHECKPOINT = None\n",
    "    print(\"No checkpoint selected. Will evaluate base model only.\")\n",
    "\n",
    "# Option 2: Manual override (uncomment and set path)\n",
    "# LORA_CHECKPOINT = \"/path/to/your/checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models (Base vs Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "print(\"Loading base OpenVLA model...\")\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded. Device: {base_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model (if checkpoint exists)\n",
    "if LORA_CHECKPOINT:\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(f\"Loading LoRA weights from: {LORA_CHECKPOINT}\")\n",
    "    finetuned_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        LORA_CHECKPOINT,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    finetuned_model.eval()\n",
    "    print(\"Fine-tuned model loaded!\")\n",
    "else:\n",
    "    finetuned_model = None\n",
    "    print(\"No fine-tuned model - will compare base model predictions to ground truth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Action Tokenizer (for decoding predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTokenizer:\n",
    "    \"\"\"Decode action tokens back to continuous values.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000, n_bins=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = np.linspace(-1, 1, n_bins)\n",
    "        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2\n",
    "        self.action_token_start = vocab_size - n_bins  # 31744\n",
    "        self.action_token_end = vocab_size - 1  # 31999\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs to continuous actions.\"\"\"\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().numpy()\n",
    "        discretized = self.vocab_size - token_ids\n",
    "        indices = np.clip(discretized - 1, 0, len(self.bin_centers) - 1)\n",
    "        return self.bin_centers[indices]\n",
    "    \n",
    "    def is_action_token(self, token_id):\n",
    "        \"\"\"Check if token is an action token.\"\"\"\n",
    "        return self.action_token_start <= token_id <= self.action_token_end\n",
    "\n",
    "action_tokenizer = ActionTokenizer(vocab_size=len(processor.tokenizer))\n",
    "print(f\"Action tokens range: [{action_tokenizer.action_token_start}, {action_tokenizer.action_token_end}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load LIBERO Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_libero_samples(data_dir, suite=\"libero_spatial\", max_demos_per_task=5, max_steps_per_demo=50):\n    \"\"\"Load samples from LIBERO for evaluation.\n    \n    Args:\n        data_dir: Path to LIBERO data\n        suite: Which suite to load\n        max_demos_per_task: How many demos to use per task (default: 5 = 10% of data)\n        max_steps_per_demo: How many timesteps per demo (default: 50)\n    \n    For libero_spatial with defaults: 10 tasks √ó 5 demos √ó 50 steps = 2,500 samples\n    This is enough for reliable metrics while being fast to evaluate.\n    \"\"\"\n    data_path = Path(data_dir)\n    \n    # Find HDF5 files\n    hdf5_files = sorted(list(data_path.rglob(\"*.hdf5\")))\n    if not hdf5_files:\n        print(f\"No HDF5 files found in {data_dir}\")\n        return [], []\n    \n    print(f\"Found {len(hdf5_files)} task files (HDF5)\")\n    \n    all_samples = []\n    episodes = []  # For trajectory visualization\n    \n    for filepath in tqdm(hdf5_files, desc=\"Loading tasks\"):\n        try:\n            with h5py.File(filepath, 'r') as f:\n                # Get instruction\n                instruction = \"complete the task\"\n                for key in ['language_instruction', 'problem_info', 'language']:\n                    if key in f.attrs:\n                        inst = f.attrs[key]\n                        if isinstance(inst, bytes):\n                            inst = inst.decode('utf-8')\n                        instruction = inst\n                        break\n                \n                if 'data' not in f:\n                    continue\n                \n                demo_keys = sorted([k for k in f['data'].keys() if k.startswith('demo_')])\n                \n                # Use last N demos as \"validation\" (held-out from training)\n                val_demos = demo_keys[-max_demos_per_task:]\n                \n                for demo_key in val_demos:\n                    demo = f['data'][demo_key]\n                    if 'actions' not in demo or 'obs' not in demo:\n                        continue\n                    \n                    # Find image key\n                    img_key = None\n                    for k in ['agentview_rgb', 'agentview_image', 'rgb', 'image']:\n                        if k in demo['obs']:\n                            img_key = k\n                            break\n                    if img_key is None:\n                        continue\n                    \n                    n_steps = len(demo['actions'])\n                    \n                    # Store episode for trajectory viz\n                    episode_data = {\n                        'images': [],\n                        'actions': [],\n                        'instruction': instruction,\n                        'file': filepath.name,\n                        'demo': demo_key,\n                    }\n                    \n                    # Sample timesteps evenly across the episode\n                    step_indices = np.linspace(0, n_steps-1, min(max_steps_per_demo, n_steps), dtype=int)\n                    \n                    for t in step_indices:\n                        image = demo['obs'][img_key][t]\n                        image = np.rot90(image, k=2)  # 180 degree rotation\n                        \n                        action = demo['actions'][t]\n                        if len(action) < 7:\n                            action = np.pad(action, (0, 7 - len(action)))\n                        else:\n                            action = action[:7]\n                        \n                        # Apply official LIBERO transform\n                        action = action.astype(np.float32)\n                        action[:6] = np.clip(action[:6], -1.0, 1.0)\n                        gripper = np.clip(action[6], 0.0, 1.0)\n                        action[6] = 1.0 - gripper  # Invert gripper\n                        \n                        all_samples.append({\n                            'image': image,\n                            'instruction': instruction,\n                            'action': action,\n                            'file': filepath.name,\n                            'demo': demo_key,\n                            'timestep': t,\n                        })\n                        \n                        episode_data['images'].append(image)\n                        episode_data['actions'].append(action)\n                    \n                    if len(episode_data['actions']) > 0:\n                        episode_data['actions'] = np.array(episode_data['actions'])\n                        episodes.append(episode_data)\n                        \n        except Exception as e:\n            print(f\"Error reading {filepath}: {e}\")\n    \n    return all_samples, episodes\n\n# ============================================================\n# CONFIGURE EVALUATION SIZE\n# ============================================================\n# For quick evaluation: max_demos_per_task=2, max_steps_per_demo=20 (~400 samples)\n# For standard evaluation: max_demos_per_task=5, max_steps_per_demo=50 (~2,500 samples)\n# For thorough evaluation: max_demos_per_task=10, max_steps_per_demo=100 (~10,000 samples)\n\nMAX_DEMOS_PER_TASK = 5   # Use last 5 demos per task as validation\nMAX_STEPS_PER_DEMO = 50  # Sample 50 timesteps per demo\n\nprint(\"\\n\" + \"=\"*60)\nprint(\" LOADING LIBERO VALIDATION SAMPLES\")\nprint(\"=\"*60)\nprint(f\"\\nConfiguration:\")\nprint(f\"  Demos per task: {MAX_DEMOS_PER_TASK} (last N demos held out)\")\nprint(f\"  Steps per demo: {MAX_STEPS_PER_DEMO}\")\n\ntest_samples, episodes = load_libero_samples(\n    LIBERO_DATA_DIR, \n    max_demos_per_task=MAX_DEMOS_PER_TASK,\n    max_steps_per_demo=MAX_STEPS_PER_DEMO\n)\n\nprint(f\"\\n‚úÖ Loaded {len(test_samples)} validation samples from {len(episodes)} episodes\")\n\nif test_samples:\n    print(f\"\\nDataset Statistics:\")\n    unique_instructions = set(s['instruction'] for s in test_samples)\n    print(f\"  Unique tasks: {len(unique_instructions)}\")\n    print(f\"  Samples per task: ~{len(test_samples) // len(unique_instructions)}\")\n    print(f\"\\nSample instructions:\")\n    for inst in list(unique_instructions)[:5]:\n        print(f\"  - {inst[:60]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predict_action(model, processor, image, instruction, action_tokenizer):\n    \"\"\"Predict 7-DoF action from image and instruction.\"\"\"\n    # Create prompt\n    prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n    \n    # Convert image to PIL\n    if isinstance(image, np.ndarray):\n        pil_image = Image.fromarray(image.astype(np.uint8))\n    else:\n        pil_image = image\n    \n    # Resize to 224x224\n    if pil_image.size != (224, 224):\n        pil_image = pil_image.resize((224, 224), Image.LANCZOS)\n    \n    # Process inputs\n    inputs = processor(prompt, pil_image, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # CRITICAL: Convert pixel_values to bfloat16 to match model dtype\n    if 'pixel_values' in inputs:\n        inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=7,\n            do_sample=False,\n            pad_token_id=processor.tokenizer.pad_token_id,\n        )\n    \n    # Extract action tokens (last 7)\n    action_tokens = outputs[0, -7:].cpu().numpy()\n    \n    # Decode to continuous actions\n    action = action_tokenizer.decode(action_tokens)\n    \n    return action, action_tokens\n\n# Test prediction\nif test_samples:\n    sample = test_samples[0]\n    model_to_test = finetuned_model if finetuned_model else base_model\n    pred_action, pred_tokens = predict_action(\n        model_to_test, processor, sample['image'], sample['instruction'], action_tokenizer\n    )\n    print(\"Test prediction:\")\n    print(f\"  Instruction: {sample['instruction'][:50]}...\")\n    print(f\"  Ground truth: {sample['action']}\")\n    print(f\"  Prediction:   {pred_action}\")\n    print(f\"  Token IDs:    {pred_tokens}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, samples, action_tokenizer, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model on samples and compute metrics.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name} on {len(samples)} samples...\")\n",
    "    \n",
    "    for sample in tqdm(samples):\n",
    "        try:\n",
    "            pred_action, pred_tokens = predict_action(\n",
    "                model, processor, sample['image'], sample['instruction'], action_tokenizer\n",
    "            )\n",
    "            \n",
    "            gt_action = sample['action']\n",
    "            \n",
    "            results.append({\n",
    "                'gt_action': gt_action,\n",
    "                'pred_action': pred_action,\n",
    "                'instruction': sample['instruction'],\n",
    "                'image': sample['image'],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "if finetuned_model and test_samples:\n",
    "    finetuned_results = evaluate_model(\n",
    "        finetuned_model, processor, test_samples, action_tokenizer, \"Fine-tuned\"\n",
    "    )\n",
    "elif test_samples:\n",
    "    print(\"No fine-tuned model. Evaluating base model...\")\n",
    "    finetuned_results = evaluate_model(\n",
    "        base_model, processor, test_samples, action_tokenizer, \"Base\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Interpretable Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results):\n",
    "    \"\"\"Compute interpretable metrics from evaluation results.\"\"\"\n",
    "    gt_actions = np.array([r['gt_action'] for r in results])\n",
    "    pred_actions = np.array([r['pred_action'] for r in results])\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Overall L1 Error (lower is better)\n",
    "    l1_error = np.abs(pred_actions - gt_actions).mean()\n",
    "    metrics['L1 Error (Overall)'] = l1_error\n",
    "    \n",
    "    # 2. Per-dimension L1 Error\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "    for i, name in enumerate(dim_names):\n",
    "        metrics[f'L1 Error ({name})'] = np.abs(pred_actions[:, i] - gt_actions[:, i]).mean()\n",
    "    \n",
    "    # 3. Position Error (dims 0-2)\n",
    "    pos_error = np.abs(pred_actions[:, :3] - gt_actions[:, :3]).mean()\n",
    "    metrics['Position Error (xyz)'] = pos_error\n",
    "    \n",
    "    # 4. Rotation Error (dims 3-5)\n",
    "    rot_error = np.abs(pred_actions[:, 3:6] - gt_actions[:, 3:6]).mean()\n",
    "    metrics['Rotation Error (rpy)'] = rot_error\n",
    "    \n",
    "    # 5. Gripper Accuracy (is open/close correct?)\n",
    "    # Gripper > 0.5 = open, < 0.5 = close\n",
    "    gt_gripper_open = gt_actions[:, 6] > 0.5\n",
    "    pred_gripper_open = pred_actions[:, 6] > 0.5\n",
    "    gripper_accuracy = (gt_gripper_open == pred_gripper_open).mean() * 100\n",
    "    metrics['Gripper Accuracy (%)'] = gripper_accuracy\n",
    "    \n",
    "    # 6. Direction Accuracy (is movement direction correct?)\n",
    "    # Check if sign matches for position\n",
    "    gt_signs = np.sign(gt_actions[:, :3])\n",
    "    pred_signs = np.sign(pred_actions[:, :3])\n",
    "    # Only count where ground truth is not ~0\n",
    "    significant_movement = np.abs(gt_actions[:, :3]) > 0.05\n",
    "    if significant_movement.sum() > 0:\n",
    "        direction_accuracy = (gt_signs[significant_movement] == pred_signs[significant_movement]).mean() * 100\n",
    "    else:\n",
    "        direction_accuracy = 0\n",
    "    metrics['Direction Accuracy (%)'] = direction_accuracy\n",
    "    \n",
    "    # 7. Action magnitude correlation\n",
    "    gt_magnitude = np.linalg.norm(gt_actions[:, :3], axis=1)\n",
    "    pred_magnitude = np.linalg.norm(pred_actions[:, :3], axis=1)\n",
    "    if gt_magnitude.std() > 0:\n",
    "        correlation = np.corrcoef(gt_magnitude, pred_magnitude)[0, 1]\n",
    "    else:\n",
    "        correlation = 0\n",
    "    metrics['Magnitude Correlation'] = correlation\n",
    "    \n",
    "    return metrics, gt_actions, pred_actions\n",
    "\n",
    "# Compute metrics\n",
    "if finetuned_results:\n",
    "    metrics, gt_actions, pred_actions = compute_metrics(finetuned_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìä OVERALL METRICS:\")\n",
    "    print(f\"  L1 Error:           {metrics['L1 Error (Overall)']:.4f}  (lower is better, target < 0.15)\")\n",
    "    print(f\"  Gripper Accuracy:   {metrics['Gripper Accuracy (%)']:.1f}%  (higher is better, target > 90%)\")\n",
    "    print(f\"  Direction Accuracy: {metrics['Direction Accuracy (%)']:.1f}%  (higher is better, target > 80%)\")\n",
    "    print(f\"  Magnitude Corr:     {metrics['Magnitude Correlation']:.3f}  (higher is better, target > 0.7)\")\n",
    "    \n",
    "    print(\"\\nüìè PER-DIMENSION L1 ERROR:\")\n",
    "    print(f\"  Position (x,y,z):  {metrics['Position Error (xyz)']:.4f}\")\n",
    "    print(f\"  Rotation (r,p,y):  {metrics['Rotation Error (rpy)']:.4f}\")\n",
    "    print(f\"  Gripper:           {metrics['L1 Error (gripper)']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìà INTERPRETATION:\")\n",
    "    if metrics['L1 Error (Overall)'] < 0.15:\n",
    "        print(\"  ‚úÖ L1 Error is GOOD - model is making accurate predictions\")\n",
    "    elif metrics['L1 Error (Overall)'] < 0.25:\n",
    "        print(\"  ‚ö†Ô∏è L1 Error is MODERATE - model is learning but could improve\")\n",
    "    else:\n",
    "        print(\"  ‚ùå L1 Error is HIGH - model needs more training or debugging\")\n",
    "    \n",
    "    if metrics['Gripper Accuracy (%)'] > 90:\n",
    "        print(\"  ‚úÖ Gripper Accuracy is EXCELLENT\")\n",
    "    elif metrics['Gripper Accuracy (%)'] > 70:\n",
    "        print(\"  ‚ö†Ô∏è Gripper Accuracy is OK but could improve\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Gripper Accuracy is LOW - check gripper transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(results, n_samples=6):\n",
    "    \"\"\"Visualize side-by-side predictions vs ground truth.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    indices = np.random.choice(len(results), min(n_samples, len(results)), replace=False)\n",
    "    \n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'grip']\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        result = results[idx]\n",
    "        \n",
    "        # Show image\n",
    "        ax.imshow(result['image'])\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add action comparison as text\n",
    "        gt = result['gt_action']\n",
    "        pred = result['pred_action']\n",
    "        \n",
    "        text = f\"Instruction: {result['instruction'][:30]}...\\n\\n\"\n",
    "        text += \"       GT    Pred   Err\\n\"\n",
    "        for i, name in enumerate(dim_names):\n",
    "            err = abs(gt[i] - pred[i])\n",
    "            color = '‚úì' if err < 0.15 else '‚úó'\n",
    "            text += f\"{name:5}: {gt[i]:+.2f}  {pred[i]:+.2f}  {color}\\n\"\n",
    "        \n",
    "        ax.set_title(text, fontsize=8, family='monospace', loc='left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved to: prediction_visualization.png\")\n",
    "\n",
    "if finetuned_results:\n",
    "    visualize_predictions(finetuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_distribution(gt_actions, pred_actions):\n",
    "    \"\"\"Plot distribution of predicted vs ground truth actions.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes.flatten()[:7], dim_names)):\n",
    "        ax.scatter(gt_actions[:, i], pred_actions[:, i], alpha=0.5, s=20)\n",
    "        ax.plot([-1, 1], [-1, 1], 'r--', label='Perfect')\n",
    "        ax.set_xlabel(f'Ground Truth {name}')\n",
    "        ax.set_ylabel(f'Predicted {name}')\n",
    "        ax.set_title(f'{name}: corr={np.corrcoef(gt_actions[:, i], pred_actions[:, i])[0,1]:.3f}')\n",
    "        ax.set_xlim(-1.1, 1.1)\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide last subplot\n",
    "    axes.flatten()[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Predicted vs Ground Truth Actions (each point = one sample)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('action_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved to: action_distribution.png\")\n",
    "\n",
    "if finetuned_results:\n",
    "    plot_action_distribution(gt_actions, pred_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_histogram(gt_actions, pred_actions):\n",
    "    \"\"\"Plot histogram of prediction errors.\"\"\"\n",
    "    errors = pred_actions - gt_actions\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes.flatten()[:7], dim_names)):\n",
    "        ax.hist(errors[:, i], bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(x=0, color='r', linestyle='--', label='Zero error')\n",
    "        ax.axvline(x=errors[:, i].mean(), color='g', linestyle='-', label=f'Mean: {errors[:, i].mean():.3f}')\n",
    "        ax.set_xlabel(f'Error in {name}')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'{name}: std={errors[:, i].std():.3f}')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    axes.flatten()[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Distribution of Prediction Errors (0 = perfect)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_histogram.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved to: error_histogram.png\")\n",
    "\n",
    "if finetuned_results:\n",
    "    plot_error_histogram(gt_actions, pred_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Base vs Fine-tuned (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare base model vs fine-tuned model on ALL samples\n# NOTE: PEFT modifies base_model in-place, so we need to disable adapters for true comparison\n\nif finetuned_model and len(test_samples) > 0:\n    print(\"=\"*70)\n    print(\" EVALUATING BASE MODEL (LoRA adapters DISABLED) - ALL SAMPLES\")\n    print(\"=\"*70)\n    print(\"\\n‚ö†Ô∏è  IMPORTANT: We disable LoRA adapters to get the TRUE base OpenVLA-7B\")\n    print(\"    performance. This lets us see how much fine-tuning actually helped.\")\n    print(f\"\\nüìä Evaluating on ALL {len(test_samples)} samples for consistent comparison.\\n\")\n    \n    # CRITICAL: Disable LoRA adapters to get true base model behavior\n    finetuned_model.disable_adapter_layers()\n    \n    base_results = evaluate_model(\n        finetuned_model, processor, test_samples, action_tokenizer, \"Base OpenVLA-7B (no LoRA)\"\n    )\n    \n    # Re-enable LoRA adapters for fine-tuned comparison\n    finetuned_model.enable_adapter_layers()\n    \n    # Compute base metrics\n    base_metrics, base_gt, base_pred = compute_metrics(base_results)\n    \n    # Also evaluate fine-tuned on same samples for fair comparison\n    print(\"\\n\" + \"=\"*70)\n    print(\" EVALUATING FINE-TUNED MODEL (LoRA adapters ENABLED) - ALL SAMPLES\")\n    print(\"=\"*70)\n    \n    ft_results_comparison = evaluate_model(\n        finetuned_model, processor, test_samples, action_tokenizer, \"Fine-tuned OpenVLA\"\n    )\n    ft_metrics_comparison, ft_gt, ft_pred = compute_metrics(ft_results_comparison)\n    \n    # Store for trajectory visualization\n    comparison_results = {\n        'base': base_results,\n        'finetuned': ft_results_comparison,\n    }\n    \n    # Display comparison table\n    print(\"\\n\" + \"=\"*70)\n    print(f\" üìä BASE vs FINE-TUNED COMPARISON (on {len(test_samples)} samples)\")\n    print(\"=\"*70)\n    print(\"\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n    print(\"‚îÇ Metric                  ‚îÇ Base Model   ‚îÇ Fine-tuned   ‚îÇ Change       ‚îÇ\")\n    print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n    \n    for key in ['L1 Error (Overall)', 'Gripper Accuracy (%)', 'Direction Accuracy (%)']:\n        base_val = base_metrics[key]\n        ft_val = ft_metrics_comparison[key]\n        \n        if 'Error' in key:\n            improvement = base_val - ft_val  # Lower is better for error\n            sign = \"-\" if improvement > 0 else \"+\"\n            better = \"‚úÖ\" if improvement > 0 else \"‚ùå\"\n        else:\n            improvement = ft_val - base_val  # Higher is better for accuracy\n            sign = \"+\" if improvement > 0 else \"\"\n            better = \"‚úÖ\" if improvement > 0 else \"‚ùå\"\n        \n        print(f\"‚îÇ {key:<23} ‚îÇ {base_val:>10.3f}   ‚îÇ {ft_val:>10.3f}   ‚îÇ {sign}{abs(improvement):>8.3f} {better} ‚îÇ\")\n    \n    print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n    \n    print(\"\\nüìå EXPLANATION:\")\n    print(\"   ‚Ä¢ Base Model     = Pretrained OpenVLA-7B (trained on Open-X Embodiment)\")\n    print(\"   ‚Ä¢ Fine-tuned     = Same model + LoRA adapters trained on YOUR LIBERO data\")\n    print(\"   ‚Ä¢ Change column  = How much fine-tuning improved (‚úÖ) or hurt (‚ùå) the metric\")\n    \n    # Calculate overall improvement\n    l1_improvement = (base_metrics['L1 Error (Overall)'] - ft_metrics_comparison['L1 Error (Overall)']) / base_metrics['L1 Error (Overall)'] * 100\n    grip_improvement = ft_metrics_comparison['Gripper Accuracy (%)'] - base_metrics['Gripper Accuracy (%)']\n    dir_improvement = ft_metrics_comparison['Direction Accuracy (%)'] - base_metrics['Direction Accuracy (%)']\n    \n    print(f\"\\nüéØ IMPROVEMENTS FROM FINE-TUNING:\")\n    print(f\"   ‚Ä¢ L1 Error reduced by:        {l1_improvement:.1f}%\")\n    print(f\"   ‚Ä¢ Gripper Accuracy improved:  {grip_improvement:+.1f}%\")\n    print(f\"   ‚Ä¢ Direction Accuracy improved: {dir_improvement:+.1f}%\")\n    \n    if l1_improvement > 30 and grip_improvement > 5:\n        print(\"\\n‚úÖ Fine-tuning was SUCCESSFUL! The model learned LIBERO-specific actions.\")\n    elif l1_improvement > 10:\n        print(\"\\n‚ö†Ô∏è Fine-tuning showed MODERATE improvement. Consider more epochs or data.\")\n    else:\n        print(\"\\n‚ùå Fine-tuning showed MINIMAL improvement. Check hyperparameters or data quality.\")"
  },
  {
   "cell_type": "code",
   "source": "# Diagnose direction accuracy issue\nif 'ft_gt' in dir() and 'ft_pred' in dir():\n    print(\"=\"*70)\n    print(\" DIAGNOSING DIRECTION ACCURACY\")\n    print(\"=\"*70)\n    \n    # Compare action magnitude distributions\n    print(\"\\nüìä ACTION MAGNITUDE ANALYSIS (Position dims 0-2):\")\n    print(\"\\n  Ground Truth:\")\n    print(f\"    Mean magnitude: {np.abs(ft_gt[:, :3]).mean():.4f}\")\n    print(f\"    Std magnitude:  {np.abs(ft_gt[:, :3]).std():.4f}\")\n    print(f\"    % near zero (|a| < 0.05): {(np.abs(ft_gt[:, :3]) < 0.05).mean()*100:.1f}%\")\n    \n    print(\"\\n  Base Model Predictions:\")\n    print(f\"    Mean magnitude: {np.abs(base_pred[:, :3]).mean():.4f}\")\n    print(f\"    Std magnitude:  {np.abs(base_pred[:, :3]).std():.4f}\")\n    print(f\"    % near zero (|a| < 0.05): {(np.abs(base_pred[:, :3]) < 0.05).mean()*100:.1f}%\")\n    \n    print(\"\\n  Fine-tuned Predictions:\")\n    print(f\"    Mean magnitude: {np.abs(ft_pred[:, :3]).mean():.4f}\")\n    print(f\"    Std magnitude:  {np.abs(ft_pred[:, :3]).std():.4f}\")\n    print(f\"    % near zero (|a| < 0.05): {(np.abs(ft_pred[:, :3]) < 0.05).mean()*100:.1f}%\")\n    \n    # Check for sign inversion pattern\n    print(\"\\nüìä SIGN ANALYSIS (for significant movements |gt| > 0.1):\")\n    significant = np.abs(ft_gt[:, :3]) > 0.1\n    \n    if significant.sum() > 0:\n        gt_signs = np.sign(ft_gt[:, :3][significant])\n        base_signs = np.sign(base_pred[:, :3][significant])\n        ft_signs = np.sign(ft_pred[:, :3][significant])\n        \n        base_match = (gt_signs == base_signs).mean() * 100\n        ft_match = (gt_signs == ft_signs).mean() * 100\n        \n        print(f\"  Samples with |gt| > 0.1: {significant.sum()}\")\n        print(f\"  Base direction accuracy:      {base_match:.1f}%\")\n        print(f\"  Fine-tuned direction accuracy: {ft_match:.1f}%\")\n        \n        # Check if there's systematic sign inversion\n        ft_inverted = (-gt_signs == ft_signs).mean() * 100\n        print(f\"\\n  Fine-tuned INVERTED direction: {ft_inverted:.1f}%\")\n        \n        if ft_inverted > 60:\n            print(\"\\n  ‚ö†Ô∏è POSSIBLE SIGN INVERSION DETECTED!\")\n            print(\"     The model may have learned inverted actions.\")\n            print(\"     Check: Is there a coordinate frame mismatch?\")\n        elif ft_match < 40:\n            print(\"\\n  ‚ö†Ô∏è PREDICTIONS CLUSTER NEAR ZERO\")\n            print(\"     The model predicts small magnitudes, making signs unreliable.\")\n    \n    # Per-dimension analysis\n    print(\"\\nüìä PER-DIMENSION DIRECTION ACCURACY:\")\n    dim_names = ['dx', 'dy', 'dz']\n    for i, name in enumerate(dim_names):\n        sig = np.abs(ft_gt[:, i]) > 0.05\n        if sig.sum() > 10:\n            gt_s = np.sign(ft_gt[:, i][sig])\n            base_s = np.sign(base_pred[:, i][sig])\n            ft_s = np.sign(ft_pred[:, i][sig])\n            \n            base_acc = (gt_s == base_s).mean() * 100\n            ft_acc = (gt_s == ft_s).mean() * 100\n            ft_inv = (-gt_s == ft_s).mean() * 100\n            \n            status = \"‚úÖ\" if ft_acc > base_acc else (\"‚ö†Ô∏è INVERTED\" if ft_inv > 60 else \"‚ùå\")\n            print(f\"  {name}: Base={base_acc:.1f}%, Fine-tuned={ft_acc:.1f}%, Inverted={ft_inv:.1f}% {status}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if finetuned_results:\n",
    "    print(f\"\\nüìä Final Metrics:\")\n",
    "    print(f\"   L1 Error:         {metrics['L1 Error (Overall)']:.4f}\")\n",
    "    print(f\"   Gripper Accuracy: {metrics['Gripper Accuracy (%)']:.1f}%\")\n",
    "    print(f\"   Direction Acc:    {metrics['Direction Accuracy (%)']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Quality Assessment:\")\n",
    "    \n",
    "    score = 0\n",
    "    if metrics['L1 Error (Overall)'] < 0.15:\n",
    "        score += 3\n",
    "        print(\"   ‚úÖ Excellent L1 Error\")\n",
    "    elif metrics['L1 Error (Overall)'] < 0.25:\n",
    "        score += 2\n",
    "        print(\"   ‚ö†Ô∏è Moderate L1 Error - more training may help\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print(\"   ‚ùå High L1 Error - check preprocessing\")\n",
    "    \n",
    "    if metrics['Gripper Accuracy (%)'] > 90:\n",
    "        score += 3\n",
    "        print(\"   ‚úÖ Excellent Gripper Accuracy\")\n",
    "    elif metrics['Gripper Accuracy (%)'] > 70:\n",
    "        score += 2\n",
    "        print(\"   ‚ö†Ô∏è Moderate Gripper Accuracy\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print(\"   ‚ùå Low Gripper Accuracy - check gripper transform\")\n",
    "    \n",
    "    if metrics['Direction Accuracy (%)'] > 80:\n",
    "        score += 3\n",
    "        print(\"   ‚úÖ Excellent Direction Accuracy\")\n",
    "    elif metrics['Direction Accuracy (%)'] > 60:\n",
    "        score += 2\n",
    "        print(\"   ‚ö†Ô∏è Moderate Direction Accuracy\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print(\"   ‚ùå Low Direction Accuracy\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Overall Score: {score}/9\")\n",
    "    if score >= 7:\n",
    "        print(\"   Model is ready for deployment!\")\n",
    "    elif score >= 5:\n",
    "        print(\"   Model is learning. Consider more training epochs.\")\n",
    "    else:\n",
    "        print(\"   Model needs debugging. Check preprocessing and hyperparameters.\")\n",
    "\n",
    "print(\"\\nüìÅ Saved Files:\")\n",
    "print(\"   - prediction_visualization.png\")\n",
    "print(\"   - action_distribution.png\")\n",
    "print(\"   - error_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "def predict_episode_trajectory(model, processor, episode, action_tokenizer):\n    \"\"\"Predict actions for an entire episode.\"\"\"\n    predicted_actions = []\n    \n    for i, image in enumerate(episode['images']):\n        try:\n            pred_action, _ = predict_action(\n                model, processor, image, episode['instruction'], action_tokenizer\n            )\n            predicted_actions.append(pred_action)\n        except Exception as e:\n            # Use zeros if prediction fails\n            predicted_actions.append(np.zeros(7))\n    \n    return np.array(predicted_actions)\n\ndef create_trajectory_animation(episode, gt_actions, pred_actions, save_path=\"trajectory_animation.gif\"):\n    \"\"\"Create animated GIF comparing GT vs predicted trajectories.\"\"\"\n    from matplotlib.animation import FuncAnimation, PillowWriter\n    from mpl_toolkits.mplot3d import Axes3D\n    \n    n_frames = len(episode['images'])\n    \n    # Compute cumulative positions from delta actions\n    gt_positions = np.cumsum(gt_actions[:, :3], axis=0)\n    pred_positions = np.cumsum(pred_actions[:, :3], axis=0)\n    \n    # Create figure with subplots\n    fig = plt.figure(figsize=(16, 8))\n    \n    # Subplot 1: Camera view\n    ax1 = fig.add_subplot(2, 3, 1)\n    ax1.set_title(\"Camera View\")\n    ax1.axis('off')\n    \n    # Subplot 2: 3D trajectory\n    ax2 = fig.add_subplot(2, 3, 2, projection='3d')\n    ax2.set_title(\"3D Trajectory\")\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('Z')\n    \n    # Subplot 3: Position over time\n    ax3 = fig.add_subplot(2, 3, 3)\n    ax3.set_title(\"Position Actions (dx, dy, dz)\")\n    ax3.set_xlabel(\"Timestep\")\n    ax3.set_ylabel(\"Action Value\")\n    \n    # Subplot 4: Rotation over time\n    ax4 = fig.add_subplot(2, 3, 4)\n    ax4.set_title(\"Rotation Actions (rx, ry, rz)\")\n    ax4.set_xlabel(\"Timestep\")\n    ax4.set_ylabel(\"Action Value\")\n    \n    # Subplot 5: Gripper over time\n    ax5 = fig.add_subplot(2, 3, 5)\n    ax5.set_title(\"Gripper Action\")\n    ax5.set_xlabel(\"Timestep\")\n    ax5.set_ylabel(\"Gripper (1=open, 0=close)\")\n    \n    # Subplot 6: Per-step error\n    ax6 = fig.add_subplot(2, 3, 6)\n    ax6.set_title(\"Per-Step L1 Error\")\n    ax6.set_xlabel(\"Timestep\")\n    ax6.set_ylabel(\"L1 Error\")\n    \n    # Pre-compute trajectory bounds\n    all_pos = np.vstack([gt_positions, pred_positions])\n    pos_min, pos_max = all_pos.min(axis=0), all_pos.max(axis=0)\n    margin = (pos_max - pos_min).max() * 0.1 + 0.1\n    \n    # Initialize plots\n    img_display = ax1.imshow(episode['images'][0])\n    \n    # 3D trajectory lines\n    gt_line_3d, = ax2.plot([], [], [], 'b-', linewidth=2, label='Ground Truth')\n    pred_line_3d, = ax2.plot([], [], [], 'r-', linewidth=2, label='Predicted')\n    gt_point_3d, = ax2.plot([], [], [], 'bo', markersize=10)\n    pred_point_3d, = ax2.plot([], [], [], 'ro', markersize=10)\n    ax2.legend(loc='upper left', fontsize=8)\n    \n    # Set 3D axis limits\n    ax2.set_xlim(pos_min[0] - margin, pos_max[0] + margin)\n    ax2.set_ylim(pos_min[1] - margin, pos_max[1] + margin)\n    ax2.set_zlim(pos_min[2] - margin, pos_max[2] + margin)\n    \n    # Time series data\n    timesteps = np.arange(n_frames)\n    per_step_errors = np.abs(pred_actions - gt_actions).mean(axis=1)\n    \n    def init():\n        return []\n    \n    def animate(frame):\n        # Update camera view\n        img_display.set_array(episode['images'][frame])\n        ax1.set_title(f\"Camera View (t={frame}/{n_frames-1})\")\n        \n        # Update 3D trajectory\n        ax2.clear()\n        ax2.set_title(\"3D Trajectory\")\n        ax2.set_xlabel('X')\n        ax2.set_ylabel('Y')\n        ax2.set_zlabel('Z')\n        ax2.set_xlim(pos_min[0] - margin, pos_max[0] + margin)\n        ax2.set_ylim(pos_min[1] - margin, pos_max[1] + margin)\n        ax2.set_zlim(pos_min[2] - margin, pos_max[2] + margin)\n        \n        # Plot trajectories up to current frame\n        if frame > 0:\n            ax2.plot(gt_positions[:frame+1, 0], gt_positions[:frame+1, 1], gt_positions[:frame+1, 2], \n                    'b-', linewidth=2, label='Ground Truth')\n            ax2.plot(pred_positions[:frame+1, 0], pred_positions[:frame+1, 1], pred_positions[:frame+1, 2], \n                    'r-', linewidth=2, label='Predicted')\n        ax2.scatter([gt_positions[frame, 0]], [gt_positions[frame, 1]], [gt_positions[frame, 2]], \n                   c='blue', s=100, marker='o')\n        ax2.scatter([pred_positions[frame, 0]], [pred_positions[frame, 1]], [pred_positions[frame, 2]], \n                   c='red', s=100, marker='o')\n        ax2.legend(loc='upper left', fontsize=8)\n        \n        # Update position plot\n        ax3.clear()\n        ax3.set_title(\"Position Actions (dx, dy, dz)\")\n        ax3.set_xlabel(\"Timestep\")\n        ax3.set_ylabel(\"Action Value\")\n        ax3.set_xlim(0, n_frames)\n        ax3.set_ylim(-1.1, 1.1)\n        for i, (color, label) in enumerate(zip(['r', 'g', 'b'], ['dx', 'dy', 'dz'])):\n            ax3.plot(timesteps[:frame+1], gt_actions[:frame+1, i], f'{color}-', alpha=0.7, label=f'GT {label}')\n            ax3.plot(timesteps[:frame+1], pred_actions[:frame+1, i], f'{color}--', alpha=0.7, label=f'Pred {label}')\n        ax3.axvline(x=frame, color='gray', linestyle=':', alpha=0.5)\n        ax3.legend(loc='upper right', fontsize=6, ncol=2)\n        ax3.grid(True, alpha=0.3)\n        \n        # Update rotation plot\n        ax4.clear()\n        ax4.set_title(\"Rotation Actions (rx, ry, rz)\")\n        ax4.set_xlabel(\"Timestep\")\n        ax4.set_ylabel(\"Action Value\")\n        ax4.set_xlim(0, n_frames)\n        ax4.set_ylim(-1.1, 1.1)\n        for i, (color, label) in enumerate(zip(['r', 'g', 'b'], ['rx', 'ry', 'rz'])):\n            ax4.plot(timesteps[:frame+1], gt_actions[:frame+1, i+3], f'{color}-', alpha=0.7, label=f'GT {label}')\n            ax4.plot(timesteps[:frame+1], pred_actions[:frame+1, i+3], f'{color}--', alpha=0.7, label=f'Pred {label}')\n        ax4.axvline(x=frame, color='gray', linestyle=':', alpha=0.5)\n        ax4.legend(loc='upper right', fontsize=6, ncol=2)\n        ax4.grid(True, alpha=0.3)\n        \n        # Update gripper plot\n        ax5.clear()\n        ax5.set_title(\"Gripper Action (1=open, 0=close)\")\n        ax5.set_xlabel(\"Timestep\")\n        ax5.set_ylabel(\"Gripper Value\")\n        ax5.set_xlim(0, n_frames)\n        ax5.set_ylim(-0.1, 1.1)\n        ax5.plot(timesteps[:frame+1], gt_actions[:frame+1, 6], 'b-', linewidth=2, label='GT Gripper')\n        ax5.plot(timesteps[:frame+1], pred_actions[:frame+1, 6], 'r--', linewidth=2, label='Pred Gripper')\n        ax5.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Threshold')\n        ax5.axvline(x=frame, color='gray', linestyle=':', alpha=0.5)\n        ax5.legend(loc='upper right', fontsize=8)\n        ax5.grid(True, alpha=0.3)\n        \n        # Update error plot\n        ax6.clear()\n        ax6.set_title(f\"Per-Step L1 Error (current: {per_step_errors[frame]:.3f})\")\n        ax6.set_xlabel(\"Timestep\")\n        ax6.set_ylabel(\"L1 Error\")\n        ax6.set_xlim(0, n_frames)\n        ax6.set_ylim(0, per_step_errors.max() * 1.1 + 0.01)\n        ax6.bar(timesteps[:frame+1], per_step_errors[:frame+1], color='purple', alpha=0.7)\n        ax6.axhline(y=per_step_errors.mean(), color='orange', linestyle='--', label=f'Avg: {per_step_errors.mean():.3f}')\n        ax6.axvline(x=frame, color='gray', linestyle=':', alpha=0.5)\n        ax6.legend(loc='upper right', fontsize=8)\n        ax6.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        return []\n    \n    # Create animation\n    anim = FuncAnimation(fig, animate, init_func=init, frames=n_frames, interval=200, blit=False)\n    \n    # Save as GIF\n    print(f\"Saving animation to {save_path}...\")\n    writer = PillowWriter(fps=5)\n    anim.save(save_path, writer=writer)\n    plt.close()\n    \n    print(f\"‚úÖ Animation saved to: {save_path}\")\n    return save_path",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Trajectory Animation: Ground Truth vs Prediction\n\nThis section creates animated visualizations comparing:\n1. **Ground Truth trajectory** (what the robot actually did)\n2. **Predicted trajectory** (what the model thinks should happen)\n\nThe animation shows:\n- Robot camera view (image frames)\n- 3D position trajectory (x, y, z)\n- Action values over time\n- Gripper state changes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display animations inline in the notebook\nfrom IPython.display import Image, display, HTML\n\nif 'animation_files' in dir() and animation_files:\n    print(\"=\"*70)\n    print(\" DISPLAYING TRAJECTORY ANIMATIONS\")\n    print(\"=\"*70)\n    print(\"\\nüé¨ Showing animated comparisons of Ground Truth (blue) vs Predicted (red):\\n\")\n    \n    for i, gif_path in enumerate(animation_files):\n        if os.path.exists(gif_path):\n            print(f\"\\n--- Episode {i+1} ---\")\n            display(Image(filename=gif_path))\n        else:\n            print(f\"Animation file not found: {gif_path}\")\nelse:\n    print(\"No animations to display. Run the previous cell first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate trajectory animations for a few episodes\n# This shows how well the model predicts actions over an entire episode\n\nif finetuned_model and len(episodes) > 0:\n    print(\"=\"*70)\n    print(\" GENERATING TRAJECTORY ANIMATIONS\")\n    print(\"=\"*70)\n    print(\"\\nThis creates animated GIFs comparing Ground Truth vs Predicted trajectories.\")\n    print(\"Each animation shows an entire episode with the model's predictions.\\n\")\n    \n    # Select a few diverse episodes\n    n_animations = min(3, len(episodes))\n    selected_indices = np.linspace(0, len(episodes)-1, n_animations, dtype=int)\n    \n    animation_files = []\n    \n    for i, idx in enumerate(selected_indices):\n        episode = episodes[idx]\n        print(f\"\\n[{i+1}/{n_animations}] Episode: {episode['demo']} from {episode['file']}\")\n        print(f\"    Instruction: {episode['instruction'][:50]}...\")\n        print(f\"    Length: {len(episode['images'])} steps\")\n        \n        # Get ground truth actions\n        gt_actions_ep = episode['actions']\n        \n        # Predict actions for this episode\n        print(f\"    Predicting actions...\")\n        pred_actions_ep = predict_episode_trajectory(\n            finetuned_model, processor, episode, action_tokenizer\n        )\n        \n        # Compute episode metrics\n        episode_l1 = np.abs(pred_actions_ep - gt_actions_ep).mean()\n        gt_gripper = gt_actions_ep[:, 6] > 0.5\n        pred_gripper = pred_actions_ep[:, 6] > 0.5\n        gripper_acc = (gt_gripper == pred_gripper).mean() * 100\n        \n        print(f\"    Episode L1 Error: {episode_l1:.4f}\")\n        print(f\"    Episode Gripper Accuracy: {gripper_acc:.1f}%\")\n        \n        # Create animation\n        save_path = f\"trajectory_episode_{i+1}.gif\"\n        create_trajectory_animation(episode, gt_actions_ep, pred_actions_ep, save_path)\n        animation_files.append(save_path)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\" ANIMATIONS COMPLETE\")\n    print(\"=\"*70)\n    print(\"\\nüìÅ Generated animation files:\")\n    for f in animation_files:\n        print(f\"   - {f}\")\n    print(\"\\nüí° Open these GIF files to see the animated comparison!\")\nelse:\n    print(\"No fine-tuned model or episodes available for animation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}