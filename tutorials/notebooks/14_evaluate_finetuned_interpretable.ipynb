{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVLA Fine-tuned Model Evaluation: Understanding What the Model Learned\n",
    "\n",
    "This notebook helps you understand:\n",
    "1. **What do the metrics mean?** (Loss, L1 Error explained)\n",
    "2. **What does the model actually predict?** (Side-by-side comparison)\n",
    "3. **How accurate is each action dimension?** (Position, rotation, gripper)\n",
    "4. **Is the model making reasonable predictions?** (Visualizations)\n",
    "\n",
    "## Metrics Explained\n",
    "\n",
    "| Metric | What it measures | Good value |\n",
    "|--------|------------------|------------|\n",
    "| **Loss** | How wrong the predicted tokens are (cross-entropy) | < 2.0 |\n",
    "| **L1 Error** | Average absolute difference between predicted and true actions | < 0.15 |\n",
    "| **Gripper Accuracy** | % of times gripper open/close is correct | > 90% |\n",
    "| **Direction Accuracy** | % of times movement direction is correct | > 80% |\n",
    "\n",
    "### What does L1 Error = 0.15 mean?\n",
    "- Actions are in range [-1, 1]\n",
    "- L1 = 0.15 means predictions are off by ~15% on average\n",
    "- For a robot moving 10cm, that's ~1.5cm error per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "if 'SCRATCH' in os.environ:\n",
    "    BASE_DIR = os.environ['SCRATCH']\n",
    "else:\n",
    "    BASE_DIR = \"/home/idies/workspace/Temporary/dpark1/scratch\"\n",
    "\n",
    "CACHE_DIR = f\"{BASE_DIR}/.cache\"\n",
    "LIBERO_DATA_DIR = f\"{BASE_DIR}/libero_data\"\n",
    "CHECKPOINT_DIR = f\"{BASE_DIR}/openvla_finetuned\"\n",
    "\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"LIBERO data directory: {LIBERO_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available checkpoints\n",
    "checkpoint_path = Path(CHECKPOINT_DIR)\n",
    "if checkpoint_path.exists():\n",
    "    runs = sorted(checkpoint_path.glob(\"libero_*\"))\n",
    "    print(\"Available fine-tuning runs:\")\n",
    "    for i, run in enumerate(runs):\n",
    "        print(f\"  [{i}] {run.name}\")\n",
    "        # Check for best/final\n",
    "        if (run / \"best\").exists():\n",
    "            print(f\"      - best/ (recommended)\")\n",
    "        if (run / \"final\").exists():\n",
    "            print(f\"      - final/\")\n",
    "else:\n",
    "    print(f\"No checkpoints found at {CHECKPOINT_DIR}\")\n",
    "    print(\"Please run fine-tuning first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SELECT YOUR CHECKPOINT HERE\n",
    "# ============================================================\n",
    "# Option 1: Use the latest run's best model\n",
    "if runs:\n",
    "    SELECTED_RUN = runs[-1]  # Latest run\n",
    "    if (SELECTED_RUN / \"best\").exists():\n",
    "        LORA_CHECKPOINT = str(SELECTED_RUN / \"best\")\n",
    "    elif (SELECTED_RUN / \"final\").exists():\n",
    "        LORA_CHECKPOINT = str(SELECTED_RUN / \"final\")\n",
    "    else:\n",
    "        # Find latest checkpoint-XXXX\n",
    "        checkpoints = sorted(SELECTED_RUN.glob(\"checkpoint-*\"))\n",
    "        LORA_CHECKPOINT = str(checkpoints[-1]) if checkpoints else None\n",
    "    \n",
    "    print(f\"Selected checkpoint: {LORA_CHECKPOINT}\")\n",
    "else:\n",
    "    LORA_CHECKPOINT = None\n",
    "    print(\"No checkpoint selected. Will evaluate base model only.\")\n",
    "\n",
    "# Option 2: Manual override (uncomment and set path)\n",
    "# LORA_CHECKPOINT = \"/path/to/your/checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models (Base vs Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "print(\"Loading base OpenVLA model...\")\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"{CACHE_DIR}/huggingface\",\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded. Device: {base_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model (if checkpoint exists)\n",
    "if LORA_CHECKPOINT:\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(f\"Loading LoRA weights from: {LORA_CHECKPOINT}\")\n",
    "    finetuned_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        LORA_CHECKPOINT,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    finetuned_model.eval()\n",
    "    print(\"Fine-tuned model loaded!\")\n",
    "else:\n",
    "    finetuned_model = None\n",
    "    print(\"No fine-tuned model - will compare base model predictions to ground truth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Action Tokenizer (for decoding predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTokenizer:\n",
    "    \"\"\"Decode action tokens back to continuous values.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000, n_bins=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = np.linspace(-1, 1, n_bins)\n",
    "        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2\n",
    "        self.action_token_start = vocab_size - n_bins  # 31744\n",
    "        self.action_token_end = vocab_size - 1  # 31999\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs to continuous actions.\"\"\"\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().numpy()\n",
    "        discretized = self.vocab_size - token_ids\n",
    "        indices = np.clip(discretized - 1, 0, len(self.bin_centers) - 1)\n",
    "        return self.bin_centers[indices]\n",
    "    \n",
    "    def is_action_token(self, token_id):\n",
    "        \"\"\"Check if token is an action token.\"\"\"\n",
    "        return self.action_token_start <= token_id <= self.action_token_end\n",
    "\n",
    "action_tokenizer = ActionTokenizer(vocab_size=len(processor.tokenizer))\n",
    "print(f\"Action tokens range: [{action_tokenizer.action_token_start}, {action_tokenizer.action_token_end}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load LIBERO Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_libero_samples(data_dir, suite=\"libero_spatial\", n_samples=50):\n",
    "    \"\"\"Load random samples from LIBERO for evaluation.\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Find HDF5 files\n",
    "    hdf5_files = list(data_path.rglob(\"*.hdf5\"))\n",
    "    if not hdf5_files:\n",
    "        print(f\"No HDF5 files found in {data_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(hdf5_files)} HDF5 files\")\n",
    "    \n",
    "    samples = []\n",
    "    for filepath in hdf5_files:\n",
    "        try:\n",
    "            with h5py.File(filepath, 'r') as f:\n",
    "                # Get instruction\n",
    "                instruction = \"complete the task\"\n",
    "                for key in ['language_instruction', 'problem_info', 'language']:\n",
    "                    if key in f.attrs:\n",
    "                        inst = f.attrs[key]\n",
    "                        if isinstance(inst, bytes):\n",
    "                            inst = inst.decode('utf-8')\n",
    "                        instruction = inst\n",
    "                        break\n",
    "                \n",
    "                if 'data' not in f:\n",
    "                    continue\n",
    "                \n",
    "                demo_keys = [k for k in f['data'].keys() if k.startswith('demo_')]\n",
    "                \n",
    "                for demo_key in demo_keys[:2]:  # 2 demos per file\n",
    "                    demo = f['data'][demo_key]\n",
    "                    if 'actions' not in demo or 'obs' not in demo:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find image key\n",
    "                    img_key = None\n",
    "                    for k in ['agentview_rgb', 'agentview_image', 'rgb', 'image']:\n",
    "                        if k in demo['obs']:\n",
    "                            img_key = k\n",
    "                            break\n",
    "                    if img_key is None:\n",
    "                        continue\n",
    "                    \n",
    "                    n_steps = len(demo['actions'])\n",
    "                    # Sample a few timesteps per demo\n",
    "                    timesteps = np.random.choice(n_steps, min(3, n_steps), replace=False)\n",
    "                    \n",
    "                    for t in timesteps:\n",
    "                        image = demo['obs'][img_key][t]\n",
    "                        image = np.rot90(image, k=2)  # 180 degree rotation\n",
    "                        \n",
    "                        action = demo['actions'][t]\n",
    "                        if len(action) < 7:\n",
    "                            action = np.pad(action, (0, 7 - len(action)))\n",
    "                        else:\n",
    "                            action = action[:7]\n",
    "                        \n",
    "                        # Apply official LIBERO transform\n",
    "                        action = action.astype(np.float32)\n",
    "                        action[:6] = np.clip(action[:6], -1.0, 1.0)\n",
    "                        gripper = np.clip(action[6], 0.0, 1.0)\n",
    "                        action[6] = 1.0 - gripper  # Invert gripper\n",
    "                        \n",
    "                        samples.append({\n",
    "                            'image': image,\n",
    "                            'instruction': instruction,\n",
    "                            'action': action,\n",
    "                            'file': filepath.name,\n",
    "                            'demo': demo_key,\n",
    "                            'timestep': t,\n",
    "                        })\n",
    "                        \n",
    "                        if len(samples) >= n_samples:\n",
    "                            return samples\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load samples\n",
    "print(\"\\nLoading LIBERO samples for evaluation...\")\n",
    "test_samples = load_libero_samples(LIBERO_DATA_DIR, n_samples=100)\n",
    "print(f\"Loaded {len(test_samples)} test samples\")\n",
    "\n",
    "if test_samples:\n",
    "    print(f\"\\nSample instructions:\")\n",
    "    unique_instructions = set(s['instruction'] for s in test_samples)\n",
    "    for inst in list(unique_instructions)[:5]:\n",
    "        print(f\"  - {inst[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(model, processor, image, instruction, action_tokenizer):\n",
    "    \"\"\"Predict 7-DoF action from image and instruction.\"\"\"\n",
    "    # Create prompt\n",
    "    prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "    \n",
    "    # Convert image to PIL\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    else:\n",
    "        pil_image = image\n",
    "    \n",
    "    # Resize to 224x224\n",
    "    if pil_image.size != (224, 224):\n",
    "        pil_image = pil_image.resize((224, 224), Image.LANCZOS)\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(prompt, pil_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=7,\n",
    "            do_sample=False,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Extract action tokens (last 7)\n",
    "    action_tokens = outputs[0, -7:].cpu().numpy()\n",
    "    \n",
    "    # Decode to continuous actions\n",
    "    action = action_tokenizer.decode(action_tokens)\n",
    "    \n",
    "    return action, action_tokens\n",
    "\n",
    "# Test prediction\n",
    "if test_samples:\n",
    "    sample = test_samples[0]\n",
    "    model_to_test = finetuned_model if finetuned_model else base_model\n",
    "    pred_action, pred_tokens = predict_action(\n",
    "        model_to_test, processor, sample['image'], sample['instruction'], action_tokenizer\n",
    "    )\n",
    "    print(\"Test prediction:\")\n",
    "    print(f\"  Instruction: {sample['instruction'][:50]}...\")\n",
    "    print(f\"  Ground truth: {sample['action']}\")\n",
    "    print(f\"  Prediction:   {pred_action}\")\n",
    "    print(f\"  Token IDs:    {pred_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, samples, action_tokenizer, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model on samples and compute metrics.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name} on {len(samples)} samples...\")\n",
    "    \n",
    "    for sample in tqdm(samples):\n",
    "        try:\n",
    "            pred_action, pred_tokens = predict_action(\n",
    "                model, processor, sample['image'], sample['instruction'], action_tokenizer\n",
    "            )\n",
    "            \n",
    "            gt_action = sample['action']\n",
    "            \n",
    "            results.append({\n",
    "                'gt_action': gt_action,\n",
    "                'pred_action': pred_action,\n",
    "                'instruction': sample['instruction'],\n",
    "                'image': sample['image'],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "if finetuned_model and test_samples:\n",
    "    finetuned_results = evaluate_model(\n",
    "        finetuned_model, processor, test_samples, action_tokenizer, \"Fine-tuned\"\n",
    "    )\n",
    "elif test_samples:\n",
    "    print(\"No fine-tuned model. Evaluating base model...\")\n",
    "    finetuned_results = evaluate_model(\n",
    "        base_model, processor, test_samples, action_tokenizer, \"Base\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Interpretable Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results):\n",
    "    \"\"\"Compute interpretable metrics from evaluation results.\"\"\"\n",
    "    gt_actions = np.array([r['gt_action'] for r in results])\n",
    "    pred_actions = np.array([r['pred_action'] for r in results])\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Overall L1 Error (lower is better)\n",
    "    l1_error = np.abs(pred_actions - gt_actions).mean()\n",
    "    metrics['L1 Error (Overall)'] = l1_error\n",
    "    \n",
    "    # 2. Per-dimension L1 Error\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "    for i, name in enumerate(dim_names):\n",
    "        metrics[f'L1 Error ({name})'] = np.abs(pred_actions[:, i] - gt_actions[:, i]).mean()\n",
    "    \n",
    "    # 3. Position Error (dims 0-2)\n",
    "    pos_error = np.abs(pred_actions[:, :3] - gt_actions[:, :3]).mean()\n",
    "    metrics['Position Error (xyz)'] = pos_error\n",
    "    \n",
    "    # 4. Rotation Error (dims 3-5)\n",
    "    rot_error = np.abs(pred_actions[:, 3:6] - gt_actions[:, 3:6]).mean()\n",
    "    metrics['Rotation Error (rpy)'] = rot_error\n",
    "    \n",
    "    # 5. Gripper Accuracy (is open/close correct?)\n",
    "    # Gripper > 0.5 = open, < 0.5 = close\n",
    "    gt_gripper_open = gt_actions[:, 6] > 0.5\n",
    "    pred_gripper_open = pred_actions[:, 6] > 0.5\n",
    "    gripper_accuracy = (gt_gripper_open == pred_gripper_open).mean() * 100\n",
    "    metrics['Gripper Accuracy (%)'] = gripper_accuracy\n",
    "    \n",
    "    # 6. Direction Accuracy (is movement direction correct?)\n",
    "    # Check if sign matches for position\n",
    "    gt_signs = np.sign(gt_actions[:, :3])\n",
    "    pred_signs = np.sign(pred_actions[:, :3])\n",
    "    # Only count where ground truth is not ~0\n",
    "    significant_movement = np.abs(gt_actions[:, :3]) > 0.05\n",
    "    if significant_movement.sum() > 0:\n",
    "        direction_accuracy = (gt_signs[significant_movement] == pred_signs[significant_movement]).mean() * 100\n",
    "    else:\n",
    "        direction_accuracy = 0\n",
    "    metrics['Direction Accuracy (%)'] = direction_accuracy\n",
    "    \n",
    "    # 7. Action magnitude correlation\n",
    "    gt_magnitude = np.linalg.norm(gt_actions[:, :3], axis=1)\n",
    "    pred_magnitude = np.linalg.norm(pred_actions[:, :3], axis=1)\n",
    "    if gt_magnitude.std() > 0:\n",
    "        correlation = np.corrcoef(gt_magnitude, pred_magnitude)[0, 1]\n",
    "    else:\n",
    "        correlation = 0\n",
    "    metrics['Magnitude Correlation'] = correlation\n",
    "    \n",
    "    return metrics, gt_actions, pred_actions\n",
    "\n",
    "# Compute metrics\n",
    "if finetuned_results:\n",
    "    metrics, gt_actions, pred_actions = compute_metrics(finetuned_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìä OVERALL METRICS:\")\n",
    "    print(f\"  L1 Error:           {metrics['L1 Error (Overall)']:.4f}  (lower is better, target < 0.15)\")\n",
    "    print(f\"  Gripper Accuracy:   {metrics['Gripper Accuracy (%)']:.1f}%  (higher is better, target > 90%)\")\n",
    "    print(f\"  Direction Accuracy: {metrics['Direction Accuracy (%)']:.1f}%  (higher is better, target > 80%)\")\n",
    "    print(f\"  Magnitude Corr:     {metrics['Magnitude Correlation']:.3f}  (higher is better, target > 0.7)\")\n",
    "    \n",
    "    print(\"\\nüìè PER-DIMENSION L1 ERROR:\")\n",
    "    print(f\"  Position (x,y,z):  {metrics['Position Error (xyz)']:.4f}\")\n",
    "    print(f\"  Rotation (r,p,y):  {metrics['Rotation Error (rpy)']:.4f}\")\n",
    "    print(f\"  Gripper:           {metrics['L1 Error (gripper)']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìà INTERPRETATION:\")\n",
    "    if metrics['L1 Error (Overall)'] < 0.15:\n",
    "        print(\"  ‚úÖ L1 Error is GOOD - model is making accurate predictions\")\n",
    "    elif metrics['L1 Error (Overall)'] < 0.25:\n",
    "        print(\"  ‚ö†Ô∏è L1 Error is MODERATE - model is learning but could improve\")\n",
    "    else:\n",
    "        print(\"  ‚ùå L1 Error is HIGH - model needs more training or debugging\")\n",
    "    \n",
    "    if metrics['Gripper Accuracy (%)'] > 90:\n",
    "        print(\"  ‚úÖ Gripper Accuracy is EXCELLENT\")\n",
    "    elif metrics['Gripper Accuracy (%)'] > 70:\n",
    "        print(\"  ‚ö†Ô∏è Gripper Accuracy is OK but could improve\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Gripper Accuracy is LOW - check gripper transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(results, n_samples=6):\n",
    "    \"\"\"Visualize side-by-side predictions vs ground truth.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    indices = np.random.choice(len(results), min(n_samples, len(results)), replace=False)\n",
    "    \n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'grip']\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        result = results[idx]\n",
    "        \n",
    "        # Show image\n",
    "        ax.imshow(result['image'])\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add action comparison as text\n",
    "        gt = result['gt_action']\n",
    "        pred = result['pred_action']\n",
    "        \n",
    "        text = f\"Instruction: {result['instruction'][:30]}...\\n\\n\"\n",
    "        text += \"       GT    Pred   Err\\n\"\n",
    "        for i, name in enumerate(dim_names):\n",
    "            err = abs(gt[i] - pred[i])\n",
    "            color = '‚úì' if err < 0.15 else '‚úó'\n",
    "            text += f\"{name:5}: {gt[i]:+.2f}  {pred[i]:+.2f}  {color}\\n\"\n",
    "        \n",
    "        ax.set_title(text, fontsize=8, family='monospace', loc='left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved to: prediction_visualization.png\")\n",
    "\n",
    "if finetuned_results:\n",
    "    visualize_predictions(finetuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_distribution(gt_actions, pred_actions):\n",
    "    \"\"\"Plot distribution of predicted vs ground truth actions.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes.flatten()[:7], dim_names)):\n",
    "        ax.scatter(gt_actions[:, i], pred_actions[:, i], alpha=0.5, s=20)\n",
    "        ax.plot([-1, 1], [-1, 1], 'r--', label='Perfect')\n",
    "        ax.set_xlabel(f'Ground Truth {name}')\n",
    "        ax.set_ylabel(f'Predicted {name}')\n",
    "        ax.set_title(f'{name}: corr={np.corrcoef(gt_actions[:, i], pred_actions[:, i])[0,1]:.3f}')\n",
    "        ax.set_xlim(-1.1, 1.1)\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide last subplot\n",
    "    axes.flatten()[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Predicted vs Ground Truth Actions (each point = one sample)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('action_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved to: action_distribution.png\")\n",
    "\n",
    "if finetuned_results:\n",
    "    plot_action_distribution(gt_actions, pred_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_histogram(gt_actions, pred_actions):\n",
    "    \"\"\"Plot histogram of prediction errors.\"\"\"\n",
    "    errors = pred_actions - gt_actions\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    dim_names = ['dx', 'dy', 'dz', 'rx', 'ry', 'rz', 'gripper']\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes.flatten()[:7], dim_names)):\n",
    "        ax.hist(errors[:, i], bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(x=0, color='r', linestyle='--', label='Zero error')\n",
    "        ax.axvline(x=errors[:, i].mean(), color='g', linestyle='-', label=f'Mean: {errors[:, i].mean():.3f}')\n",
    "        ax.set_xlabel(f'Error in {name}')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'{name}: std={errors[:, i].std():.3f}')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    axes.flatten()[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Distribution of Prediction Errors (0 = perfect)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_histogram.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved to: error_histogram.png\")\n",
    "\n",
    "if finetuned_results:\n",
    "    plot_error_histogram(gt_actions, pred_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Base vs Fine-tuned (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs fine-tuned model\n",
    "if finetuned_model and len(test_samples) > 0:\n",
    "    print(\"Evaluating BASE model for comparison...\")\n",
    "    base_results = evaluate_model(\n",
    "        base_model, processor, test_samples[:30], action_tokenizer, \"Base\"\n",
    "    )\n",
    "    \n",
    "    base_metrics, base_gt, base_pred = compute_metrics(base_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" BASE vs FINE-TUNED COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Metric':<25} {'Base':>12} {'Fine-tuned':>12} {'Improvement':>12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for key in ['L1 Error (Overall)', 'Gripper Accuracy (%)', 'Direction Accuracy (%)']:\n",
    "        base_val = base_metrics[key]\n",
    "        ft_val = metrics[key]\n",
    "        \n",
    "        if 'Error' in key:\n",
    "            improvement = base_val - ft_val  # Lower is better\n",
    "            better = \"‚úÖ\" if improvement > 0 else \"‚ùå\"\n",
    "        else:\n",
    "            improvement = ft_val - base_val  # Higher is better\n",
    "            better = \"‚úÖ\" if improvement > 0 else \"‚ùå\"\n",
    "        \n",
    "        print(f\"{key:<25} {base_val:>12.3f} {ft_val:>12.3f} {improvement:>+10.3f} {better}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if finetuned_results:\n",
    "    print(f\"\\nüìä Final Metrics:\")\n",
    "    print(f\"   L1 Error:         {metrics['L1 Error (Overall)']:.4f}\")\n",
    "    print(f\"   Gripper Accuracy: {metrics['Gripper Accuracy (%)']:.1f}%\")\n",
    "    print(f\"   Direction Acc:    {metrics['Direction Accuracy (%)']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Quality Assessment:\")\n",
    "    \n",
    "    score = 0\n",
    "    if metrics['L1 Error (Overall)'] < 0.15:\n",
    "        score += 3\n",
    "        print(\"   ‚úÖ Excellent L1 Error\")\n",
    "    elif metrics['L1 Error (Overall)'] < 0.25:\n",
    "        score += 2\n",
    "        print(\"   ‚ö†Ô∏è Moderate L1 Error - more training may help\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print(\"   ‚ùå High L1 Error - check preprocessing\")\n",
    "    \n",
    "    if metrics['Gripper Accuracy (%)'] > 90:\n",
    "        score += 3\n",
    "        print(\"   ‚úÖ Excellent Gripper Accuracy\")\n",
    "    elif metrics['Gripper Accuracy (%)'] > 70:\n",
    "        score += 2\n",
    "        print(\"   ‚ö†Ô∏è Moderate Gripper Accuracy\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print(\"   ‚ùå Low Gripper Accuracy - check gripper transform\")\n",
    "    \n",
    "    if metrics['Direction Accuracy (%)'] > 80:\n",
    "        score += 3\n",
    "        print(\"   ‚úÖ Excellent Direction Accuracy\")\n",
    "    elif metrics['Direction Accuracy (%)'] > 60:\n",
    "        score += 2\n",
    "        print(\"   ‚ö†Ô∏è Moderate Direction Accuracy\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print(\"   ‚ùå Low Direction Accuracy\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Overall Score: {score}/9\")\n",
    "    if score >= 7:\n",
    "        print(\"   Model is ready for deployment!\")\n",
    "    elif score >= 5:\n",
    "        print(\"   Model is learning. Consider more training epochs.\")\n",
    "    else:\n",
    "        print(\"   Model needs debugging. Check preprocessing and hyperparameters.\")\n",
    "\n",
    "print(\"\\nüìÅ Saved Files:\")\n",
    "print(\"   - prediction_visualization.png\")\n",
    "print(\"   - action_distribution.png\")\n",
    "print(\"   - error_histogram.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
