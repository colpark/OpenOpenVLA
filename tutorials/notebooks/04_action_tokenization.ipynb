{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Action Tokenization\n",
    "\n",
    "**Goal**: Understand how OpenVLA converts continuous robot actions to discrete tokens.\n",
    "\n",
    "## What We'll Learn\n",
    "1. Why discretize actions?\n",
    "2. The action tokenization process\n",
    "3. Token vocabulary structure\n",
    "4. Encoding and decoding actions\n",
    "5. Quantization resolution and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why Discretize Actions?\n",
    "\n",
    "### The Problem\n",
    "- Robot actions are **continuous**: (0.15, -0.32, 0.78, ...)\n",
    "- LLMs generate **discrete tokens**: vocabulary indices\n",
    "\n",
    "### The Solution\n",
    "OpenVLA treats action prediction as **classification**, not regression:\n",
    "- Divide continuous range into bins (buckets)\n",
    "- Each bin corresponds to a token\n",
    "- LLM generates tokens \u2192 decode to action values\n",
    "\n",
    "### Benefits\n",
    "1. **Unified training**: Same cross-entropy loss for language and actions\n",
    "2. **LLM strengths**: LLMs excel at discrete prediction\n",
    "3. **Multi-modal robustness**: Works across different action spaces\n",
    "4. **Autoregressive generation**: Natural fit with LLM decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL: Set environment variables BEFORE importing packages!\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# Auto-detect environment (NERSC vs SciServer)\n",
    "import os\n",
    "if os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']  # NERSC Perlmutter\n",
    "elif os.environ.get('SCRATCH'):\n",
    "    SCRATCH = os.environ['SCRATCH']  # Generic scratch\n",
    "else:\n",
    "    SCRATCH = \"/home/idies/workspace/Temporary/dpark1/scratch\"  # SciServer default  # CHANGE THIS TO YOUR PATH\n",
    "CACHE_DIR = f\"{SCRATCH}/.cache\"\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = CACHE_DIR\n",
    "os.environ['HF_HOME'] = f\"{CACHE_DIR}/huggingface\"\n",
    "os.environ['TFDS_DATA_DIR'] = f\"{CACHE_DIR}/tensorflow_datasets\"\n",
    "os.environ['TORCH_HOME'] = f\"{CACHE_DIR}/torch\"\n",
    "\n",
    "for path in [CACHE_DIR, os.environ['HF_HOME'], os.environ['TFDS_DATA_DIR'], os.environ['TORCH_HOME']]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"\u2705 All caches \u2192 {CACHE_DIR}\")\n",
    "\n",
    "# Now import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the discretization concept\n",
    "def visualize_discretization(n_bins=256):\n",
    "    \"\"\"Visualize how continuous values map to discrete bins.\"\"\"\n",
    "    \n",
    "    # Create bin edges and centers\n",
    "    bin_edges = np.linspace(-1, 1, n_bins + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Plot 1: Continuous to discrete mapping\n",
    "    ax1 = axes[0]\n",
    "    continuous_values = np.linspace(-1, 1, 1000)\n",
    "    discrete_bins = np.digitize(continuous_values, bin_edges) - 1\n",
    "    discrete_bins = np.clip(discrete_bins, 0, n_bins - 1)\n",
    "    reconstructed = bin_centers[discrete_bins]\n",
    "    \n",
    "    ax1.plot(continuous_values, continuous_values, 'b-', label='Original', alpha=0.7)\n",
    "    ax1.plot(continuous_values, reconstructed, 'r-', label='After discretization', alpha=0.7)\n",
    "    ax1.set_xlabel('Original Value')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.set_title(f'Continuous \u2192 Discrete \u2192 Continuous ({n_bins} bins)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Quantization error\n",
    "    ax2 = axes[1]\n",
    "    error = np.abs(continuous_values - reconstructed)\n",
    "    ax2.plot(continuous_values, error * 1000, 'g-', alpha=0.7)\n",
    "    ax2.set_xlabel('Original Value')\n",
    "    ax2.set_ylabel('Error (\u00d710\u207b\u00b3)')\n",
    "    ax2.set_title(f'Quantization Error (max: {error.max():.6f}, mean: {error.mean():.6f})')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nQuantization Statistics:\")\n",
    "    print(f\"  Number of bins: {n_bins}\")\n",
    "    print(f\"  Bin width: {2/n_bins:.6f}\")\n",
    "    print(f\"  Max error: {2/(2*n_bins):.6f} (half bin width)\")\n",
    "    print(f\"  Resolution: {1/n_bins:.4f} of full range\")\n",
    "\n",
    "visualize_discretization(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. OpenVLA's Action Tokenizer\n",
    "\n",
    "Let's look at the actual implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the action tokenizer source code\n",
    "import os\n",
    "\n",
    "# Path to action tokenizer in the repository\n",
    "REPO_ROOT = \"/Users/davidpark/Documents/Claude/openvla\"\n",
    "tokenizer_path = os.path.join(REPO_ROOT, \"prismatic/vla/action_tokenizer.py\")\n",
    "\n",
    "print(\"ActionTokenizer Source Code Location:\")\n",
    "print(f\"  {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simplified action tokenizer for understanding\n",
    "class SimpleActionTokenizer:\n",
    "    \"\"\"Simplified action tokenizer for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bins=256, vocab_size=32000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_bins: Number of discrete bins per action dimension\n",
    "            vocab_size: Total vocabulary size of the LLM\n",
    "        \"\"\"\n",
    "        self.n_bins = n_bins\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Action tokens occupy the LAST n_bins positions in vocabulary\n",
    "        # This avoids collision with text tokens\n",
    "        self.action_token_start = vocab_size - n_bins\n",
    "        \n",
    "        # Create bin edges for discretization\n",
    "        self.bin_edges = np.linspace(-1, 1, n_bins + 1)\n",
    "        self.bin_centers = (self.bin_edges[:-1] + self.bin_edges[1:]) / 2\n",
    "    \n",
    "    def encode(self, continuous_action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert continuous action to token IDs.\n",
    "        \n",
    "        Args:\n",
    "            continuous_action: Array of shape (action_dim,) with values in [-1, 1]\n",
    "            \n",
    "        Returns:\n",
    "            Array of token IDs, one per action dimension\n",
    "        \"\"\"\n",
    "        # Clip to valid range\n",
    "        clipped = np.clip(continuous_action, -1, 1)\n",
    "        \n",
    "        # Find bin indices (0 to n_bins-1)\n",
    "        bin_indices = np.digitize(clipped, self.bin_edges) - 1\n",
    "        bin_indices = np.clip(bin_indices, 0, self.n_bins - 1)\n",
    "        \n",
    "        # Convert to vocabulary token IDs\n",
    "        token_ids = bin_indices + self.action_token_start\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert token IDs back to continuous actions.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Array of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            Array of continuous action values in [-1, 1]\n",
    "        \"\"\"\n",
    "        # Convert token IDs to bin indices\n",
    "        bin_indices = token_ids - self.action_token_start\n",
    "        \n",
    "        # Map to bin centers\n",
    "        continuous_actions = self.bin_centers[bin_indices]\n",
    "        \n",
    "        return continuous_actions\n",
    "\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleActionTokenizer(n_bins=256, vocab_size=32000)\n",
    "\n",
    "print(\"SimpleActionTokenizer Configuration:\")\n",
    "print(f\"  Total vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Number of action bins: {tokenizer.n_bins}\")\n",
    "print(f\"  Action tokens range: {tokenizer.action_token_start} to {tokenizer.vocab_size - 1}\")\n",
    "print(f\"  Bin width: {2/tokenizer.n_bins:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding and decoding\n",
    "print(\"\\nEncoding/Decoding Examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample continuous actions\n",
    "sample_actions = np.array([\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],       # Zero action\n",
    "    [0.5, -0.5, 0.25, -0.25, 0.1, -0.1, 1.0],  # Mixed action\n",
    "    [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0],    # Extreme action\n",
    "])\n",
    "\n",
    "for i, action in enumerate(sample_actions):\n",
    "    # Encode\n",
    "    tokens = tokenizer.encode(action)\n",
    "    # Decode\n",
    "    reconstructed = tokenizer.decode(tokens)\n",
    "    # Error\n",
    "    error = np.abs(action - reconstructed)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original:      {action}\")\n",
    "    print(f\"  Token IDs:     {tokens}\")\n",
    "    print(f\"  Reconstructed: {reconstructed}\")\n",
    "    print(f\"  Max error:     {error.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Token Vocabulary Structure\n",
    "\n",
    "OpenVLA extends the LLM vocabulary to include action tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_structure = \"\"\"\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    OpenVLA Vocabulary Structure                     \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  Token ID: 0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 32000     \u2502\n",
    "\u2502            \u2502                                                \u2502      \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502      TEXT TOKENS (0 - 31743)         \u2502  ACTION TOKENS        \u2502  \u2502\n",
    "\u2502  \u2502                                       \u2502  (31744 - 31999)      \u2502  \u2502\n",
    "\u2502  \u2502  \u2022 BOS, EOS, PAD tokens              \u2502                       \u2502  \u2502\n",
    "\u2502  \u2502  \u2022 Language vocabulary               \u2502  \u2022 256 bins           \u2502  \u2502\n",
    "\u2502  \u2502  \u2022 Special tokens                     \u2502  \u2022 One per action val \u2502  \u2502\n",
    "\u2502  \u2502                                       \u2502                       \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  For 7-DoF robot action:                                            \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 [x_token] [y_token] [z_token] [rx_token] [ry_token]        \u2502    \u2502\n",
    "\u2502  \u2502 [rz_token] [gripper_token]                                  \u2502    \u2502\n",
    "\u2502  \u2502                                                             \u2502    \u2502\n",
    "\u2502  \u2502 Each token ID = action_token_start + bin_index              \u2502    \u2502\n",
    "\u2502  \u2502 Example: x=0.5 \u2192 bin 192 \u2192 token 31936                     \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\"\"\"\n",
    "print(vocab_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load actual OpenVLA tokenizer\n# Note: Environment variables were set in cell-2 above\nimport torch\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\n\nprint(\"Loading OpenVLA processor (this may take a while on first run)...\")\nprocessor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\ntokenizer_hf = processor.tokenizer\n\nprint(\"OpenVLA Tokenizer Info:\")\nprint(f\"  Vocabulary size: {tokenizer_hf.vocab_size}\")\nprint(f\"  BOS token: {tokenizer_hf.bos_token} (ID: {tokenizer_hf.bos_token_id})\")\nprint(f\"  EOS token: {tokenizer_hf.eos_token} (ID: {tokenizer_hf.eos_token_id})\")\nprint(f\"  PAD token: {tokenizer_hf.pad_token} (ID: {tokenizer_hf.pad_token_id})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the action token range\n",
    "n_bins = 256\n",
    "vocab_size = tokenizer_hf.vocab_size\n",
    "action_start = vocab_size - n_bins\n",
    "\n",
    "print(f\"\\nAction Token Configuration:\")\n",
    "print(f\"  Total vocab: {vocab_size}\")\n",
    "print(f\"  Action bins: {n_bins}\")\n",
    "print(f\"  Action token range: [{action_start}, {vocab_size - 1}]\")\n",
    "print(f\"\\n  Bin 0 (value \u2248 -1.0) \u2192 Token {action_start}\")\n",
    "print(f\"  Bin 127 (value \u2248 0.0) \u2192 Token {action_start + 127}\")\n",
    "print(f\"  Bin 255 (value \u2248 1.0) \u2192 Token {vocab_size - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Full Encoding/Decoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete action tokenization pipeline\n",
    "class OpenVLAActionTokenizer:\n",
    "    \"\"\"\n",
    "    Full-featured action tokenizer matching OpenVLA implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, n_bins=256, action_dim=7):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_bins = n_bins\n",
    "        self.action_dim = action_dim\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        \n",
    "        # Action tokens occupy last n_bins positions\n",
    "        self.action_token_start = self.vocab_size - n_bins\n",
    "        \n",
    "        # Create discretization bins\n",
    "        self.bins = np.linspace(-1, 1, n_bins + 1)\n",
    "        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2\n",
    "    \n",
    "    def action_to_tokens(self, action: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Convert continuous action to token string.\n",
    "        \n",
    "        This is what gets appended to the prompt during training.\n",
    "        \"\"\"\n",
    "        # Clip to [-1, 1]\n",
    "        action = np.clip(action, -1, 1)\n",
    "        \n",
    "        # Discretize each dimension\n",
    "        bin_indices = np.digitize(action, self.bins) - 1\n",
    "        bin_indices = np.clip(bin_indices, 0, self.n_bins - 1)\n",
    "        \n",
    "        # Convert to token IDs\n",
    "        token_ids = bin_indices + self.action_token_start\n",
    "        \n",
    "        # Decode token IDs to string\n",
    "        action_string = self.tokenizer.decode(token_ids)\n",
    "        \n",
    "        return action_string, token_ids\n",
    "    \n",
    "    def tokens_to_action(self, token_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert token IDs back to continuous action.\n",
    "        \n",
    "        This is used during inference to decode model output.\n",
    "        \"\"\"\n",
    "        # Validate token IDs are in action range\n",
    "        assert np.all(token_ids >= self.action_token_start), \"Invalid action token\"\n",
    "        assert np.all(token_ids < self.vocab_size), \"Invalid action token\"\n",
    "        \n",
    "        # Convert to bin indices\n",
    "        bin_indices = token_ids - self.action_token_start\n",
    "        \n",
    "        # Map to continuous values\n",
    "        action = self.bin_centers[bin_indices]\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "# Create the tokenizer\n",
    "action_tokenizer = OpenVLAActionTokenizer(tokenizer_hf)\n",
    "\n",
    "print(\"OpenVLA Action Tokenizer:\")\n",
    "print(f\"  Action dimensions: {action_tokenizer.action_dim}\")\n",
    "print(f\"  Bins per dimension: {action_tokenizer.n_bins}\")\n",
    "print(f\"  Action token range: [{action_tokenizer.action_token_start}, {action_tokenizer.vocab_size-1}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full pipeline\n",
    "print(\"\\nFull Encoding/Decoding Test:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample robot action\n",
    "sample_action = np.array([0.15, -0.32, 0.78, 0.0, 0.1, -0.05, 0.8])\n",
    "\n",
    "print(f\"Original action: {sample_action}\")\n",
    "print(f\"  x (position): {sample_action[0]:.4f}\")\n",
    "print(f\"  y (position): {sample_action[1]:.4f}\")\n",
    "print(f\"  z (position): {sample_action[2]:.4f}\")\n",
    "print(f\"  roll:         {sample_action[3]:.4f}\")\n",
    "print(f\"  pitch:        {sample_action[4]:.4f}\")\n",
    "print(f\"  yaw:          {sample_action[5]:.4f}\")\n",
    "print(f\"  gripper:      {sample_action[6]:.4f}\")\n",
    "\n",
    "# Encode\n",
    "action_string, token_ids = action_tokenizer.action_to_tokens(sample_action)\n",
    "print(f\"\\nEncoded token IDs: {token_ids}\")\n",
    "print(f\"As string: '{action_string}'\")\n",
    "\n",
    "# Decode\n",
    "reconstructed = action_tokenizer.tokens_to_action(token_ids)\n",
    "print(f\"\\nReconstructed: {reconstructed}\")\n",
    "\n",
    "# Error analysis\n",
    "error = np.abs(sample_action - reconstructed)\n",
    "print(f\"\\nQuantization error per dimension:\")\n",
    "for i, (orig, recon, err) in enumerate(zip(sample_action, reconstructed, error)):\n",
    "    dim_names = ['x', 'y', 'z', 'roll', 'pitch', 'yaw', 'gripper']\n",
    "    print(f\"  {dim_names[i]:8s}: {orig:+.4f} \u2192 {recon:+.4f}  (error: {err:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Quantization Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze quantization resolution for different bin counts\n",
    "def analyze_resolution(n_bins_list=[64, 128, 256, 512, 1024]):\n",
    "    \"\"\"Compare quantization resolution for different bin counts.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for n_bins in n_bins_list:\n",
    "        bin_width = 2.0 / n_bins\n",
    "        max_error = bin_width / 2\n",
    "        \n",
    "        # In robot context: if action range is \u00b110cm, what's the error?\n",
    "        range_cm = 10  # \u00b110cm typical\n",
    "        error_mm = max_error * range_cm * 10  # Convert to mm\n",
    "        \n",
    "        results.append({\n",
    "            'n_bins': n_bins,\n",
    "            'bin_width': bin_width,\n",
    "            'max_error': max_error,\n",
    "            'error_mm': error_mm\n",
    "        })\n",
    "    \n",
    "    print(\"Quantization Resolution Analysis:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Bins':>8} {'Bin Width':>12} {'Max Error':>12} {'Error (mm)*':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    for r in results:\n",
    "        print(f\"{r['n_bins']:>8} {r['bin_width']:>12.6f} {r['max_error']:>12.6f} {r['error_mm']:>12.3f}\")\n",
    "    \n",
    "    print(\"\\n* Assuming \u00b110cm action range\")\n",
    "    print(\"\\nOpenVLA uses 256 bins: ~0.39mm error for typical robot manipulation\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = analyze_resolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize resolution comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "n_bins_list = [r['n_bins'] for r in results]\n",
    "errors_mm = [r['error_mm'] for r in results]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(n_bins_list)), errors_mm, color='steelblue')\n",
    "plt.xticks(range(len(n_bins_list)), n_bins_list)\n",
    "plt.xlabel('Number of Bins')\n",
    "plt.ylabel('Max Error (mm)')\n",
    "plt.title('Quantization Error vs Bin Count')\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label='1mm threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show what 256 bins looks like for different action ranges\n",
    "action_ranges = [5, 10, 20, 50, 100]  # cm\n",
    "errors = [r['max_error'] * ar * 10 for ar in action_ranges for r in results if r['n_bins'] == 256]\n",
    "errors = [results[2]['max_error'] * ar * 10 for ar in action_ranges]  # 256 bins\n",
    "\n",
    "plt.bar(range(len(action_ranges)), errors, color='coral')\n",
    "plt.xticks(range(len(action_ranges)), [f'\u00b1{ar}cm' for ar in action_ranges])\n",
    "plt.xlabel('Action Range')\n",
    "plt.ylabel('Max Error (mm)')\n",
    "plt.title('Error vs Action Range (256 bins)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Action Normalization\n",
    "\n",
    "Before tokenization, actions are normalized to [-1, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action normalization strategies\n",
    "class ActionNormalizer:\n",
    "    \"\"\"\n",
    "    Normalize robot actions to [-1, 1] range for tokenization.\n",
    "    \n",
    "    OpenVLA supports two strategies:\n",
    "    1. Bounds-based: min/max clipping\n",
    "    2. Statistics-based: mean/std normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stats: dict, strategy='bounds'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stats: Dictionary with normalization statistics\n",
    "            strategy: 'bounds' or 'normal'\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        if strategy == 'bounds':\n",
    "            self.min_action = np.array(stats['min'])\n",
    "            self.max_action = np.array(stats['max'])\n",
    "        else:  # normal\n",
    "            self.mean = np.array(stats['mean'])\n",
    "            self.std = np.array(stats['std'])\n",
    "    \n",
    "    def normalize(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize action to [-1, 1] range.\"\"\"\n",
    "        if self.strategy == 'bounds':\n",
    "            # Scale from [min, max] to [-1, 1]\n",
    "            normalized = 2 * (action - self.min_action) / (self.max_action - self.min_action) - 1\n",
    "        else:\n",
    "            # Normalize using mean/std, then clip\n",
    "            normalized = (action - self.mean) / (self.std + 1e-8)\n",
    "            normalized = np.clip(normalized, -1, 1)\n",
    "        \n",
    "        return np.clip(normalized, -1, 1)\n",
    "    \n",
    "    def denormalize(self, normalized_action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert normalized action back to original scale.\"\"\"\n",
    "        if self.strategy == 'bounds':\n",
    "            action = (normalized_action + 1) / 2 * (self.max_action - self.min_action) + self.min_action\n",
    "        else:\n",
    "            action = normalized_action * self.std + self.mean\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "# Example: LIBERO normalization statistics\n",
    "libero_stats = {\n",
    "    'bounds': {\n",
    "        'min': [-0.1, -0.1, -0.1, -0.5, -0.5, -0.5, 0.0],\n",
    "        'max': [0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 1.0]\n",
    "    },\n",
    "    'description': 'LIBERO uses delta position/rotation actions'\n",
    "}\n",
    "\n",
    "normalizer = ActionNormalizer(libero_stats['bounds'], strategy='bounds')\n",
    "\n",
    "# Test normalization\n",
    "raw_action = np.array([0.05, -0.03, 0.02, 0.1, -0.2, 0.15, 0.8])\n",
    "normalized = normalizer.normalize(raw_action)\n",
    "denormalized = normalizer.denormalize(normalized)\n",
    "\n",
    "print(\"Action Normalization Example:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Raw action:        {raw_action}\")\n",
    "print(f\"Normalized [-1,1]: {normalized}\")\n",
    "print(f\"Denormalized:      {denormalized}\")\n",
    "print(f\"Reconstruction OK: {np.allclose(raw_action, denormalized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Complete Action Generation Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_flow = \"\"\"\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502               Complete Action Generation Pipeline                   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  TRAINING:                                                          \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                                          \u2502\n",
    "\u2502  Raw Action        \u2192  Normalize   \u2192  Discretize  \u2192  Token IDs      \u2502\n",
    "\u2502  [0.05, -0.03, ..]    [-1, 1]        [0, 255]       [31744+bin]    \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502 Prompt: \"Pick up red block\\n\" + action_tokens                 \u2502  \u2502\n",
    "\u2502  \u2502 Loss: Cross-entropy on action tokens only                     \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  INFERENCE:                                                         \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                                         \u2502\n",
    "\u2502  Token IDs     \u2192  Undiscretize  \u2192  Denormalize  \u2192  Raw Action      \u2502\n",
    "\u2502  [31872, ...]     [-1, 1]          Original        [0.05, ...]     \u2502\n",
    "\u2502                                       scale                         \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502 1. LLM autoregressively generates 7 action tokens            \u2502  \u2502\n",
    "\u2502  \u2502 2. Each token decoded to bin center value                     \u2502  \u2502\n",
    "\u2502  \u2502 3. Values denormalized using dataset statistics               \u2502  \u2502\n",
    "\u2502  \u2502 4. Final action sent to robot controller                      \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\"\"\"\n",
    "print(action_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the complete pipeline\n",
    "def simulate_action_pipeline(\n",
    "    raw_action: np.ndarray,\n",
    "    normalizer: ActionNormalizer,\n",
    "    tokenizer: OpenVLAActionTokenizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate full action encoding/decoding pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Action Pipeline Simulation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Start with raw action\n",
    "    print(f\"\\n1. Raw action (robot units):\")\n",
    "    print(f\"   {raw_action}\")\n",
    "    \n",
    "    # Step 2: Normalize\n",
    "    normalized = normalizer.normalize(raw_action)\n",
    "    print(f\"\\n2. Normalized to [-1, 1]:\")\n",
    "    print(f\"   {normalized}\")\n",
    "    \n",
    "    # Step 3: Tokenize\n",
    "    action_string, token_ids = tokenizer.action_to_tokens(normalized)\n",
    "    print(f\"\\n3. Tokenized (token IDs):\")\n",
    "    print(f\"   {token_ids}\")\n",
    "    print(f\"   As string: '{action_string}'\")\n",
    "    \n",
    "    # Step 4: Decode tokens (simulating LLM output)\n",
    "    decoded_normalized = tokenizer.tokens_to_action(token_ids)\n",
    "    print(f\"\\n4. Decoded from tokens:\")\n",
    "    print(f\"   {decoded_normalized}\")\n",
    "    \n",
    "    # Step 5: Denormalize\n",
    "    final_action = normalizer.denormalize(decoded_normalized)\n",
    "    print(f\"\\n5. Denormalized (robot units):\")\n",
    "    print(f\"   {final_action}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    total_error = np.abs(raw_action - final_action)\n",
    "    print(f\"\\n6. Reconstruction error:\")\n",
    "    print(f\"   {total_error}\")\n",
    "    print(f\"   Max: {total_error.max():.6f}\")\n",
    "    print(f\"   Mean: {total_error.mean():.6f}\")\n",
    "    \n",
    "    return final_action\n",
    "\n",
    "# Run simulation\n",
    "sample_raw_action = np.array([0.05, -0.03, 0.02, 0.1, -0.2, 0.15, 0.8])\n",
    "final = simulate_action_pipeline(sample_raw_action, normalizer, action_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Action Discretization**: Continuous actions \u2192 discrete bins \u2192 vocabulary tokens\n",
    "\n",
    "2. **256 Bins**: Default resolution providing ~0.4mm precision at typical scales\n",
    "\n",
    "3. **Vocabulary Extension**: Action tokens occupy last 256 positions in LLM vocabulary\n",
    "\n",
    "4. **Normalization**: Raw actions normalized to [-1, 1] before tokenization\n",
    "\n",
    "5. **Two-Stage Process**:\n",
    "   - Training: raw \u2192 normalize \u2192 discretize \u2192 cross-entropy loss\n",
    "   - Inference: generate tokens \u2192 undiscretize \u2192 denormalize \u2192 execute\n",
    "\n",
    "### Why It Works\n",
    "- LLMs are optimized for discrete token prediction\n",
    "- Same loss function for language and actions (unified training)\n",
    "- 256 bins provide sufficient precision for manipulation\n",
    "- Autoregressive generation naturally handles action sequences\n",
    "\n",
    "### Next Steps\n",
    "\u2192 Continue to **05_data_pipeline.ipynb** to understand how training data is prepared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}